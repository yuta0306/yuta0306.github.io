---
Title: collate_fnã§è¤‡æ•°ã®å¼•æ•°ã‚’å–ã‚ŠãŸã„!!
Date: '2021-12-24'
Category: Python
Tags: [ML, Python, Pytorch]
Authors: ã‚†ã†ã¼ã†
Slug: pytorch-collate_fn-args
Thumbnail: /images/thumbnails/pytorch-logo.jpg
Description: Transformersã‚’ä½¿ã£ã¦å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’tokenizeã™ã‚‹ã¨ãã«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºãŒå¤§ãã‹ã£ãŸã®ã§ï¼Œãƒãƒƒãƒå˜ä½ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸã‹ã£ãŸæ™‚ãŒã‚ã‚Šã¾ã—ãŸï¼ã“ã®æ™‚ï¼Œcollate_fnã«å¯¾ã—ã¦è¤‡æ•°ã®å¼•æ•°ã‚’ä¸ãˆãŸã‹ã£ãŸçŠ¶æ³ã®æ™‚ã®å¯¾å‡¦æ³•ã§ã™ï¼(æ—¥æœ¬èªå¤‰ã‹ã‚‚)
Published: true
---

Transformersã‚’ä½¿ã£ã¦å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’tokenizeã™ã‚‹ã¨ãã«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºãŒå¤§ãã‹ã£ãŸã®ã§ï¼Œãƒãƒƒãƒå˜ä½ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸã‹ã£ãŸæ™‚ãŒã‚ã‚Šã¾ã—ãŸï¼

ã“ã®æ™‚ï¼Œcollate_fnã«å¯¾ã—ã¦è¤‡æ•°ã®å¼•æ•°ã‚’ä¸ãˆãŸã‹ã£ãŸçŠ¶æ³ã®æ™‚ã®å¯¾å‡¦æ³•ã§ã™ï¼(æ—¥æœ¬èªå¤‰ã‹?)

## ã‚„ã‚ŠãŸã‹ã£ãŸã“ã¨

DataLoaderã‚’å®šç¾©ã™ã‚‹ã¨ãã«ï¼Œ`collate_fn`ã®ã¨ã“ã‚ã§è‡ªä½œcollate_fnã‚’æŒ‡å®šã—ã¦ï¼Œbatchå˜ä½ã§æµã‚Œã¦ãã‚‹ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ï¼

ã“ã‚ŒãŒã‚„ã‚ŠãŸã„ã“ã¨ã«ãªã‚Šã¾ã™ï¼ã¤ã¾ã‚Šã“ã‚“ãªæ„Ÿã˜

~~~python
from torch.utils import Dataset, DataLoader

class MyDataset(Dataset):
    def __init__(self, *args, **kwargs):
        super().__init__()
        ...
    
    def __len__(self):
        ...

    def __getitem__(self, idx: int):
        ...

dataloader = DataLoader(dataset=MyDataset(), batch_size=16, shuffle=True,
            num_workers=os.cpu_count(),
            collate_fn=custom_collate_fn)  # <--- ã“ã“ã§è‡ªä½œcollate_fnã‚’æŒ‡å®šã—ã¦åˆ¶å¾¡
~~~

## ã‚„ã£ã¦ã†ã¾ãã„ã‹ãªã‹ã£ãŸã“ã¨

å…ˆã«ã‚„ã£ã¦ã†ã¾ãã„ã‹ãªã‹ã£ãŸã“ã¨ã‚’å…±æœ‰ã—ã¦ãŠãã¾ã™ï¼

è‡ªåˆ†ãŒä½¿ã£ã¦ã„ã‚‹ã®ãŒï¼Œ`pytorch-lightning`ãªã®ã§ãã®ã›ã„ã‚‚ã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼ãªã®ã§ï¼Œã‚‚ã—ã‹ã—ãŸã‚‰æ™®é€šã«ç´ ã®Pytorchãªã‚‰ã†ã¾ãã„ãã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼

æ•™ãˆã¦ãã ã•ã„ğŸ™

### lambdaå¼ã§åˆ¶å¾¡ã™ã‚‹ (functools.partialã‚’ä½¿ã†)

ã“ã‚“ãªã“ã¨ã‚’ã—ã¾ã—ãŸï¼

~~~python
from torch.utils import Dataset, DataLoader
from transformers import AutoTokenizer

class MyDataset(Dataset):
    def __init__(self, *args, **kwargs):
        super().__init__()
        ...
    
    def __len__(self):
        ...

    def __getitem__(self, idx: int):
        ...
        return text, label

def custom_collate_fn(data, tokenizer, max_length):
    texts, labels = zip(*data)
    texts = list(texts)
    texts = tokenizer.batch_encode_plus(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors='pt',
    )
    labels = torch.LongTensor(labels)
    return texts, labels

tokenizer = AutoTokenizer.from_pretrained(...)
max_length = 256
dataloader = DataLoader(dataset=MyDataset(), batch_size=16, shuffle=True,
            num_workers=os.cpu_count(),
            collate_fn=lambda data: custom_collate_fn(data, tokenizer, max_length))
~~~

`pytorch-lightning`ã®ä»•æ§˜ã ã¨ã¯æ€ã†ã®ã§ã™ãŒï¼Œ`pickle`ã§åœ§ç¸®ã™ã‚‹ã‚‰ã—ããã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼ã‚’åã‹ã‚Œã¾ã—ãŸï¼

ãªãœã ã‚ã†...æœ‰è­˜è€…ã®æ–¹æ•™ãˆã¦ãã ã•ã„...

## ã€è§£æ±ºç­–ã€‘ classã§å®šç¾©ã™ã‚‹

lambdaå¼ã§ãƒ€ãƒ¡ã ã£ãŸã®ã§ï¼Œã‚‚ã†ã‚¯ãƒ©ã‚¹ã®å†…éƒ¨ã«å¿…è¦ãªã‚‚ã®ã‚’ä¿æŒã•ã›ã¦ãŠã“ã†ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã—ãŸï¼(åƒ•ã®ä¸­ã§ã¯)

æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ãªæ„Ÿã˜ã§è§£æ±ºã—ã¾ã—ãŸï¼

~~~python
from torch.utils import Dataset, DataLoader
from transformers import AutoTokenizer

class MyDataset(Dataset):
    def __init__(self, *args, **kwargs):
        super().__init__()
        ...
    
    def __len__(self):
        ...

    def __getitem__(self, idx: int):
        ...
        return text, label

class CollateFn:
    def __init__(self, tokenizer, max_length: int) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        os.environ["TOKENIZERS_PARALLELISM"] = "true"  # <--- å¤šåˆ†ã“ã‚Œã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ãªã„ã¨æ€’ã‚‰ã‚Œã¾ã™ (true|false)

    def __call__(self, data):
        texts, labels = zip(*data)
        texts = list(texts)
        texts = self.tokenizer.batch_encode_plus(
            texts,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt',
        )
        labels = torch.LongTensor(labels)
        return texts, labels

tokenizer = AutoTokenizer.from_pretrained(...)
max_length = 256
dataloader = DataLoader(dataset=MyDataset(), batch_size=16, shuffle=True,
            num_workers=os.cpu_count(),
            collate_fn=CollateFn(tokenizer, max_length))
~~~

## ã¾ã¨ã‚

ç´ ã®Pytorchã§çµ„ã‚ã°å•é¡Œãªã‹ã£ãŸã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒï¼Œ`pytorch-lightning`ã‚’ä½¿ã£ã¦ã„ã‚‹æ–¹ã¯åŒã˜çŠ¶æ³ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼

ãã®æ™‚ã¯ï¼Œãœã²å‚è€ƒã«classã§collate_fnã§å®Ÿè£…ã—ã¦ã¿ã¦è§£æ±ºã®ä¸€åŠ©ã¨ãªã‚ŒãŸã‚‰å¹¸ã„ã§ã™ï¼
