<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>「<!-- -->multi-modal<!-- -->」<!-- -->1<!-- -->ページ目 | <!-- -->ゆうぼうの書跡棚</title><meta name="next-head-count" content="3"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta property="og:title" content="ゆうぼうの書跡棚"/><meta property="og:type" content="website"/><meta property="og:url" content="https://yuta0306.github.io"/><meta property="og:image" content="https://yuta0306.github.io/images/default.png"/><meta property="og:site_name" content="ゆうぼうの書跡棚"/><meta property="og:description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/b31a7883dd1db9d4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b31a7883dd1db9d4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/71fbbc4c2f48f099.css" as="style"/><link rel="stylesheet" href="/_next/static/css/71fbbc4c2f48f099.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/664-60e06c839f82ba03.js" defer=""></script><script src="/_next/static/chunks/768-c61e8ddb09e59da7.js" defer=""></script><script src="/_next/static/chunks/pages/tag/%5Btag%5D/%5Bpage%5D-f9e21eb9aa12ee31.js" defer=""></script><script src="/_next/static/C3_RTYu8LDtRlpbNSHQHc/_buildManifest.js" defer=""></script><script src="/_next/static/C3_RTYu8LDtRlpbNSHQHc/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="header_container__IoqX_"><div class="header_container__inner__pDDDU"><header class="header_header__pKEQL"><a href="/"><h1 class="header_header__title__uoTF0">ゆうぼうの書跡棚</h1></a></header><div class="header_hamburger__kYfxY"><div><img src="/icons/hamburger.png"/></div></div><nav class="header_nav__closed__1h469"><ul class="header_nav__list__eqFqF"><li class="header_nav__item__FNSzb"><a href="/about">About</a></li><li class="header_nav__item_active__qVXxE"><a href="/">Blog</a></li><li class="header_nav__item__FNSzb"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdgyok9pi697ZJvVizRNEw0qghDWz517k1FrbcRmfvvERlraA/viewform">Contact</a></li></ul></nav></div></div><div class="header_category__WFSrL"><ul class="header_category__items____MmN"><a class="header_category__item__0CmfH" href="/category/%E8%AB%96%E6%96%87/1"><li>論文</li></a><a class="header_category__item__0CmfH" href="/category/Web/1"><li>Web</li></a><a class="header_category__item__0CmfH" href="/category/JavaScript/1"><li>JavaScript</li></a><a class="header_category__item__0CmfH" href="/category/Competition/1"><li>Competition</li></a><a class="header_category__item__0CmfH" href="/category/Cloud/1"><li>Cloud</li></a><a class="header_category__item__0CmfH" href="/category/Linux/1"><li>Linux</li></a><a class="header_category__item__0CmfH" href="/category/Python/1"><li>Python</li></a><a class="header_category__item__0CmfH" href="/category/ML/1"><li>ML</li></a><a class="header_category__item__0CmfH" href="/category/Go/1"><li>Go</li></a><a class="header_category__item__0CmfH" href="/category/SQL/1"><li>SQL</li></a></ul></div><div class="main_main__VZQGI"><div class="main_main__container__PFqpL"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__grid__Bpzhl"><div class="card_card__container__PrCEE"><a href="/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9d8a6445-fdde-46c8-b0b9-72b1f53e4491/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-08_21.44.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230521T191039Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=4f4bc4025bf9505ee1318bdbd65422ff21071f8b17540644c4abcf65009097a0&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject" alt="【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9d8a6445-fdde-46c8-b0b9-72b1f53e4491/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-08_21.44.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230521T180328Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=47425b16d3accc25b1ca817eec4d5bf1148e6cf566a7737adf14227c8da0f921&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject" alt="【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms</h2></div></div></a></div><div class="paginager_container____vsL"><ul class="paginager_container__pagers__1zmK9"><a class="paginager_container__pager__GaEzu" href="/tag/multi-modal/1"><li class="paginager_container__pager__page__2YOZs">&lt;&lt;</li></a><li class="paginager_container__pager_deactive__Gjmav">&lt;</li><li class="paginager_container__pager_active__k3Sib">1</li><li class="paginager_container__pager_deactive__Gjmav">&gt;</li><a class="paginager_container__pager__GaEzu" href="/tag/multi-modal/1"><li class="paginager_container__pager__page__2YOZs">&gt;&gt;</li></a></ul></div></div></main><aside class="main_sidebar__tM28d"><div class="shortbio_container__4psan" itemscope="" itemProp="author" itemType="http://schema.org/Person"><div class="shortbio_container__image__eljVd"><img src="/images/profile.jpeg" alt="ゆうぼう" loading="lazy"/></div><h3 class="shortbio_author__A2bKB" itemscope="" itemProp="name">ゆうぼう</h3><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">国立大学院M1のナマケモノです．</p></div><div><p class="shortbio_container__paragraph__EJbWG">human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．</p></div><div><p class="shortbio_container__paragraph__EJbWG">学部時代はコモンセンスを利用したユーモア検出の研究を行っていました(Knowledge-intensive NLP)．</p></div><div><p class="shortbio_container__paragraph__EJbWG">このブログはNext.jsで書いてます．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">Kaggle等のデータ分析コンペは活動休止中．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div></div><div class="followme_container__T1oVi"><h3 class="followme_container__header__Pt5SP">Follow Me</h3><div class="followme_container__links__b3XW5"><a target="_blank" href="https://github.com/yuta0306"><img src="/icons/github.png" alt="GitHub"/></a><a target="_blank" href="https://kaggle.com/yutasasaki"><img src="/icons/kaggle.png" alt="Kaggle"/></a><a target="_blank" href="https://twitter.com/Sloth65557166"><img src="/icons/twitter.png" alt="Twitter"/></a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="categories_container__J8nCF"><h3 class="categories_container__header__zt836">Categories</h3><div class="categories_container__links__MFrVK"><a class="categories_container__link__AuvVr" href="/category/%E8%AB%96%E6%96%87/1">論文</a><a class="categories_container__link__AuvVr" href="/category/Web/1">Web</a><a class="categories_container__link__AuvVr" href="/category/JavaScript/1">JavaScript</a><a class="categories_container__link__AuvVr" href="/category/Competition/1">Competition</a><a class="categories_container__link__AuvVr" href="/category/Cloud/1">Cloud</a><a class="categories_container__link__AuvVr" href="/category/Linux/1">Linux</a><a class="categories_container__link__AuvVr" href="/category/Python/1">Python</a><a class="categories_container__link__AuvVr" href="/category/ML/1">ML</a><a class="categories_container__link__AuvVr" href="/category/Go/1">Go</a><a class="categories_container__link__AuvVr" href="/category/SQL/1">SQL</a></div></div><div class="tags_container___e3ez"><h3 class="tags_container__header__hxPW8">Tags</h3><div class="tags_container__links__X38Ga"><a class="tags_container__link__1Ts3a" href="/tag/Apache/1">Apache</a><a class="tags_container__link__1Ts3a" href="/tag/Appium/1">Appium</a><a class="tags_container__link__1Ts3a" href="/tag/atmaCup/1">atmaCup</a><a class="tags_container__link__1Ts3a" href="/tag/AWS/1">AWS</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS7/1">CentOS7</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS8/1">CentOS8</a><a class="tags_container__link__1Ts3a" href="/tag/Colab/1">Colab</a><a class="tags_container__link__1Ts3a" href="/tag/COMET,mental%20health,NLP,mental%20state%20knowledge,mentalisation,Contrasive%20Learning,MentalRoBERTa,KC-Net/1">COMET,mental health,NLP,mental state knowledge,mentalisation,Contrasive Learning,MentalRoBERTa,KC-Net</a><a class="tags_container__link__1Ts3a" href="/tag/conda/1">conda</a><a class="tags_container__link__1Ts3a" href="/tag/CSS/1">CSS</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system/1">dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,Internet-Augmented/1">dialogue system,Internet-Augmented</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,knowledge-base/1">dialogue system,knowledge-base</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,NLI/1">dialogue system,NLI</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,persona,Prompt-Tuning/1">dialogue system,persona,Prompt-Tuning</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,survey,DST/1">dialogue system,survey,DST</a><a class="tags_container__link__1Ts3a" href="/tag/DST/1">DST</a><a class="tags_container__link__1Ts3a" href="/tag/ESPNet/1">ESPNet</a><a class="tags_container__link__1Ts3a" href="/tag/ffmpeg/1">ffmpeg</a><a class="tags_container__link__1Ts3a" href="/tag/Flask/1">Flask</a><a class="tags_container__link__1Ts3a" href="/tag/Gating%20Mechanism/1">Gating Mechanism</a><a class="tags_container__link__1Ts3a" href="/tag/Go/1">Go</a><a class="tags_container__link__1Ts3a" href="/tag/Google%20Colaboratory/1">Google Colaboratory</a><a class="tags_container__link__1Ts3a" href="/tag/Heroku/1">Heroku</a><a class="tags_container__link__1Ts3a" href="/tag/Highway%20Transformer/1">Highway Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/HTML/1">HTML</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection/1">humor detection</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection,multi-modal/1">humor detection,multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/JavaScript/1">JavaScript</a><a class="tags_container__link__1Ts3a" href="/tag/JSON/1">JSON</a><a class="tags_container__link__1Ts3a" href="/tag/Kaggle/1">Kaggle</a><a class="tags_container__link__1Ts3a" href="/tag/laughter,shared%20laughter/1">laughter,shared laughter</a><a class="tags_container__link__1Ts3a" href="/tag/Linux/1">Linux</a><a class="tags_container__link__1Ts3a" href="/tag/Mac/1">Mac</a><a class="tags_container__link__1Ts3a" href="/tag/make/1">make</a><a class="tags_container__link__1Ts3a" href="/tag/map/1">map</a><a class="tags_container__link__1Ts3a" href="/tag/MeCab/1">MeCab</a><a class="tags_container__link__1Ts3a" href="/tag/ML/1">ML</a><a class="tags_container__link__1Ts3a" href="/tag/MT,transformer,Multi-Hop%20Transformer/1">MT,transformer,Multi-Hop Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/multi-modal/1">multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/MySQL/1">MySQL</a><a class="tags_container__link__1Ts3a" href="/tag/NLP/1">NLP</a><a class="tags_container__link__1Ts3a" href="/tag/Node/1">Node</a><a class="tags_container__link__1Ts3a" href="/tag/node.js/1">node.js</a><a class="tags_container__link__1Ts3a" href="/tag/npm/1">npm</a><a class="tags_container__link__1Ts3a" href="/tag/Pandas/1">Pandas</a><a class="tags_container__link__1Ts3a" href="/tag/Poetry/1">Poetry</a><a class="tags_container__link__1Ts3a" href="/tag/Python/1">Python</a><a class="tags_container__link__1Ts3a" href="/tag/Pytorch/1">Pytorch</a><a class="tags_container__link__1Ts3a" href="/tag/pytorch-lightning/1">pytorch-lightning</a><a class="tags_container__link__1Ts3a" href="/tag/Scikit-learn/1">Scikit-learn</a><a class="tags_container__link__1Ts3a" href="/tag/Selenium/1">Selenium</a><a class="tags_container__link__1Ts3a" href="/tag/Self-Dependency-Units%20(SDU)/1">Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/SISR/1">SISR</a><a class="tags_container__link__1Ts3a" href="/tag/subprocess/1">subprocess</a><a class="tags_container__link__1Ts3a" href="/tag/Super-Resolution/1">Super-Resolution</a><a class="tags_container__link__1Ts3a" href="/tag/survey/1">survey</a><a class="tags_container__link__1Ts3a" href="/tag/survey,dialogue%20system/1">survey,dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive%20NLP/1">survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP</a><a class="tags_container__link__1Ts3a" href="/tag/tensorflow/1">tensorflow</a><a class="tags_container__link__1Ts3a" href="/tag/Tkinter/1">Tkinter</a><a class="tags_container__link__1Ts3a" href="/tag/transformer/1">transformer</a><a class="tags_container__link__1Ts3a" href="/tag/transformer,Highway%20Transformer,Gating%20Mechanism,Self-Dependency-Units%20(SDU)/1">transformer,Highway Transformer,Gating Mechanism,Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/zsh/1">zsh</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91/1">オブジェクト指向</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%82%B3%E3%83%AC%E3%83%BC%E3%82%BF/1">デコレータ</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90/1">データ分析</a><a class="tags_container__link__1Ts3a" href="/tag/%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/1">特殊メソッド</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%B6%85%E8%A7%A3%E5%83%8F/1">超解像</a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div id="TOC"></div></aside></div></div><footer class="footer_footer__WCChH"><div class="footer_footer__inner__287VQ"><div><a class="footer_footer__link__Ql5Ng" href="/privacy-policy">プライバシーポリシー</a></div><div class="footer_footer__title__PRn_u"><a href="/">ゆうぼうの書跡棚</a></div><div class="footer_footer__small__RlIHP"><small>Powered by <a target="_blank" class="footer_footer__small__link__u5kuV" href="https://twitter.com/Sloth65557166">ゆうぼう</a></small></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"TaggedPostData":[{"contentHtml":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eマルチモダールなユーモアデータセット(\u003cstrong\u003eMHD; Multimodal Humor Dataset\u003c/strong\u003e)（The Big Bang Theoryを使用）を構築\u003c/p\u003e\n\u003cp\u003e海外のSitcoms (Situation comedies) では笑い声がドラマ内に含まれている\u003c/p\u003e\n\u003cp\u003e→ sitcomsは定期的に作成されていて，この笑い声を自動で追加するタスクがクリティカルなタスク\u003c/p\u003e\n\u003cp\u003e→ \u003cstrong\u003e笑い声の自動挿入のタスクを自動化することが狙い\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e構築されたデータセットを用いて，マルチモーダルを利用したAttentionベースのモデルを構\u003c/p\u003e\n\u003cp\u003e→SoTA \u0026#x26; データセット分析\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e9dbc394-bcf4-4316-8d7d-26b3a8df346a/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T191045Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=875fcef8e4c34d0244652019e2b3b6e83e11f873d4b7fdd4f6d41626a426947d\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f4ba132d-d530-41ca-8574-543ce3f59b7b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.53.14.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T191049Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=30e66c37ab34dc93b2d82b9f35ca0987d5c57b5fc45ea877041f0344b2aa6548\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/90a64f3d-7b08-4725-abe3-c1a208d7ef88/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.31.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T191049Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=56532addba0d167bc67f748d52ce045a1e6fba89d117ea47209aa29bbe6e78e2\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eデータセットのこと\u003c/h3\u003e\n\u003cp\u003e対話のチャンクに対してlaughter tracksを使用してラベルを付与\u003c/p\u003e\n\u003cp\u003e笑い声をアノテーションすることがは間接的に人手でのアノテーションと同じになるという過程\u003c/p\u003e\n\u003cp\u003e→ 笑い声の起こる直前の発話の集合をユーモアとしてラベル付け\u003c/p\u003e\n\u003cp\u003eAttributes\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eScene\u003c/li\u003e\n\u003cli\u003eSpeaker\u003c/li\u003e\n\u003cli\u003eRecipients\u003c/li\u003e\n\u003cli\u003eParticipants\u003c/li\u003e\n\u003cli\u003eDialogue Turns\u003c/li\u003e\n\u003cli\u003eDialogue Start/End time\u003c/li\u003e\n\u003cli\u003eHumor Start/End time\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e対話のチャンクに複数のlaughter tracksがある場合，最後のみ適用\u003c/p\u003e\n\u003cp\u003eデータ分析の結果はFig 3.を参照のこと\u003c/p\u003e\n\u003ch3\u003eモデルのこと\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2362b493-1c85-4602-ba74-d181ad8ced3d/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.01.01.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T191059Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=68e23c4842088e321a7cc6ad05f2acdee6532c032005ca8cb42788bd72189555\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e手動でアノテーションされたマルチモーダルな大規模ユーモアデータセットを構築\u003c/li\u003e\n\u003cli\u003eこれまでのSoTA手法を実験しつつ，multimodal self attention based modelを提案\u003c/li\u003e\n\u003cli\u003e提案手法の汎化性能を検証\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003e5 turns / dialogueとする\u003c/p\u003e\n\u003cp\u003ehumor : non-humor = 1 : 2としてサンプリング\u003c/p\u003e\n\u003cp\u003ehumorのラベルが85%と高く，かなり不均衡のため\u003c/p\u003e\n\u003cp\u003e実験モデル\u003c/p\u003e\n\u003cp\u003e{Attention, Fusion, Sequential} with {only Text, only Video, both of them}\u003c/p\u003e\n\u003cp\u003e評価指標：\u003c/p\u003e\n\u003cp\u003eAccuracy, ROC, F1\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/28252124-9d19-4422-a8a3-b60fc13c8c83/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.08.22.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T191109Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=ec270447a882b5eab596d1efdba311a8ff756fa2cc5c5b10b00f9468be752803\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5652a72f-b61e-4e35-9a75-8acc0515616b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T191110Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=0fe8d621ae3114e7a98d82e7b35357d0d6c1459d988c0f55a9dbb12ec249d6f9\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/97772d42-0cc9-4b9e-bdbe-e76c5fcc5514/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.24.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T191111Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=8f39111d3b8e6aa30d6582ef9fa29f12fa86d6386f52b359bd80f29d69e1dafe\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e提案手法のMSAMが強い\u003c/p\u003e\n\u003cp\u003e表情や動作のようなvisual特徴量がユーモアの合図になっていることがある\u003c/p\u003e\n\u003cp\u003e→ visual特徴量を使うことが有効である\u003c/p\u003e\n\u003cp\u003e@Table 6.より，dialogueのターン数を長くするとよりcontextualにできるが，長くしすぎても精度が落ちている\u003c/p\u003e\n\u003cp\u003e→ dialogue 5, 6がピークになっている→ ゆえにturn数を5として本研究は進められている\u003c/p\u003e\n\u003ch3\u003eDiscussion\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e良いモデルはテキストと視覚的な特徴量の重みづけの仕方を正しく考慮しなければならない\u003c/li\u003e\n\u003cli\u003e失敗例への対策\u003c/li\u003e\n\u003cli\u003eよりlong tailなユーモアにロバストにならなければいけない\u003c/li\u003e\n\u003cli\u003e例）Sheldonは滅多にブランケットを羽織らない→羽織った時面白くなる\u003c/li\u003e\n\u003cli\u003e知識ベースの弱さへの改善\u003c/li\u003e\n\u003cli\u003esitcomsは皮肉での笑いが多い（知識がないと伝わらないことがある\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003cp\u003eHere is a summary of the paper based on the web search results:\u003c/p\u003e\n\u003ch2\u003eTitle: Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms\u003c/h2\u003e\n\u003cp\u003eURL: \u003ca href=\"https://ieeexplore.ieee.org/document/9423266\"\u003ehttps://ieeexplore.ieee.org/document/9423266\u003c/a\u003e\nConference or Journal: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)\nPublished at: 14 June 2021\nKeywords: multimodal humor, laughter prediction, sitcoms, Big Bang Theory, self-attention\nCited by: 0 (as of 27 April 2023)\u003c/p\u003e\n\u003cp\u003eThe paper aims to automate the task of adding laughter tracks to sitcoms by annotating an existing sitcom (Big Bang Theory) and evaluating various state-of-the-art baselines. The paper also proposes a novel multimodal self-attention based model that outperforms other models.\u003c/p\u003e\n\u003cp\u003eThe paper introduces a new dataset and task of predicting laughter tracks for sitcoms, which is a challenging semantic and practical problem. The paper also proposes a novel multimodal self-attention based model that leverages both text and video modalities.\u003c/p\u003e\n\u003cp\u003eThe proposed method consists of three main components: a multimodal encoder, a self-attention layer, and a binary classifier. The multimodal encoder encodes the text and video features separately using LSTM and CNN respectively, and then concatenates them. The self-attention layer computes the attention weights for each modality and each time step, and then applies them to the encoded features. The binary classifier takes the attended features as input and outputs a probability of laughter for each time step.\u003c/p\u003e\n\u003cp\u003eThe paper conducts experiments on the Big Bang Theory dataset, which contains 10 episodes from season 1 with manual annotations of laughter tracks. The paper compares the proposed method with several baselines, including LSTM, BERT, CNN-LSTM, and CNN-BERT. The paper uses accuracy, precision, recall, F1-score, and ROC-AUC as evaluation metrics.\u003c/p\u003e\n\u003cp\u003eThe paper reports that the proposed method achieves the best performance on all metrics, followed by CNN-LSTM and CNN-BERT. The paper also shows that using both text and video modalities improves the performance over using only one modality. The paper further analyzes the attention weights and finds that they are able to capture some humorous cues in the text and video.\u003c/p\u003e\n\u003cp\u003eThe paper concludes that predicting laughter tracks for sitcoms is a novel and interesting task that requires multimodal understanding of humor. The paper also concludes that the proposed multimodal self-attention based model is effective and interpretable for this task.\u003c/p\u003e\n\u003cp\u003eSome possible papers to read next are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHumor Recognition using Deep Learning by Weller et al., 2019\u003c/li\u003e\n\u003cli\u003eA Multimodal Dataset for Authoring and Editing Jokes by Chakrabarty et al., 2020\u003c/li\u003e\n\u003cli\u003eMultimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems by Le et al., 2019\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eソース: Bing との会話 2023/4/27(1) Multimodal Humor Dataset: Predicting Laughter tracks for .... \u003ca href=\"https://ieeexplore.ieee.org/document/9423266\"\u003ehttps://ieeexplore.ieee.org/document/9423266\u003c/a\u003e アクセス日時 2023/4/27.\n(2) Multimodal humor dataset: Predicting laughter tracks for sitcoms. \u003ca href=\"https://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms\"\u003ehttps://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms\u003c/a\u003e アクセス日時 2023/4/27.\n(3) Multimodal Humor Dataset: Predicting Laughter Tracks for .... \u003ca href=\"https://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html\"\u003ehttps://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html\u003c/a\u003e アクセス日時 2023/4/27.\n(4) Multimodal Humor Dataset: Predicting Laughter tracks for .... \u003ca href=\"https://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22\"\u003ehttps://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22\u003c/a\u003e アクセス日時 2023/4/27.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n","Title":"【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms","Date":"2023-05-21","Category":"論文","Tags":["humor detection","multi-modal"],"Authos":"ゆうぼう","Slug":"Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms","Thumbnail":"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9d8a6445-fdde-46c8-b0b9-72b1f53e4491/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-08_21.44.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230521T191039Z\u0026X-Amz-Expires=3600\u0026X-Amz-Signature=4f4bc4025bf9505ee1318bdbd65422ff21071f8b17540644c4abcf65009097a0\u0026X-Amz-SignedHeaders=host\u0026x-id=GetObject","Description":"Multimodal Humor Dataset: Predicting Laughter tracks for Sitcomsのまとめ","Published":true},{"contentHtml":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eマルチモダールなユーモアデータセット(\u003cstrong\u003eMHD; Multimodal Humor Dataset\u003c/strong\u003e)（The Big Bang Theoryを使用）を構築\u003c/p\u003e\n\u003cp\u003e海外のSitcoms (Situation comedies) では笑い声がドラマ内に含まれている\u003c/p\u003e\n\u003cp\u003e→ sitcomsは定期的に作成されていて，この笑い声を自動で追加するタスクがクリティカルなタスク\u003c/p\u003e\n\u003cp\u003e→ \u003cstrong\u003e笑い声の自動挿入のタスクを自動化することが狙い\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e構築されたデータセットを用いて，マルチモーダルを利用したAttentionベースのモデルを構\u003c/p\u003e\n\u003cp\u003e→SoTA \u0026#x26; データセット分析\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e9dbc394-bcf4-4316-8d7d-26b3a8df346a/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180341Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=5b27ee778b4b52d926cc58b26a85b63ef0e845ca327755aed28fb3059d42d555\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f4ba132d-d530-41ca-8574-543ce3f59b7b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.53.14.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180344Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=9e5f87561ac85c1eda997f5e70d3637833b25116e77abd2a6af1f01e4400a41c\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/90a64f3d-7b08-4725-abe3-c1a208d7ef88/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.31.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180348Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=2af88be8e09237c755126c35e5078a9639502857fb07a760c48c50037aa5448d\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eデータセットのこと\u003c/h3\u003e\n\u003cp\u003e対話のチャンクに対してlaughter tracksを使用してラベルを付与\u003c/p\u003e\n\u003cp\u003e笑い声をアノテーションすることがは間接的に人手でのアノテーションと同じになるという過程\u003c/p\u003e\n\u003cp\u003e→ 笑い声の起こる直前の発話の集合をユーモアとしてラベル付け\u003c/p\u003e\n\u003cp\u003eAttributes\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eScene\u003c/li\u003e\n\u003cli\u003eSpeaker\u003c/li\u003e\n\u003cli\u003eRecipients\u003c/li\u003e\n\u003cli\u003eParticipants\u003c/li\u003e\n\u003cli\u003eDialogue Turns\u003c/li\u003e\n\u003cli\u003eDialogue Start/End time\u003c/li\u003e\n\u003cli\u003eHumor Start/End time\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e対話のチャンクに複数のlaughter tracksがある場合，最後のみ適用\u003c/p\u003e\n\u003cp\u003eデータ分析の結果はFig 3.を参照のこと\u003c/p\u003e\n\u003ch3\u003eモデルのこと\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2362b493-1c85-4602-ba74-d181ad8ced3d/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.01.01.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180414Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=4b9c37202b1c79aa7b8c8fd0daf1a02543384956e8839ea12043668fb8886f60\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e手動でアノテーションされたマルチモーダルな大規模ユーモアデータセットを構築\u003c/li\u003e\n\u003cli\u003eこれまでのSoTA手法を実験しつつ，multimodal self attention based modelを提案\u003c/li\u003e\n\u003cli\u003e提案手法の汎化性能を検証\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003e5 turns / dialogueとする\u003c/p\u003e\n\u003cp\u003ehumor : non-humor = 1 : 2としてサンプリング\u003c/p\u003e\n\u003cp\u003ehumorのラベルが85%と高く，かなり不均衡のため\u003c/p\u003e\n\u003cp\u003e実験モデル\u003c/p\u003e\n\u003cp\u003e{Attention, Fusion, Sequential} with {only Text, only Video, both of them}\u003c/p\u003e\n\u003cp\u003e評価指標：\u003c/p\u003e\n\u003cp\u003eAccuracy, ROC, F1\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/28252124-9d19-4422-a8a3-b60fc13c8c83/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.08.22.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180430Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=13d483e8c4cbfb256f50730b13afc36c6da8e156ebac43d3fd7feb7ac6ceadde\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5652a72f-b61e-4e35-9a75-8acc0515616b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180432Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=1265762aa3394b65c75eddd17c718fd880950065f9214246790590ceae6008ce\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/97772d42-0cc9-4b9e-bdbe-e76c5fcc5514/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.24.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180433Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=c5470ae7c6d1c7389fa4ff5c3a966286463bf12e37511a2ab1e73d215e2c8f01\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e提案手法のMSAMが強い\u003c/p\u003e\n\u003cp\u003e表情や動作のようなvisual特徴量がユーモアの合図になっていることがある\u003c/p\u003e\n\u003cp\u003e→ visual特徴量を使うことが有効である\u003c/p\u003e\n\u003cp\u003e@Table 6.より，dialogueのターン数を長くするとよりcontextualにできるが，長くしすぎても精度が落ちている\u003c/p\u003e\n\u003cp\u003e→ dialogue 5, 6がピークになっている→ ゆえにturn数を5として本研究は進められている\u003c/p\u003e\n\u003ch3\u003eDiscussion\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e良いモデルはテキストと視覚的な特徴量の重みづけの仕方を正しく考慮しなければならない\u003c/li\u003e\n\u003cli\u003e失敗例への対策\u003c/li\u003e\n\u003cli\u003eよりlong tailなユーモアにロバストにならなければいけない\u003c/li\u003e\n\u003cli\u003e例）Sheldonは滅多にブランケットを羽織らない→羽織った時面白くなる\u003c/li\u003e\n\u003cli\u003e知識ベースの弱さへの改善\u003c/li\u003e\n\u003cli\u003esitcomsは皮肉での笑いが多い（知識がないと伝わらないことがある\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003cp\u003eHere is a summary of the paper based on the web search results:\u003c/p\u003e\n\u003ch2\u003eTitle: Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms\u003c/h2\u003e\n\u003cp\u003eURL: \u003ca href=\"https://ieeexplore.ieee.org/document/9423266\"\u003ehttps://ieeexplore.ieee.org/document/9423266\u003c/a\u003e\nConference or Journal: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)\nPublished at: 14 June 2021\nKeywords: multimodal humor, laughter prediction, sitcoms, Big Bang Theory, self-attention\nCited by: 0 (as of 27 April 2023)\u003c/p\u003e\n\u003cp\u003eThe paper aims to automate the task of adding laughter tracks to sitcoms by annotating an existing sitcom (Big Bang Theory) and evaluating various state-of-the-art baselines. The paper also proposes a novel multimodal self-attention based model that outperforms other models.\u003c/p\u003e\n\u003cp\u003eThe paper introduces a new dataset and task of predicting laughter tracks for sitcoms, which is a challenging semantic and practical problem. The paper also proposes a novel multimodal self-attention based model that leverages both text and video modalities.\u003c/p\u003e\n\u003cp\u003eThe proposed method consists of three main components: a multimodal encoder, a self-attention layer, and a binary classifier. The multimodal encoder encodes the text and video features separately using LSTM and CNN respectively, and then concatenates them. The self-attention layer computes the attention weights for each modality and each time step, and then applies them to the encoded features. The binary classifier takes the attended features as input and outputs a probability of laughter for each time step.\u003c/p\u003e\n\u003cp\u003eThe paper conducts experiments on the Big Bang Theory dataset, which contains 10 episodes from season 1 with manual annotations of laughter tracks. The paper compares the proposed method with several baselines, including LSTM, BERT, CNN-LSTM, and CNN-BERT. The paper uses accuracy, precision, recall, F1-score, and ROC-AUC as evaluation metrics.\u003c/p\u003e\n\u003cp\u003eThe paper reports that the proposed method achieves the best performance on all metrics, followed by CNN-LSTM and CNN-BERT. The paper also shows that using both text and video modalities improves the performance over using only one modality. The paper further analyzes the attention weights and finds that they are able to capture some humorous cues in the text and video.\u003c/p\u003e\n\u003cp\u003eThe paper concludes that predicting laughter tracks for sitcoms is a novel and interesting task that requires multimodal understanding of humor. The paper also concludes that the proposed multimodal self-attention based model is effective and interpretable for this task.\u003c/p\u003e\n\u003cp\u003eSome possible papers to read next are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHumor Recognition using Deep Learning by Weller et al., 2019\u003c/li\u003e\n\u003cli\u003eA Multimodal Dataset for Authoring and Editing Jokes by Chakrabarty et al., 2020\u003c/li\u003e\n\u003cli\u003eMultimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems by Le et al., 2019\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eソース: Bing との会話 2023/4/27(1) Multimodal Humor Dataset: Predicting Laughter tracks for .... \u003ca href=\"https://ieeexplore.ieee.org/document/9423266\"\u003ehttps://ieeexplore.ieee.org/document/9423266\u003c/a\u003e アクセス日時 2023/4/27.\n(2) Multimodal humor dataset: Predicting laughter tracks for sitcoms. \u003ca href=\"https://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms\"\u003ehttps://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms\u003c/a\u003e アクセス日時 2023/4/27.\n(3) Multimodal Humor Dataset: Predicting Laughter Tracks for .... \u003ca href=\"https://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html\"\u003ehttps://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html\u003c/a\u003e アクセス日時 2023/4/27.\n(4) Multimodal Humor Dataset: Predicting Laughter tracks for .... \u003ca href=\"https://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22\"\u003ehttps://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22\u003c/a\u003e アクセス日時 2023/4/27.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n","Title":"【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms","Date":"2023-05-21","Category":"論文","Tags":"humor detection,multi-modal","Authos":"ゆうぼう","Slug":"Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms","Thumbnail":"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9d8a6445-fdde-46c8-b0b9-72b1f53e4491/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-08_21.44.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230521T180328Z\u0026X-Amz-Expires=3600\u0026X-Amz-Signature=47425b16d3accc25b1ca817eec4d5bf1148e6cf566a7737adf14227c8da0f921\u0026X-Amz-SignedHeaders=host\u0026x-id=GetObject","Description":"Multimodal Humor Dataset: Predicting Laughter tracks for Sitcomsのまとめ","Published":true}],"tag":"multi-modal","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET,mental health,NLP,mental state knowledge,mentalisation,Contrasive Learning,MentalRoBERTa,KC-Net","conda","CSS","dialogue system","dialogue system,Internet-Augmented","dialogue system,knowledge-base","dialogue system,NLI","dialogue system,persona,Prompt-Tuning","dialogue system,survey,DST","DST","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","humor detection,multi-modal","JavaScript","JSON","Kaggle","laughter,shared laughter","Linux","Mac","make","map","MeCab","ML","MT,transformer,Multi-Hop Transformer","multi-modal","MySQL","NLP","Node","node.js","npm","Pandas","Poetry","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","SISR","subprocess","Super-Resolution","survey","survey,dialogue system","survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP","tensorflow","Tkinter","transformer","transformer,Highway Transformer,Gating Mechanism,Self-Dependency-Units (SDU)","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true},"page":"/tag/[tag]/[page]","query":{"tag":"multi-modal","page":"1"},"buildId":"C3_RTYu8LDtRlpbNSHQHc","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>