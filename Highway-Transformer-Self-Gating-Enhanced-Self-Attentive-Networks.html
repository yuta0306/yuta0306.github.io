<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks<!-- --> | <!-- -->ゆうぼうの書跡棚</title><meta name="description" content="Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ"/><meta name="og:description" content="Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ"/><meta property="og:type" content="website"/><meta property="og:title" content="【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks"/><meta property="og:image" content="https://yuta0306.github.io/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png"/><meta property="og:url" content="https://yuta0306.github.io/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks"/><script src="/js/toc.js"></script><meta name="next-head-count" content="10"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/75506965150cde8a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/75506965150cde8a.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a6e19106a865540a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a6e19106a865540a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/664-60e06c839f82ba03.js" defer=""></script><script src="/_next/static/chunks/768-c61e8ddb09e59da7.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-b275297c06761584.js" defer=""></script><script src="/_next/static/JH1zPBZfb3-e96j1PXu_c/_buildManifest.js" defer=""></script><script src="/_next/static/JH1zPBZfb3-e96j1PXu_c/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Home_container__bCOhY"><div class="header_container__IoqX_"><div class="header_container__inner__pDDDU"><header class="header_header__pKEQL"><a href="/"><div class="header_header__title__uoTF0">ゆうぼうの書跡棚</div></a></header><div class="header_hamburger__kYfxY"><div><img src="/icons/hamburger.png"/></div></div><nav class="header_nav__closed__1h469"><ul class="header_nav__list__eqFqF"><li class="header_nav__item__FNSzb"><a href="/about">About</a></li><li class="header_nav__item_active__qVXxE"><a href="/">Blog</a></li><li class="header_nav__item__FNSzb"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdgyok9pi697ZJvVizRNEw0qghDWz517k1FrbcRmfvvERlraA/viewform">Contact</a></li></ul></nav></div></div><div class="header_category__WFSrL"><ul class="header_category__items____MmN"></ul></div><div class="main_main__VZQGI"><div class="main_main__container__PFqpL"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__V_9fG"><div itemscope="" itemType="http://schema.org/BlogPosting"><div style="background:url(/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png);overflow:hidden"><div class="Home_thumbnail__xs1Hd" itemscope="" itemProp="image" itemType="https://schema.org/ImageObject"><img src="/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png" alt="【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks" loading="lazy" style="height:100%;width:auto;margin:0 auto;display:block"/></div></div><time dateTime="2023-05-21" style="color:rgb(144, 144, 144)">2023-05-21</time><h1>【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</h1><div id="TOC__mobile"></div><article style="margin-top:4rem" itemscope="" itemProp="text"><p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>
<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>
<h2>論文情報</h2>
<p>タイトル: Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</p>
<p>研究会: ACL</p>
<p>年度: 2020</p>
<p>キーワード: transformer, Highway Transformer, Gating Mechanism, Self-Dependency-Units (SDU)</p>
<p>URL: <a href="https://aclanthology.org/2020.acl-main.616.pdf">https://aclanthology.org/2020.acl-main.616.pdf</a></p>
<p>DOI: <a href="http://dx.doi.org/10.18653/v1/2020.acl-main.616">http://dx.doi.org/10.18653/v1/2020.acl-main.616</a></p>
<p>コード: <a href="https://github.com/cyk1337/Highway-Transformer">https://github.com/cyk1337/Highway-Transformer</a></p>
<p>データセット: Penn Tree Bank (PTB), enwik8</p>
<h2>概要</h2>
<p>LSTM-styleなSDUを提案</p>
<p>ゲートとしてSDUをTransformer内部に適用することにより，ハイパラをチューニングすることなく，Transformerの浅い層において，内在的な意味の重要性を捉え，より早い収束を可能に</p>
<h2>提案手法</h2>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/qrpajdel.png" alt=""></p>
<h3>Self-Dependency Units (SDU)</h3>
<p>sigmoid gatesを導入する</p>
<p><span class="math math-inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.76ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 778 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-3A8" d="M340 622Q338 623 335 625T331 629T325 631T314 634T298 635T274 636T239 637H212V683H224Q248 680 389 680T554 683H566V637H539Q479 637 464 635T439 622L438 407Q438 192 439 192Q443 193 449 195T474 207T507 232T536 276T557 344Q560 365 562 417T573 493Q587 536 620 544Q627 546 671 546H715L722 540V515Q714 509 708 509Q680 505 671 476T658 392T644 307Q599 177 451 153L438 151V106L439 61Q446 54 451 52T476 48T539 46H566V0H554Q530 3 389 3T224 0H212V46H239Q259 46 273 46T298 47T314 48T325 51T331 54T335 57T340 61V151Q126 178 117 406Q115 503 69 509Q55 509 55 526Q55 541 59 543T86 546H107H120Q150 546 161 543T184 528Q198 514 204 493Q212 472 213 420T226 316T272 230Q287 216 303 207T330 194L339 192Q340 192 340 407V622Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="3A8" xlink:href="#MJX-1-TEX-N-3A8"></use></g></g></g></svg></mjx-container></span>はゲートとして作用し，logistic sigmoidもしくはtanhで実現</p>
<p>筆者らの認識</p>
<p>tanhはupdate gateとして作用し，重要度の幅を-1 to 1に制限</p>
<p>sigmoidはLSTMのinput gateと似ていて，feature-wise levelでどれくらいの情報を残すか決定</p>
<h3>Pseudo-highway Connection</h3>
<p>residual connectionされたgating-modified encodingsでMulti Head Dot Product Attention (MHDPA)の分散表現を豊かにするため，新たな計算グラフの枝を追加し，SDUとIdentityとMHDPAをpost LNを使用してresidual connectionする</p>
<h2>新規性</h2>
<p>本来，人にとって，読み物をよりよく理解するためには，global contextだけではなく，ここの単語の意味も必要</p>
<p>→ Self-gatingなアプローチを提案</p>
<ol>
<li>Transformerにおける浅い層において，trainingとvalidationでハイパラチューニングすることなく，より高速な収束を達成</li>
<li>Transformerでの低レイヤーにおいて，local-range encodingにフォーカスした層を実現</li>
<li>Self-gating mechanismは，R-TransformerやTransformer-XLのコンポーネントとしてRNN-likeなメカニズムを補完</li>
</ol>
<h2>実験</h2>
<p>SDUを導入し，PTBデータセットにおけるSDUの効果を検証</p>
<p>sigmoidとtanhを実験</p>
<h2>まとめ</h2>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/op874d4u.png" alt=""></p>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/e99q8luh.png" alt=""></p>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4xk5l3fv.png" alt=""></p>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/gen0ole9.png" alt=""></p>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/bjrucnwe.png" alt=""></p>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4fv3x3v3.png" alt=""></p>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/c6hfqjx9.png" alt=""></p>
<p><img src="/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/aptzk9jc.png" alt=""></p>
<p>sigmoidによるSDUが安定しているが，データとタスクによってはtanhの方がoutperformすることがある</p>
<p>いずれのactivationを使っても収束は早い</p>
<p>enwik8による大規模データでの追実験において，提案手法が浅いレイヤーには寄与することが確かめられた</p>
<p>SDUで計算量が増えるが，そこまで差はなかった</p>
<h2>その他（なぜ通ったか？等）</h2>
<h2>次読みたい論文</h2>
<h2>引用</h2>
<blockquote>
<p>@inproceedings{chai-etal-2020-highway,
title = "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks",
author = "Chai, Yekun  and
Jin, Shuo  and
Hou, Xinwen",
booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
month = jul,
year = "2020",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "<a href="https://aclanthology.org/2020.acl-main.616">https://aclanthology.org/2020.acl-main.616</a>",
doi = "10.18653/v1/2020.acl-main.616",
pages = "6887--6900",
abstract = "Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.",
}</p>
</blockquote><style>
mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
  min-height: 1px;
  min-width: 1px;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line], svg[data-table] > g > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > g > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

mjx-container[jax="SVG"] path[data-c], mjx-container[jax="SVG"] use[data-c] {
  stroke-width: 3;
}
</style></article><div class="socialshare_container__SSXJE"><h3>タメになったらSHARE!!!</h3><div class="socialshare_container__links__JZs4j"><a target="_blank" href="https://twitter.com/share?url=https://yuta0306.github.io/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks"><img src="/icons/twitter.png" loading="lazy" alt="https://yuta0306.github.io/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-NetworksをTwitterに共有する"/></a><a target="_blank" href="https://www.facebook.com/share.php?u=https://yuta0306.github.io/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks"><img src="/icons/facebook.png" loading="lazy" alt="https://yuta0306.github.io/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-NetworksをFacebookに共有する"/></a><a target="_blank" href="http://b.hatena.ne.jp/entry/https:/yuta0306.github.io/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks"><img src="/icons/hatenablog.png" loading="lazy" alt="https://yuta0306.github.io/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networksをはてなブログに共有する"/></a></div></div></div></div></main><aside class="main_sidebar__tM28d"><div class="shortbio_container__4psan" itemscope="" itemProp="author" itemType="http://schema.org/Person"><div class="shortbio_container__image__eljVd"><img src="/images/profile.jpeg" alt="ゆうぼう" loading="lazy"/></div><h3 class="shortbio_author__A2bKB" itemscope="" itemProp="name">ゆうぼう</h3><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">国立大学院M1のナマケモノです．</p></div><div><p class="shortbio_container__paragraph__EJbWG">human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．</p></div><div><p class="shortbio_container__paragraph__EJbWG">学部時代はコモンセンスを利用したユーモア検出の研究を行っていました(Knowledge-intensive NLP)．</p></div><div><p class="shortbio_container__paragraph__EJbWG">このブログはNext.jsで書いてます．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">Kaggle等のデータ分析コンペは活動休止中．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div></div><div class="followme_container__T1oVi"><h3 class="followme_container__header__Pt5SP">Follow Me</h3><div class="followme_container__links__b3XW5"><a target="_blank" href="https://github.com/yuta0306"><img src="/icons/github.png" alt="GitHub"/></a><a target="_blank" href="https://kaggle.com/yutasasaki"><img src="/icons/kaggle.png" alt="Kaggle"/></a><a target="_blank" href="https://twitter.com/Sloth65557166"><img src="/icons/twitter.png" alt="Twitter"/></a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="categories_container__J8nCF"><h3 class="categories_container__header__zt836">Categories</h3><div class="categories_container__links__MFrVK"><a class="categories_container__link__AuvVr" href="/category/%E8%AB%96%E6%96%87/1">論文</a><a class="categories_container__link__AuvVr" href="/category/Web/1">Web</a><a class="categories_container__link__AuvVr" href="/category/JavaScript/1">JavaScript</a><a class="categories_container__link__AuvVr" href="/category/Competition/1">Competition</a><a class="categories_container__link__AuvVr" href="/category/Cloud/1">Cloud</a><a class="categories_container__link__AuvVr" href="/category/Linux/1">Linux</a><a class="categories_container__link__AuvVr" href="/category/Python/1">Python</a><a class="categories_container__link__AuvVr" href="/category/ML/1">ML</a><a class="categories_container__link__AuvVr" href="/category/Go/1">Go</a><a class="categories_container__link__AuvVr" href="/category/SQL/1">SQL</a></div></div><div class="tags_container___e3ez"><h3 class="tags_container__header__hxPW8">Tags</h3><div class="tags_container__links__X38Ga"><a class="tags_container__link__1Ts3a" href="/tag/Apache/1">Apache</a><a class="tags_container__link__1Ts3a" href="/tag/Appium/1">Appium</a><a class="tags_container__link__1Ts3a" href="/tag/atmaCup/1">atmaCup</a><a class="tags_container__link__1Ts3a" href="/tag/AWS/1">AWS</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS7/1">CentOS7</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS8/1">CentOS8</a><a class="tags_container__link__1Ts3a" href="/tag/Colab/1">Colab</a><a class="tags_container__link__1Ts3a" href="/tag/COMET/1">COMET</a><a class="tags_container__link__1Ts3a" href="/tag/commonsense/1">commonsense</a><a class="tags_container__link__1Ts3a" href="/tag/conda/1">conda</a><a class="tags_container__link__1Ts3a" href="/tag/Contrasive%20Learning/1">Contrasive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/Contrastive%20Learning/1">Contrastive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/CSS/1">CSS</a><a class="tags_container__link__1Ts3a" href="/tag/Dialogue%20Structure%20Learning/1">Dialogue Structure Learning</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system/1">dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/DST/1">DST</a><a class="tags_container__link__1Ts3a" href="/tag/empathetic%20dialogue%20system/1">empathetic dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/encyclopedic/1">encyclopedic</a><a class="tags_container__link__1Ts3a" href="/tag/ESPNet/1">ESPNet</a><a class="tags_container__link__1Ts3a" href="/tag/ffmpeg/1">ffmpeg</a><a class="tags_container__link__1Ts3a" href="/tag/Flask/1">Flask</a><a class="tags_container__link__1Ts3a" href="/tag/Gating%20Mechanism/1">Gating Mechanism</a><a class="tags_container__link__1Ts3a" href="/tag/Go/1">Go</a><a class="tags_container__link__1Ts3a" href="/tag/Google%20Colaboratory/1">Google Colaboratory</a><a class="tags_container__link__1Ts3a" href="/tag/Heroku/1">Heroku</a><a class="tags_container__link__1Ts3a" href="/tag/Highway%20Transformer/1">Highway Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/HTML/1">HTML</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection/1">humor detection</a><a class="tags_container__link__1Ts3a" href="/tag/Internet-Augmented/1">Internet-Augmented</a><a class="tags_container__link__1Ts3a" href="/tag/JavaScript/1">JavaScript</a><a class="tags_container__link__1Ts3a" href="/tag/JSON/1">JSON</a><a class="tags_container__link__1Ts3a" href="/tag/Kaggle/1">Kaggle</a><a class="tags_container__link__1Ts3a" href="/tag/KC-Net/1">KC-Net</a><a class="tags_container__link__1Ts3a" href="/tag/knowledge-base/1">knowledge-base</a><a class="tags_container__link__1Ts3a" href="/tag/Knowledge-Intensive%20NLP/1">Knowledge-Intensive NLP</a><a class="tags_container__link__1Ts3a" href="/tag/laughter/1">laughter</a><a class="tags_container__link__1Ts3a" href="/tag/Linux/1">Linux</a><a class="tags_container__link__1Ts3a" href="/tag/Mac/1">Mac</a><a class="tags_container__link__1Ts3a" href="/tag/make/1">make</a><a class="tags_container__link__1Ts3a" href="/tag/map/1">map</a><a class="tags_container__link__1Ts3a" href="/tag/MeCab/1">MeCab</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20health/1">mental health</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20state%20knowledge/1">mental state knowledge</a><a class="tags_container__link__1Ts3a" href="/tag/mentalisation/1">mentalisation</a><a class="tags_container__link__1Ts3a" href="/tag/MentalRoBERTa/1">MentalRoBERTa</a><a class="tags_container__link__1Ts3a" href="/tag/ML/1">ML</a><a class="tags_container__link__1Ts3a" href="/tag/MT/1">MT</a><a class="tags_container__link__1Ts3a" href="/tag/Multi-Hop%20Transformer/1">Multi-Hop Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/multi-modal/1">multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/MySQL/1">MySQL</a><a class="tags_container__link__1Ts3a" href="/tag/NLG/1">NLG</a><a class="tags_container__link__1Ts3a" href="/tag/NLI/1">NLI</a><a class="tags_container__link__1Ts3a" href="/tag/NLP/1">NLP</a><a class="tags_container__link__1Ts3a" href="/tag/Node/1">Node</a><a class="tags_container__link__1Ts3a" href="/tag/node.js/1">node.js</a><a class="tags_container__link__1Ts3a" href="/tag/npm/1">npm</a><a class="tags_container__link__1Ts3a" href="/tag/Pandas/1">Pandas</a><a class="tags_container__link__1Ts3a" href="/tag/persona/1">persona</a><a class="tags_container__link__1Ts3a" href="/tag/PLMKE/1">PLMKE</a><a class="tags_container__link__1Ts3a" href="/tag/Poetry/1">Poetry</a><a class="tags_container__link__1Ts3a" href="/tag/Prompt-Tuning/1">Prompt-Tuning</a><a class="tags_container__link__1Ts3a" href="/tag/Python/1">Python</a><a class="tags_container__link__1Ts3a" href="/tag/Pytorch/1">Pytorch</a><a class="tags_container__link__1Ts3a" href="/tag/pytorch-lightning/1">pytorch-lightning</a><a class="tags_container__link__1Ts3a" href="/tag/Scikit-learn/1">Scikit-learn</a><a class="tags_container__link__1Ts3a" href="/tag/Selenium/1">Selenium</a><a class="tags_container__link__1Ts3a" href="/tag/Self-Dependency-Units%20(SDU)/1">Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/shared%20laughter/1">shared laughter</a><a class="tags_container__link__1Ts3a" href="/tag/SISR/1">SISR</a><a class="tags_container__link__1Ts3a" href="/tag/subprocess/1">subprocess</a><a class="tags_container__link__1Ts3a" href="/tag/Super-Resolution/1">Super-Resolution</a><a class="tags_container__link__1Ts3a" href="/tag/survey/1">survey</a><a class="tags_container__link__1Ts3a" href="/tag/tensorflow/1">tensorflow</a><a class="tags_container__link__1Ts3a" href="/tag/Tkinter/1">Tkinter</a><a class="tags_container__link__1Ts3a" href="/tag/transformer/1">transformer</a><a class="tags_container__link__1Ts3a" href="/tag/zsh/1">zsh</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91/1">オブジェクト指向</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%82%B3%E3%83%AC%E3%83%BC%E3%82%BF/1">デコレータ</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90/1">データ分析</a><a class="tags_container__link__1Ts3a" href="/tag/%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/1">特殊メソッド</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%81%9E%E3%81%8D%E6%89%8B%E5%8F%8D%E5%BF%9C/1">聞き手反応</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%B6%85%E8%A7%A3%E5%83%8F/1">超解像</a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div id="TOC"></div></aside></div></div><footer class="footer_footer__WCChH"><div class="footer_footer__inner__287VQ"><div><a class="footer_footer__link__Ql5Ng" href="/privacy-policy">プライバシーポリシー</a></div><div class="footer_footer__title__PRn_u"><a href="/">ゆうぼうの書跡棚</a></div><div class="footer_footer__small__RlIHP"><small>Powered by <a target="_blank" class="footer_footer__small__link__u5kuV" href="https://twitter.com/Sloth65557166">ゆうぼう</a></small></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\u003c/p\u003e\n\u003cp\u003e研究会: ACL\u003c/p\u003e\n\u003cp\u003e年度: 2020\u003c/p\u003e\n\u003cp\u003eキーワード: transformer, Highway Transformer, Gating Mechanism, Self-Dependency-Units (SDU)\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://aclanthology.org/2020.acl-main.616.pdf\"\u003ehttps://aclanthology.org/2020.acl-main.616.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"http://dx.doi.org/10.18653/v1/2020.acl-main.616\"\u003ehttp://dx.doi.org/10.18653/v1/2020.acl-main.616\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eコード: \u003ca href=\"https://github.com/cyk1337/Highway-Transformer\"\u003ehttps://github.com/cyk1337/Highway-Transformer\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: Penn Tree Bank (PTB), enwik8\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eLSTM-styleなSDUを提案\u003c/p\u003e\n\u003cp\u003eゲートとしてSDUをTransformer内部に適用することにより，ハイパラをチューニングすることなく，Transformerの浅い層において，内在的な意味の重要性を捉え，より早い収束を可能に\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/qrpajdel.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eSelf-Dependency Units (SDU)\u003c/h3\u003e\n\u003cp\u003esigmoid gatesを導入する\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.76ex\" height=\"1.545ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 778 683\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-1-TEX-N-3A8\" d=\"M340 622Q338 623 335 625T331 629T325 631T314 634T298 635T274 636T239 637H212V683H224Q248 680 389 680T554 683H566V637H539Q479 637 464 635T439 622L438 407Q438 192 439 192Q443 193 449 195T474 207T507 232T536 276T557 344Q560 365 562 417T573 493Q587 536 620 544Q627 546 671 546H715L722 540V515Q714 509 708 509Q680 505 671 476T658 392T644 307Q599 177 451 153L438 151V106L439 61Q446 54 451 52T476 48T539 46H566V0H554Q530 3 389 3T224 0H212V46H239Q259 46 273 46T298 47T314 48T325 51T331 54T335 57T340 61V151Q126 178 117 406Q115 503 69 509Q55 509 55 526Q55 541 59 543T86 546H107H120Q150 546 161 543T184 528Q198 514 204 493Q212 472 213 420T226 316T272 230Q287 216 303 207T330 194L339 192Q340 192 340 407V622Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"3A8\" xlink:href=\"#MJX-1-TEX-N-3A8\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eはゲートとして作用し，logistic sigmoidもしくはtanhで実現\u003c/p\u003e\n\u003cp\u003e筆者らの認識\u003c/p\u003e\n\u003cp\u003etanhはupdate gateとして作用し，重要度の幅を-1 to 1に制限\u003c/p\u003e\n\u003cp\u003esigmoidはLSTMのinput gateと似ていて，feature-wise levelでどれくらいの情報を残すか決定\u003c/p\u003e\n\u003ch3\u003ePseudo-highway Connection\u003c/h3\u003e\n\u003cp\u003eresidual connectionされたgating-modified encodingsでMulti Head Dot Product Attention (MHDPA)の分散表現を豊かにするため，新たな計算グラフの枝を追加し，SDUとIdentityとMHDPAをpost LNを使用してresidual connectionする\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003e本来，人にとって，読み物をよりよく理解するためには，global contextだけではなく，ここの単語の意味も必要\u003c/p\u003e\n\u003cp\u003e→ Self-gatingなアプローチを提案\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTransformerにおける浅い層において，trainingとvalidationでハイパラチューニングすることなく，より高速な収束を達成\u003c/li\u003e\n\u003cli\u003eTransformerでの低レイヤーにおいて，local-range encodingにフォーカスした層を実現\u003c/li\u003e\n\u003cli\u003eSelf-gating mechanismは，R-TransformerやTransformer-XLのコンポーネントとしてRNN-likeなメカニズムを補完\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eSDUを導入し，PTBデータセットにおけるSDUの効果を検証\u003c/p\u003e\n\u003cp\u003esigmoidとtanhを実験\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/op874d4u.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/e99q8luh.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4xk5l3fv.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/gen0ole9.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/bjrucnwe.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4fv3x3v3.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/c6hfqjx9.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/aptzk9jc.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003esigmoidによるSDUが安定しているが，データとタスクによってはtanhの方がoutperformすることがある\u003c/p\u003e\n\u003cp\u003eいずれのactivationを使っても収束は早い\u003c/p\u003e\n\u003cp\u003eenwik8による大規模データでの追実験において，提案手法が浅いレイヤーには寄与することが確かめられた\u003c/p\u003e\n\u003cp\u003eSDUで計算量が増えるが，そこまで差はなかった\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@inproceedings{chai-etal-2020-highway,\ntitle = \"Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\",\nauthor = \"Chai, Yekun  and\nJin, Shuo  and\nHou, Xinwen\",\nbooktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\nmonth = jul,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"\u003ca href=\"https://aclanthology.org/2020.acl-main.616\"\u003ehttps://aclanthology.org/2020.acl-main.616\u003c/a\u003e\",\ndoi = \"10.18653/v1/2020.acl-main.616\",\npages = \"6887--6900\",\nabstract = \"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.\",\n}\u003c/p\u003e\n\u003c/blockquote\u003e\u003cstyle\u003e\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] \u003e g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] \u003e rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e line[data-line], svg[data-table] \u003e g \u003e line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e rect[data-frame], svg[data-table] \u003e g \u003e rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dashed, svg[data-table] \u003e g \u003e .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dotted, svg[data-table] \u003e g \u003e .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e g \u003e svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool \u003e mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool \u003e mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n\u003c/style\u003e","Title":"【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks","Date":"2023-05-21","Category":"論文","Tags":["transformer","Highway Transformer","Gating Mechanism","Self-Dependency-Units (SDU)"],"Authos":"ゆうぼう","Slug":"Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks","Thumbnail":"/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png","Description":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Dialogue Structure Learning","dialogue system","DST","empathetic dialogue system","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"]},"__N_SSG":true},"page":"/[slug]","query":{"slug":"Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks"},"buildId":"JH1zPBZfb3-e96j1PXu_c","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>