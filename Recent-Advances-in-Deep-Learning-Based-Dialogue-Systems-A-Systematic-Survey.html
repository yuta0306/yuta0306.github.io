<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey<!-- --> | <!-- -->ゆうぼうの書跡棚</title><meta name="description" content="Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Surveyのまとめ"/><meta name="og:description" content="Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Surveyのまとめ"/><meta property="og:type" content="website"/><meta property="og:title" content="【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey"/><meta property="og:image" content="https://yuta0306.github.io/images/thumbnails/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey.png"/><meta property="og:url" content="https://yuta0306.github.io/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey"/><script src="/js/toc.js"></script><meta name="next-head-count" content="10"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><link rel="alternate" type="application/rss+xml" href="https:/yuta0306.github.io/feed.xml" title="RSS2.0"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/75506965150cde8a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/75506965150cde8a.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a6e19106a865540a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a6e19106a865540a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/664-60e06c839f82ba03.js" defer=""></script><script src="/_next/static/chunks/768-c61e8ddb09e59da7.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-b275297c06761584.js" defer=""></script><script src="/_next/static/pIuzt9pMRyUqa50hVFtFJ/_buildManifest.js" defer=""></script><script src="/_next/static/pIuzt9pMRyUqa50hVFtFJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Home_container__bCOhY"><div class="header_container__IoqX_"><div class="header_container__inner__pDDDU"><header class="header_header__pKEQL"><a href="/"><div class="header_header__title__uoTF0">ゆうぼうの書跡棚</div></a></header><div class="header_hamburger__kYfxY"><div><img src="/icons/hamburger.png"/></div></div><nav class="header_nav__closed__1h469"><ul class="header_nav__list__eqFqF"><li class="header_nav__item__FNSzb"><a href="/about">About</a></li><li class="header_nav__item_active__qVXxE"><a href="/">Blog</a></li><li class="header_nav__item__FNSzb"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdgyok9pi697ZJvVizRNEw0qghDWz517k1FrbcRmfvvERlraA/viewform">Contact</a></li></ul></nav></div></div><div class="header_category__WFSrL"><ul class="header_category__items____MmN"></ul></div><div class="main_main__VZQGI"><div class="main_main__container__PFqpL"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__V_9fG"><div itemscope="" itemType="http://schema.org/BlogPosting"><div style="background:url(/images/thumbnails/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey.png);overflow:hidden"><div class="Home_thumbnail__xs1Hd" itemscope="" itemProp="image" itemType="https://schema.org/ImageObject"><img src="/images/thumbnails/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey.png" alt="【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey" loading="lazy" style="height:100%;width:auto;margin:0 auto;display:block"/></div></div><time dateTime="2023-05-22" style="color:rgb(144, 144, 144)">2023-05-22</time><h1>【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey</h1><div id="TOC__mobile"></div><article style="margin-top:4rem" itemscope="" itemProp="text"><p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>
<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>
<h2>論文情報</h2>
<p>タイトル: Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey</p>
<p>研究会: arxiv</p>
<p>年度: 2021</p>
<p>キーワード: survey, dialogue system</p>
<p>URL: <a href="https://arxiv.org/pdf/2105.04387.pdf">https://arxiv.org/pdf/2105.04387.pdf</a></p>
<p>データセット:</p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ry2fz8tn.png" alt=""></p>
<h2>概要</h2>
<p>対話システムに関するサーベイ論文</p>
<p>対話システムはNLPタスクの一種</p>
<p>研究の価値が高いNLPタスクを多く含むため，対話システムは複雑と言える．</p>
<p>ここ最近で良い成果をあげているもののほとんどがDL</p>
<p>メインは，モデルタイプとシステムタイプについて述べられる．</p>
<p>システムタイプ</p>
<p>タスク指向型</p>
<p>オープンドメイン型</p>
<h3>Keywords</h3>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/8575dpgt.png" alt=""></p>
<h3>サーベイの主張の流れ</h3>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/hpk33ao6.png" alt=""></p>
<h2>まとめ</h2>
<h3>Introduction</h3>
<p>対話システムはNLPにおいてホットな話題であり，産業においても需要が非常に高い．</p>
<p>タスク指向型とオープンドメイン型の対話システムが存在する．</p>
<p>昔ながらのタスク指向型は，Natural Language Understanding, Dialogue State Tracking, Policy Learning, Natural Language Generationの4つからなっていた</p>
<p>⇒</p>
<p>最近のSoTAモデルでは，E2Eのタスク指向型の対話システムが多い．</p>
<p>オープンドメイン型</p>
<ul>
<li>generative systems
<ul>
<li>seq2seqなモデル</li>
<li>ユーザのメッセージや対話履歴を返答系列にマッピングする(Trainingデータに存在しないであろうものも含む)</li>
<li>柔軟でコンテクストを読んだ返答をするが，時々主張が一貫しない返答や鈍感で面白くない返答を返す．</li>
</ul>
</li>
<li>retrieval-based systems (検索)
<ul>
<li>返答の集合の中から，すでに存在する適した返答を探す．</li>
<li>表面上では良い返答をする．ただし，返答集合は有限集合なので，対話上のコンテクストに対しては関係性があまりみられないこともある．</li>
</ul>
</li>
<li>ensemble systems
<ul>
<li>上記二つを含む</li>
<li>Generatie systemsは検索システムをよくするために使われる．</li>
<li>検索システムはより適した返答を選ぶために使われる．</li>
</ul>
</li>
</ul>
<p>古典的な対話システムとして，finite state-basedとstatistical learningとmachine learning-basedが挙げられる．</p>
<ul>
<li>Finite State-based
<ul>
<li>対話の流れはあらかじめ決められている</li>
<li>決まったシナリオの中でしか対応ができない．</li>
</ul>
</li>
<li>Statistical Learning-based
<ul>
<li>Finite State-basedよりは柔軟である．あらかじめ対応が決められていないから．</li>
</ul>
</li>
<li>machine learning-based
<ul>
<li>Deep learningが主流？</li>
</ul>
</li>
</ul>
<p>NLPの中には対話システムに近い領域がある．</p>
<ul>
<li>Q &#x26; A</li>
<li>reading comprehension</li>
<li>dialogue disentanglement</li>
<li>visual dialogue</li>
<li>visual Q &#x26; A</li>
<li>dialogue reasoning</li>
<li>conversational semantic parsing</li>
<li>dialogue relation extraction</li>
<li>dialogue sentiment analysis</li>
<li>hate speech detection</li>
<li>MISC detection (???)</li>
</ul>
<h3>Neural Models in Dialogue Sustems</h3>
<ul>
<li>CNN
<ul>
<li>ここ数年NLPの分野での応用も多いらしい</li>
<li>フレーズや文章，パラグラフには意味づけをするのに有用でCNNがヒラルキーなモデルになる</li>
<li>CNNは一条に乏しいため，最近のSoTAにおいては，テキストをencoderにかけたのちにCNNを用いてヒエラルキーな特徴抽出を行っている．</li>
<li>欠点として入力系列の長さは固定長のため以下の使用例
<ul>
<li>encoderの出力をCNNでベクトル化</li>
<li>contextと返答の候補を行列にして，CNNで近さを図ることによって，妥当な候補を選び出す</li>
</ul>
</li>
<li>基本的にCNNとencoderはセットか？</li>
</ul>
</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/37cmjuij.png" alt=""></p>
<ul>
<li>RNN and Vanilla seq2seq
<ul>
<li>系列として扱えるのが利点と考えるべき</li>
<li>HMMや古典的な系列モデルだと，推論時のアルゴリズムの複雑さや考えるべき状態空間の増大に合わせて行列サイズが大きくなりすぎて，大きな状態空間を必要とするデータには対応しがたい．</li>
<li>マルコフモデルは限られた条件下においては強力なモデルになりうる．</li>
<li>RNNは最近では提案されないが，NLPタスクにおいては未だ現役として活躍することもある</li>
<li>Jordan-Type &#x26; Elman-Type RNN</li>
</ul>
</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ts6afg6g.png" alt=""></p>
<pre><code class="hljs language-graphql">	- Jordan-<span class="hljs-keyword">Type</span> RNN
</code></pre>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/mz0mr5oj.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">		-</span> 最新の隠れ層の状態は，Input<span class="hljs-emphasis">_tとOutput_</span>t-1による
<span class="hljs-code">			
</span>
<span class="hljs-bullet">	-</span> Elman-Type RNN
</code></pre>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/10bbby3m.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">		-</span> 最新の隠れ層の状態は，Input<span class="hljs-emphasis">_tとHidden_</span>t-1による
<span class="hljs-bullet">	-</span> いずれにしてもシンプルなRNNは勾配消失か勾配爆発が大抵おこる
<span class="hljs-code">	
</span>
<span class="hljs-bullet">-</span> LSTM
</code></pre>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/k21pyf3t.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">	-</span> Gates
<span class="hljs-bullet">		-</span> 入力ゲート
<span class="hljs-bullet">		-</span> 忘却ゲート
<span class="hljs-bullet">		-</span> 出力ゲート


<span class="hljs-bullet">-</span> GRU; Gated Recurrent Unit
</code></pre>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/fs4fug4f.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">	-</span> Gates
<span class="hljs-bullet">		-</span> 更新ゲート
<span class="hljs-bullet">		-</span> リセットゲート
<span class="hljs-bullet">	-</span> パラメータが少ないため，
<span class="hljs-bullet">		-</span> 早い
<span class="hljs-bullet">		-</span> 汎化性がみられる
<span class="hljs-bullet">	-</span> ただし，
<span class="hljs-bullet">		-</span> 大きなデータセットには対応しきれないこともある
<span class="hljs-code">		
</span>
<span class="hljs-bullet">-</span> Bi-directional RNN
</code></pre>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/vrdvputk.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">	-</span> 双方向を考慮したRNN
<span class="hljs-code">		
</span>
<span class="hljs-bullet">-</span> seq2seq; Encoder-Decoder model
<span class="hljs-bullet">	-</span> 初めは機械翻訳のために提案された手法
<span class="hljs-bullet">	-</span> Encoderにより入力系列をベクトル化，その隠れ状態をDecodeして生成することを目指す
</code></pre>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/56q5hqna.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">	-</span> Encode時
<span class="hljs-bullet">		-</span> t時刻のinputとt-1時刻のhiddenによって，t時刻のhiddenが決まる
<span class="hljs-bullet">	-</span> Decode時
<span class="hljs-bullet">		-</span> t時刻のhiddenとt-1時刻のoutputによって，t時刻のoutputをデコードする
<span class="hljs-bullet">	-</span> 入力系列と出力系列の長さが固定長である必要はない．
<span class="hljs-bullet">		-</span> その代わり，適応させる系列長と出力される系列長は同じになることは保証されない
</code></pre>
<ul>
<li>Hierarchical Recurrent Encoder-Decoder; HRED</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/4s0olxfm.png" alt=""></p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- コンテクストを理解するためのseq2seqモデル</span>
<span class="hljs-deletion">- クエリの履歴を理解する？</span>
<span class="hljs-deletion">- トークンレベルとターンレベルで学習する</span>
</code></pre>
<ul>
<li>Memory Networks</li>
<li>Attention and Transformer
<ul>
<li>Attention</li>
<li>Transformer
<ul>
<li>Muti-head Attention</li>
</ul>
</li>
</ul>
</li>
<li>Pointer Net and CopyNet
<ul>
<li>Pointer Net</li>
<li>CopyNet</li>
</ul>
</li>
<li>Deep RL and GANs
<ul>
<li>Deep Q-Networks</li>
<li>REINFORCE</li>
<li>GANs</li>
</ul>
</li>
<li>Knowledge Graph Augmented Neural Networks</li>
</ul>
<p>2章は途中から読むのやめた．使用されるネットワークよりも課題感の方が知りたい．</p>
<h3>タスク指向型対話システム</h3>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/y29vzu3h.png" alt=""></p>
<p>ドメインの決まったタスクにおいて特定の問題を解決する．</p>
<ul>
<li>Natural Language Understanding</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/1mx5wsm3.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">-</span> 3つのタスクを持つ
<span class="hljs-bullet">	-</span> ドメイン分類
<span class="hljs-bullet">	-</span> 意図の理解
<span class="hljs-bullet">	-</span> スロット埋め
<span class="hljs-bullet">-</span> IOB; Inside Outside Beginning
<span class="hljs-bullet">-</span> NER; Named Entity Recognition
<span class="hljs-bullet">-</span> intent detectionにおいては，Task-Oriented Dialogue BERTがSoTA?
<span class="hljs-bullet">-</span> Domain classification &#x26; intent detectionは同カテゴリタスク


<span class="hljs-bullet">-</span> slot filling task = semantic tagging
<span class="hljs-bullet">-</span> NLUタスクを解く際に，音声データをそのままInputとして与える研究事例も出ているらしい
<span class="hljs-bullet">	-</span> エラーが少なくロバストなモデルになったらしい？
<span class="hljs-bullet">-</span> Natural Language UnderstandingとNatural Language Generationは逆のプロセスをふむ
<span class="hljs-bullet">	-</span> 同時にタスクを学習結果が得られるというアプローチも
</code></pre>
<ul>
<li>Dialogue State Tracking</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/aao7x0r4.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">-</span> ユーザの目的と対話履歴を追跡する
<span class="hljs-bullet">-</span> NLUとDSTのタスクは近い関係にある．
<span class="hljs-bullet">-</span> NLUは単語にtagを割り振っていくイメージ
<span class="hljs-bullet">-</span> DSTはtagのplaceholderを会話の内容から埋めていくイメージ
<span class="hljs-bullet">-</span> Dialogue Stateには3つの要素からなる
<span class="hljs-bullet">	-</span> Goal constraint corresponding with informable slots
<span class="hljs-bullet">		-</span> 特別なvalueの制約で，ユーザによって言及されるか特別な値をとる
<span class="hljs-bullet">		-</span> DontcareやNoneが特別な値にあたる
<span class="hljs-bullet">	-</span> Requested slots
<span class="hljs-bullet">	-</span> Search method of current turn
<span class="hljs-bullet">-</span> 古典的な手法でいくと，
<span class="hljs-bullet">	-</span> ルールベースはエラーが多く，ドメイン適応が大変
<span class="hljs-bullet">	-</span> 統計的手法はノイジーな状態や曖昧性に弱い
<span class="hljs-bullet">-</span> ニューラルネットな手法
<span class="hljs-bullet">	-</span> slot-valueのペアを事前定義して学習
<span class="hljs-bullet">		-</span> valueが大きくなると複雑性が増す
<span class="hljs-bullet">		-</span> slot-valueのペアを読むだけでよく，2値分類タスクとして解ける
<span class="hljs-bullet">		-</span> モデルの複雑性は避けられるが，反応速度が遅くなる可能性がある．
<span class="hljs-bullet">	-</span> slot-valueのペアを定義せずに，対話の中から直接選ぶ
</code></pre>
<ul>
<li>
<p>Policy Learning</p>
<ul>
<li>DSTモジュールの出力結果からどう行動をとるか</li>
<li>教師あり学習or 強化学習</li>
<li>教師ありだとアノテショーンデータセットを作るのがとても大変</li>
</ul>
</li>
<li>
<p>Natural Language Generation; NLG</p>
<ul>
<li>タスク指向型対話システムにおける最終層のモジュール</li>
<li>最終的な自然言語表現を生成するシステム</li>
<li>4つのコンポーネントからなる</li>
</ul>
</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/9ybi8yfr.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">	-</span> Content Determination
<span class="hljs-bullet">	-</span> Sentence Planning
<span class="hljs-bullet">	-</span> Surface Realization
<span class="hljs-bullet">	-</span> Lexicalization, Referring expression, aggregation
<span class="hljs-bullet">-</span> RNNに基づいた統計言語モデルにおいて，意味的制約や文法構造による返答生成を行うなど
<span class="hljs-bullet">-</span> コンテクストを理解した返答を生成することは重要である
<span class="hljs-bullet">-</span> タスク指向型においては，返答の多様性というよりも信頼性のほうが重要視されがち
<span class="hljs-bullet">-</span> 意味解析をビームサーチを使うことで，意味の正しさを改善する手法も提案された
</code></pre>
<ul>
<li>E2E Methods
<ul>
<li>
<p>end-to-endのパイプラインを組むことで高いパフォーマンスを発揮することがあるが，</p>
</li>
<li>
<p>多くのモジュールを組み込むため，バックプロパゲーションで誤差が伝播しないこともある．</p>
</li>
<li>
<p>すべてのモジュールが，返答の精度を向上するために，対等に重要であるとは限らない．</p>
</li>
<li>
<p>違うドメインに差し替えるとき，オントロジーを事前学習させる必要があるため，困難が生じることもある</p>
</li>
<li>
<p>やり方は大きく分けて2つ</p>
<ul>
<li>すべてのモジュールを展開して誤差逆伝播させる？</li>
<li>知識ベースの検索システムと返答生成の双方を用いてパイプラインを組む</li>
</ul>
</li>
<li>
<p>タスク指向型においては，外部の知識源が必要なことが多い</p>
</li>
</ul>
</li>
</ul>
<h3>オープンドメイン型対話システム</h3>
<ul>
<li>雑談対話システム，或いはタスク思考型ではない対話システムのこと</li>
<li>SoTAを示しているオープンドメインは大抵ニューラルネットで解決している</li>
<li>完全なるデータドリブンなものが多い</li>
<li>オープンドメイン型対話システムは，大まか3つに分けられる
<ul>
<li>生成システム</li>
<li>検索ベースシステム</li>
<li>アンサンブルシステム</li>
</ul>
</li>
</ul>
<p>３つの話が以下</p>
<ul>
<li>生成システム
<ul>
<li>訓練コーパスに出てこないような返答に対して，ユーザのメッセージや対話履歴をマッピングするために，seq2seqなモデルを適用する</li>
</ul>
</li>
<li>検索システム
<ul>
<li>決まった返答集合の中からすでに存在する返答を探そうとする</li>
</ul>
</li>
<li>アンサンブルシステム
<ul>
<li>生成手法と検索手法を合わせる．</li>
<li>生成された返答と検索された返答とを比べる．</li>
<li>生成も，検索された返答を洗練するために用いられる．</li>
</ul>
</li>
</ul>
<p>特徴として，</p>
<p>生成モデルは</p>
<p>柔軟でコンテクストを読んだ返答をできるが，ときには理解に欠けていたり，怠けた返答を見せることがある</p>
<p>検索ベースのモデルは</p>
<p>人の返答の集合から実際の返答を選ぶため，返答の集合は有限集合であり，コンテクストと相関がないことがある．</p>
<p>ただし，表面上のレベルでは，首尾一貫した返答することも多い</p>
<p>以下は，オープンドメイン型対話システムにおける，難しさとホットなトピックをまとめる</p>
<ul>
<li>Context Awareness
<ul>
<li>対話コンテクストは会話のトピックを決定したり，ユーザの目標を決定したりと重要</li>
<li>コンテクストを解釈した対話エージェントは，現メッセージだけではなく，対話履歴からももとにして返答する</li>
<li>生成モデルも検索ベースも，どちらも対話コンテクストモデリングに依存する</li>
<li>いくつかのモデルではAttentionが使用されているらしい</li>
<li>構造化されたAttentionを用いることでコンテクストを読み取れる？</li>
<li>対話をリライトする問題があるらしい
<ul>
<li>複数のメッセージから単一のメッセージに変換する目標</li>
<li>ここではコンテクストを理解させることが重要</li>
</ul>
</li>
</ul>
</li>
<li>Response Coherence
<ul>
<li>首尾一貫した返答は，良い生成器としての一つのクオリティ</li>
<li>対話の中で，論理的で首尾一貫しているか？という指標</li>
<li>生成モデルにおいてホットなトピックとなっている（検索ベースはすでに人の返答をりようするのでもともと一貫性はあるという主張）</li>
<li>一貫性のない文の順序を見つけるタスクを解くことで，返答の一貫性を改善した事例もあり</li>
</ul>
</li>
<li>Response Diversity
<ul>
<li>人が多用するような表現は訓練コーパスにも多く含まれ，それらばかりを返答してしまうことが問題となりうる</li>
<li>かつては条件付き確率において，尤度関数を解くことで尤もらしい返答をもとめていた．</li>
</ul>
</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/00fwhths.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">	-</span> この手法では，返答の精度の安全性と適切さはトレードオフになっていた？
<span class="hljs-bullet">-</span> ビームサーチを提案されたことも
<span class="hljs-bullet">-</span> 
</code></pre>
<ul>
<li>Speaker Consistency and Personality-based Response
<ul>
<li>システムは，訓練コーパスからサンプリングされた分布に対して学習
<ul>
<li>対話者の趣味といった一貫性のないものに対する返答は．．．</li>
<li>対話者の役割を理解し，その個人に合わせた返答が必要になる</li>
</ul>
</li>
<li>1ステージではなく，3ステージで個人的な嗜好に対応した事例がある</li>
<li></li>
</ul>
</li>
<li>Empathetic Response
<ul>
<li>同情する対話システムは，ユーザの感情の変化や感情に伴った適切な返答をする</li>
<li>雑談チャットについて，このトピックは重要</li>
<li>CortanaやAlexaなどの製品にもモジュールが含まれている</li>
<li>CoBERTのモデルなど</li>
<li>感情対話システムのデータセットはとぼしいが，新たなデータセットとベンチマークが提供されたらしい</li>
<li></li>
</ul>
</li>
<li>Conversation Topics
<ul>
<li>トピックや目的は，会話に参加した人と会話を続けるための重要な役割を果たす</li>
<li>トピックを理解させることが重要</li>
<li></li>
</ul>
</li>
<li>Knowledge-Grounded System
<ul>
<li>人は，会話のコンテクストと経験や記憶といったものとを関連付けて，返答をする（機会には難しい）</li>
<li>生成モデルは，単なる機械翻訳よりも複雑
<ul>
<li>より自由度が高く，制約が曖昧なため</li>
</ul>
</li>
<li>故に，雑談チャットは，外部から得られる常識と結びつけて，seq2seqなモデルによって生成する</li>
<li>メモリーネットワークなどで，知識をグラウンディングする手法</li>
<li>知識グラフは外部の情報をソースにするものもある．</li>
<li>graph attentionを用いて，常識をグラフベースで学習する手法も</li>
<li>主な考え方は，外部の知識グラフを使って，会話の論理の流れをモデリングする指標の一部として扱う</li>
</ul>
</li>
<li>Interactive Training
<ul>
<li>別名；human-in-loop training</li>
<li>アノテーションされたデータセットは限られている
<ul>
<li>すべての状況をカバーすることは不可能</li>
</ul>
</li>
<li>ユーザとの対話の中で，システムを改善する</li>
<li>強化学習における逐次学習を提案</li>
<li>対話相手と話して，その相手からフィードバックを得る</li>
<li>教師あり学習をした後，Interactive Trainingによってファインチューニングする</li>
<li></li>
</ul>
</li>
<li>Visual Dialogue</li>
</ul>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/v3baquyk.png" alt=""></p>
<pre><code class="hljs language-markdown"><span class="hljs-bullet">-</span> Visual Q &#x26; Aなど
<span class="hljs-bullet">-</span> 画像あり対話システムのほか，映像あり対話システムも面白いトピックだが難題でもある
<span class="hljs-bullet">	-</span> 特徴量抽出の複雑さも増す
<span class="hljs-bullet">-</span> visual dialogueのアノテーションは重労働であり，データセットに乏しいので，現在はデータの不十分さに悩まされている
<span class="hljs-bullet">-</span> 
</code></pre>
<h3>評価のアプローチ</h3>
<p>評価の仕方も重要なパートとなっている</p>
<ul>
<li>タスク指向型対話システムにおける評価
<ul>
<li>BLEUスコアを用いて，システムの返答と人の返答を比べるなど</li>
<li>Task Completion Rate
<ul>
<li>すべてのタスクの試行に対して，いくつのイベントが成功したかの割合</li>
</ul>
</li>
<li>Task Completion Cost
<ul>
<li>タスクをこなすのに使われたリソース</li>
<li>解決までの時間が重視されるタスクにおいて用いられる</li>
<li>なるべく短いターン数で完遂するのが良しとされる</li>
</ul>
</li>
<li>Human-based Evaluation
<ul>
<li>ユーザの対話とユーザの満足度のスコアを提供</li>
<li>方法はふたつ
<ul>
<li>クラウドソーシングで労働を雇う</li>
<li>実際にローンチしてからユーザのフィードバックで評価する</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>オープンドメイン型対話システムにおける評価
<ul>
<li>明確なメトリックはない</li>
<li>長らくHuman Evaluationを使ってきた</li>
<li>Word-overlap Metrics
<ul>
<li>生成された系列と実際の系列の近さを計算する</li>
<li>機械翻訳や要約タスクにおいて用いられる</li>
<li>n-gramのものとして
<ul>
<li>BLEU</li>
<li>ROUGE</li>
<li>METEOR(BLUEの改良版)</li>
</ul>
</li>
</ul>
</li>
<li>Neural Metrics
<ul>
<li>ニューラルモデルによって計算させる</li>
<li>RNNやCNN,GANの識別器を使うなどして，ターンレベルの特徴量抽出を行うなど</li>
</ul>
</li>
<li>今もホットなトピックになっている</li>
</ul>
</li>
</ul>
<h3>データセット</h3>
<p>タスク指向型対話システム</p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/4xcx432d.png" alt=""></p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/bvaa6edt.png" alt=""></p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/5nqvspiq.png" alt=""></p>
<p>オープンドメイン型対話システム</p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ja1g5vzq.png" alt=""></p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ki5ab45o.png" alt=""></p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ikzadi1i.png" alt=""></p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/nuxspw6o.png" alt=""></p>
<p><img src="/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/1u3frgui.png" alt=""></p>
<h3>結論とトレンド</h3>
<p>ココ最近のトレンド</p>
<ul>
<li>Mutlimodal dialogue systems
<ul>
<li>異なるモダリティを組み合わせる</li>
</ul>
</li>
<li>Multitask dialogue systems
<ul>
<li>タスク指向型と知識グラウンディングさせたオープンドメイン型を組み合わせて，一つのフレームワークまたはシングルモデルとして完結させる</li>
</ul>
</li>
<li>Corpus exploration on Internet
<ul>
<li>real-timeなコーパスをインターネットから取り出せるようになれば，期待がもてる</li>
<li>研究に値するのでは？</li>
</ul>
</li>
<li>User modeling
<ul>
<li>生成と評価の双方でホットなトピック</li>
</ul>
</li>
<li>Dialogue generation with a long-term goal
<ul>
<li>日常的な雑談は特に目的はない</li>
<li>しかし，会話が意図的にある特定の目的に向かうときは，ほんの少しでも状況があるはず</li>
<li>現在のオープンドメイン型は，長期的な目的を除いてモデリングされがち
<ul>
<li>十分な知性を備えていない</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>引用</h2>
<blockquote>
</blockquote></article><div class="socialshare_container__SSXJE"><h3>タメになったらSHARE!!!</h3><div class="socialshare_container__links__JZs4j"><a target="_blank" href="https://twitter.com/share?url=https://yuta0306.github.io/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey"><img src="/icons/twitter.png" loading="lazy" alt="https://yuta0306.github.io/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-SurveyをTwitterに共有する"/></a><a target="_blank" href="https://www.facebook.com/share.php?u=https://yuta0306.github.io/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey"><img src="/icons/facebook.png" loading="lazy" alt="https://yuta0306.github.io/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-SurveyをFacebookに共有する"/></a><a target="_blank" href="http://b.hatena.ne.jp/entry/https:/yuta0306.github.io/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey"><img src="/icons/hatenablog.png" loading="lazy" alt="https://yuta0306.github.io/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Surveyをはてなブログに共有する"/></a></div></div></div></div></main><aside class="main_sidebar__tM28d"><div class="shortbio_container__4psan" itemscope="" itemProp="author" itemType="http://schema.org/Person"><div class="shortbio_container__image__eljVd"><img src="/images/profile.jpeg" alt="ゆうぼう" loading="lazy"/></div><h3 class="shortbio_author__A2bKB" itemscope="" itemProp="name">ゆうぼう</h3><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">国立大学院M1のナマケモノです．</p></div><div><p class="shortbio_container__paragraph__EJbWG">human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．</p></div><div><p class="shortbio_container__paragraph__EJbWG">学部時代はコモンセンスを利用したユーモア検出の研究を行っていました(Knowledge-intensive NLP)．</p></div><div><p class="shortbio_container__paragraph__EJbWG">このブログはNext.jsで書いてます．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">Kaggle等のデータ分析コンペは活動休止中．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div></div><div class="followme_container__T1oVi"><h3 class="followme_container__header__Pt5SP">Follow Me</h3><div class="followme_container__links__b3XW5"><a target="_blank" href="https://github.com/yuta0306"><img src="/icons/github.png" alt="GitHub"/></a><a target="_blank" href="https://kaggle.com/yutasasaki"><img src="/icons/kaggle.png" alt="Kaggle"/></a><a target="_blank" href="https://twitter.com/Sloth65557166"><img src="/icons/twitter.png" alt="Twitter"/></a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="categories_container__J8nCF"><h3 class="categories_container__header__zt836">Categories</h3><div class="categories_container__links__MFrVK"><a class="categories_container__link__AuvVr" href="/category/%E8%AB%96%E6%96%87/1">論文</a><a class="categories_container__link__AuvVr" href="/category/Web/1">Web</a><a class="categories_container__link__AuvVr" href="/category/JavaScript/1">JavaScript</a><a class="categories_container__link__AuvVr" href="/category/Competition/1">Competition</a><a class="categories_container__link__AuvVr" href="/category/Cloud/1">Cloud</a><a class="categories_container__link__AuvVr" href="/category/Linux/1">Linux</a><a class="categories_container__link__AuvVr" href="/category/Python/1">Python</a><a class="categories_container__link__AuvVr" href="/category/ML/1">ML</a><a class="categories_container__link__AuvVr" href="/category/Go/1">Go</a><a class="categories_container__link__AuvVr" href="/category/SQL/1">SQL</a></div></div><div class="tags_container___e3ez"><h3 class="tags_container__header__hxPW8">Tags</h3><div class="tags_container__links__X38Ga"><a class="tags_container__link__1Ts3a" href="/tag/Apache/1">Apache</a><a class="tags_container__link__1Ts3a" href="/tag/Appium/1">Appium</a><a class="tags_container__link__1Ts3a" href="/tag/atmaCup/1">atmaCup</a><a class="tags_container__link__1Ts3a" href="/tag/AWS/1">AWS</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS7/1">CentOS7</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS8/1">CentOS8</a><a class="tags_container__link__1Ts3a" href="/tag/Colab/1">Colab</a><a class="tags_container__link__1Ts3a" href="/tag/COMET/1">COMET</a><a class="tags_container__link__1Ts3a" href="/tag/commonsense/1">commonsense</a><a class="tags_container__link__1Ts3a" href="/tag/conda/1">conda</a><a class="tags_container__link__1Ts3a" href="/tag/Contrasive%20Learning/1">Contrasive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/Contrastive%20Learning/1">Contrastive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/CSS/1">CSS</a><a class="tags_container__link__1Ts3a" href="/tag/Dialogue%20Structure%20Learning/1">Dialogue Structure Learning</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system/1">dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/DST/1">DST</a><a class="tags_container__link__1Ts3a" href="/tag/Emotion%20Recognition/1">Emotion Recognition</a><a class="tags_container__link__1Ts3a" href="/tag/empathetic%20dialogue%20system/1">empathetic dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/encyclopedic/1">encyclopedic</a><a class="tags_container__link__1Ts3a" href="/tag/Error%20Correction/1">Error Correction</a><a class="tags_container__link__1Ts3a" href="/tag/ESPNet/1">ESPNet</a><a class="tags_container__link__1Ts3a" href="/tag/ffmpeg/1">ffmpeg</a><a class="tags_container__link__1Ts3a" href="/tag/Flask/1">Flask</a><a class="tags_container__link__1Ts3a" href="/tag/Gating%20Mechanism/1">Gating Mechanism</a><a class="tags_container__link__1Ts3a" href="/tag/Go/1">Go</a><a class="tags_container__link__1Ts3a" href="/tag/Google%20Colaboratory/1">Google Colaboratory</a><a class="tags_container__link__1Ts3a" href="/tag/Heroku/1">Heroku</a><a class="tags_container__link__1Ts3a" href="/tag/Highway%20Transformer/1">Highway Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/HTML/1">HTML</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection/1">humor detection</a><a class="tags_container__link__1Ts3a" href="/tag/Intent%20Classification/1">Intent Classification</a><a class="tags_container__link__1Ts3a" href="/tag/Internet-Augmented/1">Internet-Augmented</a><a class="tags_container__link__1Ts3a" href="/tag/JavaScript/1">JavaScript</a><a class="tags_container__link__1Ts3a" href="/tag/JSON/1">JSON</a><a class="tags_container__link__1Ts3a" href="/tag/Kaggle/1">Kaggle</a><a class="tags_container__link__1Ts3a" href="/tag/KC-Net/1">KC-Net</a><a class="tags_container__link__1Ts3a" href="/tag/knowledge-base/1">knowledge-base</a><a class="tags_container__link__1Ts3a" href="/tag/Knowledge-Intensive%20NLP/1">Knowledge-Intensive NLP</a><a class="tags_container__link__1Ts3a" href="/tag/laughter/1">laughter</a><a class="tags_container__link__1Ts3a" href="/tag/Linux/1">Linux</a><a class="tags_container__link__1Ts3a" href="/tag/Mac/1">Mac</a><a class="tags_container__link__1Ts3a" href="/tag/make/1">make</a><a class="tags_container__link__1Ts3a" href="/tag/map/1">map</a><a class="tags_container__link__1Ts3a" href="/tag/MeCab/1">MeCab</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20health/1">mental health</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20state%20knowledge/1">mental state knowledge</a><a class="tags_container__link__1Ts3a" href="/tag/mentalisation/1">mentalisation</a><a class="tags_container__link__1Ts3a" href="/tag/MentalRoBERTa/1">MentalRoBERTa</a><a class="tags_container__link__1Ts3a" href="/tag/Merging%20Models/1">Merging Models</a><a class="tags_container__link__1Ts3a" href="/tag/ML/1">ML</a><a class="tags_container__link__1Ts3a" href="/tag/Model%20Editing/1">Model Editing</a><a class="tags_container__link__1Ts3a" href="/tag/Model%20Patching/1">Model Patching</a><a class="tags_container__link__1Ts3a" href="/tag/MT/1">MT</a><a class="tags_container__link__1Ts3a" href="/tag/Multi-Hop%20Transformer/1">Multi-Hop Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/multi-modal/1">multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/MySQL/1">MySQL</a><a class="tags_container__link__1Ts3a" href="/tag/NLG/1">NLG</a><a class="tags_container__link__1Ts3a" href="/tag/NLI/1">NLI</a><a class="tags_container__link__1Ts3a" href="/tag/NLP/1">NLP</a><a class="tags_container__link__1Ts3a" href="/tag/Node/1">Node</a><a class="tags_container__link__1Ts3a" href="/tag/node.js/1">node.js</a><a class="tags_container__link__1Ts3a" href="/tag/npm/1">npm</a><a class="tags_container__link__1Ts3a" href="/tag/Pandas/1">Pandas</a><a class="tags_container__link__1Ts3a" href="/tag/persona/1">persona</a><a class="tags_container__link__1Ts3a" href="/tag/PLMKE/1">PLMKE</a><a class="tags_container__link__1Ts3a" href="/tag/Poetry/1">Poetry</a><a class="tags_container__link__1Ts3a" href="/tag/Prompt-Tuning/1">Prompt-Tuning</a><a class="tags_container__link__1Ts3a" href="/tag/Python/1">Python</a><a class="tags_container__link__1Ts3a" href="/tag/Pytorch/1">Pytorch</a><a class="tags_container__link__1Ts3a" href="/tag/pytorch-lightning/1">pytorch-lightning</a><a class="tags_container__link__1Ts3a" href="/tag/Scikit-learn/1">Scikit-learn</a><a class="tags_container__link__1Ts3a" href="/tag/Selenium/1">Selenium</a><a class="tags_container__link__1Ts3a" href="/tag/Self-Dependency-Units%20(SDU)/1">Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/shared%20laughter/1">shared laughter</a><a class="tags_container__link__1Ts3a" href="/tag/SISR/1">SISR</a><a class="tags_container__link__1Ts3a" href="/tag/SLU/1">SLU</a><a class="tags_container__link__1Ts3a" href="/tag/Speech%20Disfluency/1">Speech Disfluency</a><a class="tags_container__link__1Ts3a" href="/tag/subprocess/1">subprocess</a><a class="tags_container__link__1Ts3a" href="/tag/Super-Resolution/1">Super-Resolution</a><a class="tags_container__link__1Ts3a" href="/tag/survey/1">survey</a><a class="tags_container__link__1Ts3a" href="/tag/tensorflow/1">tensorflow</a><a class="tags_container__link__1Ts3a" href="/tag/Tkinter/1">Tkinter</a><a class="tags_container__link__1Ts3a" href="/tag/Transfer%20Learning/1">Transfer Learning</a><a class="tags_container__link__1Ts3a" href="/tag/transformer/1">transformer</a><a class="tags_container__link__1Ts3a" href="/tag/Weight%20Interpolation/1">Weight Interpolation</a><a class="tags_container__link__1Ts3a" href="/tag/zsh/1">zsh</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91/1">オブジェクト指向</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%82%B3%E3%83%AC%E3%83%BC%E3%82%BF/1">デコレータ</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90/1">データ分析</a><a class="tags_container__link__1Ts3a" href="/tag/%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/1">特殊メソッド</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%81%9E%E3%81%8D%E6%89%8B%E5%8F%8D%E5%BF%9C/1">聞き手反応</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%B6%85%E8%A7%A3%E5%83%8F/1">超解像</a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div id="TOC"></div></aside></div></div><footer class="footer_footer__WCChH"><div class="footer_footer__inner__287VQ"><div><a class="footer_footer__link__Ql5Ng" href="/privacy-policy">プライバシーポリシー</a></div><div class="footer_footer__title__PRn_u"><a href="/">ゆうぼうの書跡棚</a></div><div class="footer_footer__small__RlIHP"><small>Powered by <a target="_blank" class="footer_footer__small__link__u5kuV" href="https://twitter.com/Sloth65557166">ゆうぼう</a></small></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey\u003c/p\u003e\n\u003cp\u003e研究会: arxiv\u003c/p\u003e\n\u003cp\u003e年度: 2021\u003c/p\u003e\n\u003cp\u003eキーワード: survey, dialogue system\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://arxiv.org/pdf/2105.04387.pdf\"\u003ehttps://arxiv.org/pdf/2105.04387.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ry2fz8tn.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e対話システムに関するサーベイ論文\u003c/p\u003e\n\u003cp\u003e対話システムはNLPタスクの一種\u003c/p\u003e\n\u003cp\u003e研究の価値が高いNLPタスクを多く含むため，対話システムは複雑と言える．\u003c/p\u003e\n\u003cp\u003eここ最近で良い成果をあげているもののほとんどがDL\u003c/p\u003e\n\u003cp\u003eメインは，モデルタイプとシステムタイプについて述べられる．\u003c/p\u003e\n\u003cp\u003eシステムタイプ\u003c/p\u003e\n\u003cp\u003eタスク指向型\u003c/p\u003e\n\u003cp\u003eオープンドメイン型\u003c/p\u003e\n\u003ch3\u003eKeywords\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/8575dpgt.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eサーベイの主張の流れ\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/hpk33ao6.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003ch3\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003e対話システムはNLPにおいてホットな話題であり，産業においても需要が非常に高い．\u003c/p\u003e\n\u003cp\u003eタスク指向型とオープンドメイン型の対話システムが存在する．\u003c/p\u003e\n\u003cp\u003e昔ながらのタスク指向型は，Natural Language Understanding, Dialogue State Tracking, Policy Learning, Natural Language Generationの4つからなっていた\u003c/p\u003e\n\u003cp\u003e⇒\u003c/p\u003e\n\u003cp\u003e最近のSoTAモデルでは，E2Eのタスク指向型の対話システムが多い．\u003c/p\u003e\n\u003cp\u003eオープンドメイン型\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egenerative systems\n\u003cul\u003e\n\u003cli\u003eseq2seqなモデル\u003c/li\u003e\n\u003cli\u003eユーザのメッセージや対話履歴を返答系列にマッピングする(Trainingデータに存在しないであろうものも含む)\u003c/li\u003e\n\u003cli\u003e柔軟でコンテクストを読んだ返答をするが，時々主張が一貫しない返答や鈍感で面白くない返答を返す．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eretrieval-based systems (検索)\n\u003cul\u003e\n\u003cli\u003e返答の集合の中から，すでに存在する適した返答を探す．\u003c/li\u003e\n\u003cli\u003e表面上では良い返答をする．ただし，返答集合は有限集合なので，対話上のコンテクストに対しては関係性があまりみられないこともある．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eensemble systems\n\u003cul\u003e\n\u003cli\u003e上記二つを含む\u003c/li\u003e\n\u003cli\u003eGeneratie systemsは検索システムをよくするために使われる．\u003c/li\u003e\n\u003cli\u003e検索システムはより適した返答を選ぶために使われる．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e古典的な対話システムとして，finite state-basedとstatistical learningとmachine learning-basedが挙げられる．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFinite State-based\n\u003cul\u003e\n\u003cli\u003e対話の流れはあらかじめ決められている\u003c/li\u003e\n\u003cli\u003e決まったシナリオの中でしか対応ができない．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStatistical Learning-based\n\u003cul\u003e\n\u003cli\u003eFinite State-basedよりは柔軟である．あらかじめ対応が決められていないから．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003emachine learning-based\n\u003cul\u003e\n\u003cli\u003eDeep learningが主流？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNLPの中には対話システムに近い領域がある．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQ \u0026#x26; A\u003c/li\u003e\n\u003cli\u003ereading comprehension\u003c/li\u003e\n\u003cli\u003edialogue disentanglement\u003c/li\u003e\n\u003cli\u003evisual dialogue\u003c/li\u003e\n\u003cli\u003evisual Q \u0026#x26; A\u003c/li\u003e\n\u003cli\u003edialogue reasoning\u003c/li\u003e\n\u003cli\u003econversational semantic parsing\u003c/li\u003e\n\u003cli\u003edialogue relation extraction\u003c/li\u003e\n\u003cli\u003edialogue sentiment analysis\u003c/li\u003e\n\u003cli\u003ehate speech detection\u003c/li\u003e\n\u003cli\u003eMISC detection (???)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eNeural Models in Dialogue Sustems\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCNN\n\u003cul\u003e\n\u003cli\u003eここ数年NLPの分野での応用も多いらしい\u003c/li\u003e\n\u003cli\u003eフレーズや文章，パラグラフには意味づけをするのに有用でCNNがヒラルキーなモデルになる\u003c/li\u003e\n\u003cli\u003eCNNは一条に乏しいため，最近のSoTAにおいては，テキストをencoderにかけたのちにCNNを用いてヒエラルキーな特徴抽出を行っている．\u003c/li\u003e\n\u003cli\u003e欠点として入力系列の長さは固定長のため以下の使用例\n\u003cul\u003e\n\u003cli\u003eencoderの出力をCNNでベクトル化\u003c/li\u003e\n\u003cli\u003econtextと返答の候補を行列にして，CNNで近さを図ることによって，妥当な候補を選び出す\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e基本的にCNNとencoderはセットか？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/37cmjuij.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRNN and Vanilla seq2seq\n\u003cul\u003e\n\u003cli\u003e系列として扱えるのが利点と考えるべき\u003c/li\u003e\n\u003cli\u003eHMMや古典的な系列モデルだと，推論時のアルゴリズムの複雑さや考えるべき状態空間の増大に合わせて行列サイズが大きくなりすぎて，大きな状態空間を必要とするデータには対応しがたい．\u003c/li\u003e\n\u003cli\u003eマルコフモデルは限られた条件下においては強力なモデルになりうる．\u003c/li\u003e\n\u003cli\u003eRNNは最近では提案されないが，NLPタスクにおいては未だ現役として活躍することもある\u003c/li\u003e\n\u003cli\u003eJordan-Type \u0026#x26; Elman-Type RNN\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ts6afg6g.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-graphql\"\u003e\t- Jordan-\u003cspan class=\"hljs-keyword\"\u003eType\u003c/span\u003e RNN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/mz0mr5oj.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 最新の隠れ層の状態は，Input\u003cspan class=\"hljs-emphasis\"\u003e_tとOutput_\u003c/span\u003et-1による\n\u003cspan class=\"hljs-code\"\u003e\t\t\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Elman-Type RNN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/10bbby3m.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 最新の隠れ層の状態は，Input\u003cspan class=\"hljs-emphasis\"\u003e_tとHidden_\u003c/span\u003et-1による\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e いずれにしてもシンプルなRNNは勾配消失か勾配爆発が大抵おこる\n\u003cspan class=\"hljs-code\"\u003e\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e LSTM\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/k21pyf3t.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Gates\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 入力ゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 忘却ゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 出力ゲート\n\n\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e GRU; Gated Recurrent Unit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/fs4fug4f.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Gates\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 更新ゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e リセットゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e パラメータが少ないため，\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 早い\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 汎化性がみられる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e ただし，\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 大きなデータセットには対応しきれないこともある\n\u003cspan class=\"hljs-code\"\u003e\t\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Bi-directional RNN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/vrdvputk.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 双方向を考慮したRNN\n\u003cspan class=\"hljs-code\"\u003e\t\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e seq2seq; Encoder-Decoder model\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 初めは機械翻訳のために提案された手法\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Encoderにより入力系列をベクトル化，その隠れ状態をDecodeして生成することを目指す\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/56q5hqna.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Encode時\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e t時刻のinputとt-1時刻のhiddenによって，t時刻のhiddenが決まる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Decode時\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e t時刻のhiddenとt-1時刻のoutputによって，t時刻のoutputをデコードする\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 入力系列と出力系列の長さが固定長である必要はない．\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e その代わり，適応させる系列長と出力される系列長は同じになることは保証されない\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eHierarchical Recurrent Encoder-Decoder; HRED\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/4s0olxfm.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-diff\"\u003e\u003cspan class=\"hljs-deletion\"\u003e- コンテクストを理解するためのseq2seqモデル\u003c/span\u003e\n\u003cspan class=\"hljs-deletion\"\u003e- クエリの履歴を理解する？\u003c/span\u003e\n\u003cspan class=\"hljs-deletion\"\u003e- トークンレベルとターンレベルで学習する\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eMemory Networks\u003c/li\u003e\n\u003cli\u003eAttention and Transformer\n\u003cul\u003e\n\u003cli\u003eAttention\u003c/li\u003e\n\u003cli\u003eTransformer\n\u003cul\u003e\n\u003cli\u003eMuti-head Attention\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePointer Net and CopyNet\n\u003cul\u003e\n\u003cli\u003ePointer Net\u003c/li\u003e\n\u003cli\u003eCopyNet\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDeep RL and GANs\n\u003cul\u003e\n\u003cli\u003eDeep Q-Networks\u003c/li\u003e\n\u003cli\u003eREINFORCE\u003c/li\u003e\n\u003cli\u003eGANs\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKnowledge Graph Augmented Neural Networks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2章は途中から読むのやめた．使用されるネットワークよりも課題感の方が知りたい．\u003c/p\u003e\n\u003ch3\u003eタスク指向型対話システム\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/y29vzu3h.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eドメインの決まったタスクにおいて特定の問題を解決する．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNatural Language Understanding\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/1mx5wsm3.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 3つのタスクを持つ\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e ドメイン分類\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 意図の理解\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e スロット埋め\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e IOB; Inside Outside Beginning\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NER; Named Entity Recognition\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e intent detectionにおいては，Task-Oriented Dialogue BERTがSoTA?\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Domain classification \u0026#x26; intent detectionは同カテゴリタスク\n\n\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e slot filling task = semantic tagging\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NLUタスクを解く際に，音声データをそのままInputとして与える研究事例も出ているらしい\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e エラーが少なくロバストなモデルになったらしい？\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Natural Language UnderstandingとNatural Language Generationは逆のプロセスをふむ\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 同時にタスクを学習結果が得られるというアプローチも\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eDialogue State Tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/aao7x0r4.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e ユーザの目的と対話履歴を追跡する\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NLUとDSTのタスクは近い関係にある．\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NLUは単語にtagを割り振っていくイメージ\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e DSTはtagのplaceholderを会話の内容から埋めていくイメージ\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Dialogue Stateには3つの要素からなる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Goal constraint corresponding with informable slots\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 特別なvalueの制約で，ユーザによって言及されるか特別な値をとる\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e DontcareやNoneが特別な値にあたる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Requested slots\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Search method of current turn\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 古典的な手法でいくと，\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e ルールベースはエラーが多く，ドメイン適応が大変\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 統計的手法はノイジーな状態や曖昧性に弱い\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e ニューラルネットな手法\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e slot-valueのペアを事前定義して学習\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e valueが大きくなると複雑性が増す\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e slot-valueのペアを読むだけでよく，2値分類タスクとして解ける\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e モデルの複雑性は避けられるが，反応速度が遅くなる可能性がある．\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e slot-valueのペアを定義せずに，対話の中から直接選ぶ\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePolicy Learning\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDSTモジュールの出力結果からどう行動をとるか\u003c/li\u003e\n\u003cli\u003e教師あり学習or 強化学習\u003c/li\u003e\n\u003cli\u003e教師ありだとアノテショーンデータセットを作るのがとても大変\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNatural Language Generation; NLG\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eタスク指向型対話システムにおける最終層のモジュール\u003c/li\u003e\n\u003cli\u003e最終的な自然言語表現を生成するシステム\u003c/li\u003e\n\u003cli\u003e4つのコンポーネントからなる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/9ybi8yfr.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Content Determination\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Sentence Planning\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Surface Realization\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Lexicalization, Referring expression, aggregation\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e RNNに基づいた統計言語モデルにおいて，意味的制約や文法構造による返答生成を行うなど\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e コンテクストを理解した返答を生成することは重要である\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e タスク指向型においては，返答の多様性というよりも信頼性のほうが重要視されがち\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 意味解析をビームサーチを使うことで，意味の正しさを改善する手法も提案された\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eE2E Methods\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eend-to-endのパイプラインを組むことで高いパフォーマンスを発揮することがあるが，\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e多くのモジュールを組み込むため，バックプロパゲーションで誤差が伝播しないこともある．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eすべてのモジュールが，返答の精度を向上するために，対等に重要であるとは限らない．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e違うドメインに差し替えるとき，オントロジーを事前学習させる必要があるため，困難が生じることもある\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eやり方は大きく分けて2つ\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eすべてのモジュールを展開して誤差逆伝播させる？\u003c/li\u003e\n\u003cli\u003e知識ベースの検索システムと返答生成の双方を用いてパイプラインを組む\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eタスク指向型においては，外部の知識源が必要なことが多い\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eオープンドメイン型対話システム\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e雑談対話システム，或いはタスク思考型ではない対話システムのこと\u003c/li\u003e\n\u003cli\u003eSoTAを示しているオープンドメインは大抵ニューラルネットで解決している\u003c/li\u003e\n\u003cli\u003e完全なるデータドリブンなものが多い\u003c/li\u003e\n\u003cli\u003eオープンドメイン型対話システムは，大まか3つに分けられる\n\u003cul\u003e\n\u003cli\u003e生成システム\u003c/li\u003e\n\u003cli\u003e検索ベースシステム\u003c/li\u003e\n\u003cli\u003eアンサンブルシステム\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e３つの話が以下\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e生成システム\n\u003cul\u003e\n\u003cli\u003e訓練コーパスに出てこないような返答に対して，ユーザのメッセージや対話履歴をマッピングするために，seq2seqなモデルを適用する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e検索システム\n\u003cul\u003e\n\u003cli\u003e決まった返答集合の中からすでに存在する返答を探そうとする\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eアンサンブルシステム\n\u003cul\u003e\n\u003cli\u003e生成手法と検索手法を合わせる．\u003c/li\u003e\n\u003cli\u003e生成された返答と検索された返答とを比べる．\u003c/li\u003e\n\u003cli\u003e生成も，検索された返答を洗練するために用いられる．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特徴として，\u003c/p\u003e\n\u003cp\u003e生成モデルは\u003c/p\u003e\n\u003cp\u003e柔軟でコンテクストを読んだ返答をできるが，ときには理解に欠けていたり，怠けた返答を見せることがある\u003c/p\u003e\n\u003cp\u003e検索ベースのモデルは\u003c/p\u003e\n\u003cp\u003e人の返答の集合から実際の返答を選ぶため，返答の集合は有限集合であり，コンテクストと相関がないことがある．\u003c/p\u003e\n\u003cp\u003eただし，表面上のレベルでは，首尾一貫した返答することも多い\u003c/p\u003e\n\u003cp\u003e以下は，オープンドメイン型対話システムにおける，難しさとホットなトピックをまとめる\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eContext Awareness\n\u003cul\u003e\n\u003cli\u003e対話コンテクストは会話のトピックを決定したり，ユーザの目標を決定したりと重要\u003c/li\u003e\n\u003cli\u003eコンテクストを解釈した対話エージェントは，現メッセージだけではなく，対話履歴からももとにして返答する\u003c/li\u003e\n\u003cli\u003e生成モデルも検索ベースも，どちらも対話コンテクストモデリングに依存する\u003c/li\u003e\n\u003cli\u003eいくつかのモデルではAttentionが使用されているらしい\u003c/li\u003e\n\u003cli\u003e構造化されたAttentionを用いることでコンテクストを読み取れる？\u003c/li\u003e\n\u003cli\u003e対話をリライトする問題があるらしい\n\u003cul\u003e\n\u003cli\u003e複数のメッセージから単一のメッセージに変換する目標\u003c/li\u003e\n\u003cli\u003eここではコンテクストを理解させることが重要\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eResponse Coherence\n\u003cul\u003e\n\u003cli\u003e首尾一貫した返答は，良い生成器としての一つのクオリティ\u003c/li\u003e\n\u003cli\u003e対話の中で，論理的で首尾一貫しているか？という指標\u003c/li\u003e\n\u003cli\u003e生成モデルにおいてホットなトピックとなっている（検索ベースはすでに人の返答をりようするのでもともと一貫性はあるという主張）\u003c/li\u003e\n\u003cli\u003e一貫性のない文の順序を見つけるタスクを解くことで，返答の一貫性を改善した事例もあり\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eResponse Diversity\n\u003cul\u003e\n\u003cli\u003e人が多用するような表現は訓練コーパスにも多く含まれ，それらばかりを返答してしまうことが問題となりうる\u003c/li\u003e\n\u003cli\u003eかつては条件付き確率において，尤度関数を解くことで尤もらしい返答をもとめていた．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/00fwhths.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e この手法では，返答の精度の安全性と適切さはトレードオフになっていた？\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e ビームサーチを提案されたことも\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eSpeaker Consistency and Personality-based Response\n\u003cul\u003e\n\u003cli\u003eシステムは，訓練コーパスからサンプリングされた分布に対して学習\n\u003cul\u003e\n\u003cli\u003e対話者の趣味といった一貫性のないものに対する返答は．．．\u003c/li\u003e\n\u003cli\u003e対話者の役割を理解し，その個人に合わせた返答が必要になる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e1ステージではなく，3ステージで個人的な嗜好に対応した事例がある\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEmpathetic Response\n\u003cul\u003e\n\u003cli\u003e同情する対話システムは，ユーザの感情の変化や感情に伴った適切な返答をする\u003c/li\u003e\n\u003cli\u003e雑談チャットについて，このトピックは重要\u003c/li\u003e\n\u003cli\u003eCortanaやAlexaなどの製品にもモジュールが含まれている\u003c/li\u003e\n\u003cli\u003eCoBERTのモデルなど\u003c/li\u003e\n\u003cli\u003e感情対話システムのデータセットはとぼしいが，新たなデータセットとベンチマークが提供されたらしい\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eConversation Topics\n\u003cul\u003e\n\u003cli\u003eトピックや目的は，会話に参加した人と会話を続けるための重要な役割を果たす\u003c/li\u003e\n\u003cli\u003eトピックを理解させることが重要\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKnowledge-Grounded System\n\u003cul\u003e\n\u003cli\u003e人は，会話のコンテクストと経験や記憶といったものとを関連付けて，返答をする（機会には難しい）\u003c/li\u003e\n\u003cli\u003e生成モデルは，単なる機械翻訳よりも複雑\n\u003cul\u003e\n\u003cli\u003eより自由度が高く，制約が曖昧なため\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e故に，雑談チャットは，外部から得られる常識と結びつけて，seq2seqなモデルによって生成する\u003c/li\u003e\n\u003cli\u003eメモリーネットワークなどで，知識をグラウンディングする手法\u003c/li\u003e\n\u003cli\u003e知識グラフは外部の情報をソースにするものもある．\u003c/li\u003e\n\u003cli\u003egraph attentionを用いて，常識をグラフベースで学習する手法も\u003c/li\u003e\n\u003cli\u003e主な考え方は，外部の知識グラフを使って，会話の論理の流れをモデリングする指標の一部として扱う\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eInteractive Training\n\u003cul\u003e\n\u003cli\u003e別名；human-in-loop training\u003c/li\u003e\n\u003cli\u003eアノテーションされたデータセットは限られている\n\u003cul\u003e\n\u003cli\u003eすべての状況をカバーすることは不可能\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eユーザとの対話の中で，システムを改善する\u003c/li\u003e\n\u003cli\u003e強化学習における逐次学習を提案\u003c/li\u003e\n\u003cli\u003e対話相手と話して，その相手からフィードバックを得る\u003c/li\u003e\n\u003cli\u003e教師あり学習をした後，Interactive Trainingによってファインチューニングする\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eVisual Dialogue\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/v3baquyk.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Visual Q \u0026#x26; Aなど\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 画像あり対話システムのほか，映像あり対話システムも面白いトピックだが難題でもある\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 特徴量抽出の複雑さも増す\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e visual dialogueのアノテーションは重労働であり，データセットに乏しいので，現在はデータの不十分さに悩まされている\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e評価のアプローチ\u003c/h3\u003e\n\u003cp\u003e評価の仕方も重要なパートとなっている\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eタスク指向型対話システムにおける評価\n\u003cul\u003e\n\u003cli\u003eBLEUスコアを用いて，システムの返答と人の返答を比べるなど\u003c/li\u003e\n\u003cli\u003eTask Completion Rate\n\u003cul\u003e\n\u003cli\u003eすべてのタスクの試行に対して，いくつのイベントが成功したかの割合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTask Completion Cost\n\u003cul\u003e\n\u003cli\u003eタスクをこなすのに使われたリソース\u003c/li\u003e\n\u003cli\u003e解決までの時間が重視されるタスクにおいて用いられる\u003c/li\u003e\n\u003cli\u003eなるべく短いターン数で完遂するのが良しとされる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHuman-based Evaluation\n\u003cul\u003e\n\u003cli\u003eユーザの対話とユーザの満足度のスコアを提供\u003c/li\u003e\n\u003cli\u003e方法はふたつ\n\u003cul\u003e\n\u003cli\u003eクラウドソーシングで労働を雇う\u003c/li\u003e\n\u003cli\u003e実際にローンチしてからユーザのフィードバックで評価する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eオープンドメイン型対話システムにおける評価\n\u003cul\u003e\n\u003cli\u003e明確なメトリックはない\u003c/li\u003e\n\u003cli\u003e長らくHuman Evaluationを使ってきた\u003c/li\u003e\n\u003cli\u003eWord-overlap Metrics\n\u003cul\u003e\n\u003cli\u003e生成された系列と実際の系列の近さを計算する\u003c/li\u003e\n\u003cli\u003e機械翻訳や要約タスクにおいて用いられる\u003c/li\u003e\n\u003cli\u003en-gramのものとして\n\u003cul\u003e\n\u003cli\u003eBLEU\u003c/li\u003e\n\u003cli\u003eROUGE\u003c/li\u003e\n\u003cli\u003eMETEOR(BLUEの改良版)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNeural Metrics\n\u003cul\u003e\n\u003cli\u003eニューラルモデルによって計算させる\u003c/li\u003e\n\u003cli\u003eRNNやCNN,GANの識別器を使うなどして，ターンレベルの特徴量抽出を行うなど\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e今もホットなトピックになっている\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eデータセット\u003c/h3\u003e\n\u003cp\u003eタスク指向型対話システム\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/4xcx432d.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/bvaa6edt.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/5nqvspiq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eオープンドメイン型対話システム\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ja1g5vzq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ki5ab45o.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ikzadi1i.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/nuxspw6o.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/1u3frgui.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003e結論とトレンド\u003c/h3\u003e\n\u003cp\u003eココ最近のトレンド\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMutlimodal dialogue systems\n\u003cul\u003e\n\u003cli\u003e異なるモダリティを組み合わせる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMultitask dialogue systems\n\u003cul\u003e\n\u003cli\u003eタスク指向型と知識グラウンディングさせたオープンドメイン型を組み合わせて，一つのフレームワークまたはシングルモデルとして完結させる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCorpus exploration on Internet\n\u003cul\u003e\n\u003cli\u003ereal-timeなコーパスをインターネットから取り出せるようになれば，期待がもてる\u003c/li\u003e\n\u003cli\u003e研究に値するのでは？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUser modeling\n\u003cul\u003e\n\u003cli\u003e生成と評価の双方でホットなトピック\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDialogue generation with a long-term goal\n\u003cul\u003e\n\u003cli\u003e日常的な雑談は特に目的はない\u003c/li\u003e\n\u003cli\u003eしかし，会話が意図的にある特定の目的に向かうときは，ほんの少しでも状況があるはず\u003c/li\u003e\n\u003cli\u003e現在のオープンドメイン型は，長期的な目的を除いてモデリングされがち\n\u003cul\u003e\n\u003cli\u003e十分な知性を備えていない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey","Date":"2023-05-22","Category":"論文","Tags":["survey","dialogue system"],"Authos":"ゆうぼう","Slug":"Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey","Thumbnail":"/images/thumbnails/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey.png","Description":"Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Surveyのまとめ","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"]},"__N_SSG":true},"page":"/[slug]","query":{"slug":"Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey"},"buildId":"pIuzt9pMRyUqa50hVFtFJ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>