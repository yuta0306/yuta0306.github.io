<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>collate_fnで複数の引数を取りたい!!<!-- --> | <!-- -->ゆうぼうの書跡棚</title><meta name="description" content="Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変かも)"/><meta name="og:description" content="Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変かも)"/><meta property="og:type" content="website"/><meta name="author" content="ゆうぼう"/><meta property="og:title" content="collate_fnで複数の引数を取りたい!!"/><meta property="og:image" content="https://yuta0306.github.io/images/thumbnails/pytorch-logo.jpg"/><meta property="og:url" content="https://yuta0306.github.io/pytorch-collate_fn-args"/><script src="/js/toc.js"></script><meta name="next-head-count" content="11"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><link rel="alternate" type="application/rss+xml" href="https:/yuta0306.github.io/feed.xml" title="RSS2.0"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/75506965150cde8a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/75506965150cde8a.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a6e19106a865540a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a6e19106a865540a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/664-60e06c839f82ba03.js" defer=""></script><script src="/_next/static/chunks/768-c61e8ddb09e59da7.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-b275297c06761584.js" defer=""></script><script src="/_next/static/SbY9LeOoZb-blH2R5VrxD/_buildManifest.js" defer=""></script><script src="/_next/static/SbY9LeOoZb-blH2R5VrxD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Home_container__bCOhY"><div class="header_container__IoqX_"><div class="header_container__inner__pDDDU"><header class="header_header__pKEQL"><a href="/"><div class="header_header__title__uoTF0">ゆうぼうの書跡棚</div></a></header><div class="header_hamburger__kYfxY"><div><img src="/icons/hamburger.png"/></div></div><nav class="header_nav__closed__1h469"><ul class="header_nav__list__eqFqF"><li class="header_nav__item__FNSzb"><a href="/about">About</a></li><li class="header_nav__item_active__qVXxE"><a href="/">Blog</a></li><li class="header_nav__item__FNSzb"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdgyok9pi697ZJvVizRNEw0qghDWz517k1FrbcRmfvvERlraA/viewform">Contact</a></li></ul></nav></div></div><div class="header_category__WFSrL"><ul class="header_category__items____MmN"></ul></div><div class="main_main__VZQGI"><div class="main_main__container__PFqpL"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__V_9fG"><div itemscope="" itemType="http://schema.org/BlogPosting"><div style="background:url(/images/thumbnails/pytorch-logo.jpg);overflow:hidden"><div class="Home_thumbnail__xs1Hd" itemscope="" itemProp="image" itemType="https://schema.org/ImageObject"><img src="/images/thumbnails/pytorch-logo.jpg" alt="collate_fnで複数の引数を取りたい!!" loading="lazy" style="height:100%;width:auto;margin:0 auto;display:block"/></div></div><time dateTime="2021-12-24" style="color:rgb(144, 144, 144)">2021-12-24</time><h1>collate_fnで複数の引数を取りたい!!</h1><div id="TOC__mobile"></div><article style="margin-top:4rem" itemscope="" itemProp="text"><p>Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．</p>
<p>この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変か?)</p>
<h2>やりたかったこと</h2>
<p>DataLoaderを定義するときに，<code>collate_fn</code>のところで自作collate_fnを指定して，batch単位で流れてくるデータに対してエンコードすること．</p>
<p>これがやりたいことになります．つまりこんな感じ</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> Dataset, DataLoader

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__()
        ...
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):
        ...

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx: <span class="hljs-built_in">int</span></span>):
        ...

dataloader = DataLoader(dataset=MyDataset(), batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>,
            num_workers=os.cpu_count(),
            collate_fn=custom_collate_fn)  <span class="hljs-comment"># &#x3C;--- ここで自作collate_fnを指定して制御</span>
</code></pre>
<h2>やってうまくいかなかったこと</h2>
<p>先にやってうまくいかなかったことを共有しておきます．</p>
<p>自分が使っているのが，<code>pytorch-lightning</code>なのでそのせいもあるかもしれません．なので，もしかしたら普通に素のPytorchならうまくいくかもしれません．</p>
<p>教えてください🙏</p>
<h3>lambda式で制御する (functools.partialを使う)</h3>
<p>こんなことをしました．</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> Dataset, DataLoader
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__()
        ...
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):
        ...

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx: <span class="hljs-built_in">int</span></span>):
        ...
        <span class="hljs-keyword">return</span> text, label

<span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_collate_fn</span>(<span class="hljs-params">data, tokenizer, max_length</span>):
    texts, labels = <span class="hljs-built_in">zip</span>(*data)
    texts = <span class="hljs-built_in">list</span>(texts)
    texts = tokenizer.batch_encode_plus(
        texts,
        padding=<span class="hljs-literal">True</span>,
        truncation=<span class="hljs-literal">True</span>,
        max_length=max_length,
        return_tensors=<span class="hljs-string">'pt'</span>,
    )
    labels = torch.LongTensor(labels)
    <span class="hljs-keyword">return</span> texts, labels

tokenizer = AutoTokenizer.from_pretrained(...)
max_length = <span class="hljs-number">256</span>
dataloader = DataLoader(dataset=MyDataset(), batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>,
            num_workers=os.cpu_count(),
            collate_fn=<span class="hljs-keyword">lambda</span> data: custom_collate_fn(data, tokenizer, max_length))
</code></pre>
<p><code>pytorch-lightning</code>の仕様だとは思うのですが，<code>pickle</code>で圧縮するらしくそのタイミングでエラーを吐かれました．</p>
<p>なぜだろう...有識者の方教えてください...</p>
<h2>【解決策】 classで定義する</h2>
<p>lambda式でダメだったので，もうクラスの内部に必要なものを保持させておこうということになりました．(僕の中では)</p>
<p>次のコードのような感じで解決しました．</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> Dataset, DataLoader
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__()
        ...
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):
        ...

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx: <span class="hljs-built_in">int</span></span>):
        ...
        <span class="hljs-keyword">return</span> text, label

<span class="hljs-keyword">class</span> <span class="hljs-title class_">CollateFn</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokenizer, max_length: <span class="hljs-built_in">int</span></span>) -> <span class="hljs-literal">None</span>:
        self.tokenizer = tokenizer
        self.max_length = max_length
        os.environ[<span class="hljs-string">"TOKENIZERS_PARALLELISM"</span>] = <span class="hljs-string">"true"</span>  <span class="hljs-comment"># &#x3C;--- 多分これを明示的に指定しないと怒られます (true|false)</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, data</span>):
        texts, labels = <span class="hljs-built_in">zip</span>(*data)
        texts = <span class="hljs-built_in">list</span>(texts)
        texts = self.tokenizer.batch_encode_plus(
            texts,
            padding=<span class="hljs-literal">True</span>,
            truncation=<span class="hljs-literal">True</span>,
            max_length=self.max_length,
            return_tensors=<span class="hljs-string">'pt'</span>,
        )
        labels = torch.LongTensor(labels)
        <span class="hljs-keyword">return</span> texts, labels

tokenizer = AutoTokenizer.from_pretrained(...)
max_length = <span class="hljs-number">256</span>
dataloader = DataLoader(dataset=MyDataset(), batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>,
            num_workers=os.cpu_count(),
            collate_fn=CollateFn(tokenizer, max_length))
</code></pre>
<h2>まとめ</h2>
<p>素のPytorchで組めば問題なかったのかもしれませんが，<code>pytorch-lightning</code>を使っている方は同じ状況になるかもしれません．</p>
<p>その時は，ぜひ参考にclassでcollate_fnで実装してみて解決の一助となれたら幸いです．</p></article><div class="socialshare_container__SSXJE"><h3>タメになったらSHARE!!!</h3><div class="socialshare_container__links__JZs4j"><a target="_blank" href="https://twitter.com/share?url=https://yuta0306.github.io/pytorch-collate_fn-args"><img src="/icons/twitter.png" loading="lazy" alt="https://yuta0306.github.io/pytorch-collate_fn-argsをTwitterに共有する"/></a><a target="_blank" href="https://www.facebook.com/share.php?u=https://yuta0306.github.io/pytorch-collate_fn-args"><img src="/icons/facebook.png" loading="lazy" alt="https://yuta0306.github.io/pytorch-collate_fn-argsをFacebookに共有する"/></a><a target="_blank" href="http://b.hatena.ne.jp/entry/https:/yuta0306.github.io/pytorch-collate_fn-args"><img src="/icons/hatenablog.png" loading="lazy" alt="https://yuta0306.github.io/pytorch-collate_fn-argsをはてなブログに共有する"/></a></div></div></div></div></main><aside class="main_sidebar__tM28d"><div class="shortbio_container__4psan" itemscope="" itemProp="author" itemType="http://schema.org/Person"><div class="shortbio_container__image__eljVd"><img src="/images/profile.jpeg" alt="ゆうぼう" loading="lazy"/></div><h3 class="shortbio_author__A2bKB" itemscope="" itemProp="name">ゆうぼう</h3><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">国立大学院M1のナマケモノです．</p></div><div><p class="shortbio_container__paragraph__EJbWG">human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．</p></div><div><p class="shortbio_container__paragraph__EJbWG">学部時代はコモンセンスを利用したユーモア検出の研究を行っていました(Knowledge-intensive NLP)．</p></div><div><p class="shortbio_container__paragraph__EJbWG">このブログはNext.jsで書いてます．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">Kaggle等のデータ分析コンペは活動休止中．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div></div><div class="followme_container__T1oVi"><h3 class="followme_container__header__Pt5SP">Follow Me</h3><div class="followme_container__links__b3XW5"><a target="_blank" href="https://github.com/yuta0306"><img src="/icons/github.png" alt="GitHub"/></a><a target="_blank" href="https://kaggle.com/yutasasaki"><img src="/icons/kaggle.png" alt="Kaggle"/></a><a target="_blank" href="https://twitter.com/Sloth65557166"><img src="/icons/twitter.png" alt="Twitter"/></a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="categories_container__J8nCF"><h3 class="categories_container__header__zt836">Categories</h3><div class="categories_container__links__MFrVK"><a class="categories_container__link__AuvVr" href="/category/%E8%AB%96%E6%96%87/1">論文</a><a class="categories_container__link__AuvVr" href="/category/Web/1">Web</a><a class="categories_container__link__AuvVr" href="/category/JavaScript/1">JavaScript</a><a class="categories_container__link__AuvVr" href="/category/Competition/1">Competition</a><a class="categories_container__link__AuvVr" href="/category/Cloud/1">Cloud</a><a class="categories_container__link__AuvVr" href="/category/Python/1">Python</a><a class="categories_container__link__AuvVr" href="/category/Linux/1">Linux</a><a class="categories_container__link__AuvVr" href="/category/ML/1">ML</a><a class="categories_container__link__AuvVr" href="/category/Go/1">Go</a><a class="categories_container__link__AuvVr" href="/category/SQL/1">SQL</a></div></div><div class="tags_container___e3ez"><h3 class="tags_container__header__hxPW8">Tags</h3><div class="tags_container__links__X38Ga"><a class="tags_container__link__1Ts3a" href="/tag/Apache/1">Apache</a><a class="tags_container__link__1Ts3a" href="/tag/Appium/1">Appium</a><a class="tags_container__link__1Ts3a" href="/tag/ASR/1">ASR</a><a class="tags_container__link__1Ts3a" href="/tag/atmaCup/1">atmaCup</a><a class="tags_container__link__1Ts3a" href="/tag/AWS/1">AWS</a><a class="tags_container__link__1Ts3a" href="/tag/brew/1">brew</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS7/1">CentOS7</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS8/1">CentOS8</a><a class="tags_container__link__1Ts3a" href="/tag/Colab/1">Colab</a><a class="tags_container__link__1Ts3a" href="/tag/COMET/1">COMET</a><a class="tags_container__link__1Ts3a" href="/tag/commonsense/1">commonsense</a><a class="tags_container__link__1Ts3a" href="/tag/conda/1">conda</a><a class="tags_container__link__1Ts3a" href="/tag/Contrasive%20Learning/1">Contrasive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/Contrastive%20Learning/1">Contrastive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/CSS/1">CSS</a><a class="tags_container__link__1Ts3a" href="/tag/Demo/1">Demo</a><a class="tags_container__link__1Ts3a" href="/tag/Dialogue%20Structure%20Learning/1">Dialogue Structure Learning</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system/1">dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/DST/1">DST</a><a class="tags_container__link__1Ts3a" href="/tag/Emotion%20Recognition/1">Emotion Recognition</a><a class="tags_container__link__1Ts3a" href="/tag/empathetic%20dialogue%20system/1">empathetic dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/encyclopedic/1">encyclopedic</a><a class="tags_container__link__1Ts3a" href="/tag/Error%20Correction/1">Error Correction</a><a class="tags_container__link__1Ts3a" href="/tag/ESPNet/1">ESPNet</a><a class="tags_container__link__1Ts3a" href="/tag/ffmpeg/1">ffmpeg</a><a class="tags_container__link__1Ts3a" href="/tag/Flask/1">Flask</a><a class="tags_container__link__1Ts3a" href="/tag/Gating%20Mechanism/1">Gating Mechanism</a><a class="tags_container__link__1Ts3a" href="/tag/Go/1">Go</a><a class="tags_container__link__1Ts3a" href="/tag/Google%20Colaboratory/1">Google Colaboratory</a><a class="tags_container__link__1Ts3a" href="/tag/Heroku/1">Heroku</a><a class="tags_container__link__1Ts3a" href="/tag/Highway%20Transformer/1">Highway Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/HTML/1">HTML</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection/1">humor detection</a><a class="tags_container__link__1Ts3a" href="/tag/Intent%20Classification/1">Intent Classification</a><a class="tags_container__link__1Ts3a" href="/tag/Internet-Augmented/1">Internet-Augmented</a><a class="tags_container__link__1Ts3a" href="/tag/JavaScript/1">JavaScript</a><a class="tags_container__link__1Ts3a" href="/tag/JSON/1">JSON</a><a class="tags_container__link__1Ts3a" href="/tag/Kaggle/1">Kaggle</a><a class="tags_container__link__1Ts3a" href="/tag/KC-Net/1">KC-Net</a><a class="tags_container__link__1Ts3a" href="/tag/knowledge-base/1">knowledge-base</a><a class="tags_container__link__1Ts3a" href="/tag/Knowledge-Intensive%20NLP/1">Knowledge-Intensive NLP</a><a class="tags_container__link__1Ts3a" href="/tag/laughter/1">laughter</a><a class="tags_container__link__1Ts3a" href="/tag/Linux/1">Linux</a><a class="tags_container__link__1Ts3a" href="/tag/LLM/1">LLM</a><a class="tags_container__link__1Ts3a" href="/tag/Mac/1">Mac</a><a class="tags_container__link__1Ts3a" href="/tag/make/1">make</a><a class="tags_container__link__1Ts3a" href="/tag/map/1">map</a><a class="tags_container__link__1Ts3a" href="/tag/MeCab/1">MeCab</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20health/1">mental health</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20state%20knowledge/1">mental state knowledge</a><a class="tags_container__link__1Ts3a" href="/tag/mentalisation/1">mentalisation</a><a class="tags_container__link__1Ts3a" href="/tag/MentalRoBERTa/1">MentalRoBERTa</a><a class="tags_container__link__1Ts3a" href="/tag/Merging%20Models/1">Merging Models</a><a class="tags_container__link__1Ts3a" href="/tag/ML/1">ML</a><a class="tags_container__link__1Ts3a" href="/tag/Model%20Editing/1">Model Editing</a><a class="tags_container__link__1Ts3a" href="/tag/Model%20Patching/1">Model Patching</a><a class="tags_container__link__1Ts3a" href="/tag/MT/1">MT</a><a class="tags_container__link__1Ts3a" href="/tag/Multi-Hop%20Transformer/1">Multi-Hop Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/multi-modal/1">multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/MySQL/1">MySQL</a><a class="tags_container__link__1Ts3a" href="/tag/NLG/1">NLG</a><a class="tags_container__link__1Ts3a" href="/tag/NLI/1">NLI</a><a class="tags_container__link__1Ts3a" href="/tag/NLP/1">NLP</a><a class="tags_container__link__1Ts3a" href="/tag/Node/1">Node</a><a class="tags_container__link__1Ts3a" href="/tag/node.js/1">node.js</a><a class="tags_container__link__1Ts3a" href="/tag/npm/1">npm</a><a class="tags_container__link__1Ts3a" href="/tag/Overleaf/1">Overleaf</a><a class="tags_container__link__1Ts3a" href="/tag/Pandas/1">Pandas</a><a class="tags_container__link__1Ts3a" href="/tag/persona/1">persona</a><a class="tags_container__link__1Ts3a" href="/tag/PLMKE/1">PLMKE</a><a class="tags_container__link__1Ts3a" href="/tag/Poetry/1">Poetry</a><a class="tags_container__link__1Ts3a" href="/tag/Prompt-Tuning/1">Prompt-Tuning</a><a class="tags_container__link__1Ts3a" href="/tag/Python/1">Python</a><a class="tags_container__link__1Ts3a" href="/tag/Pytorch/1">Pytorch</a><a class="tags_container__link__1Ts3a" href="/tag/pytorch-lightning/1">pytorch-lightning</a><a class="tags_container__link__1Ts3a" href="/tag/Scikit-learn/1">Scikit-learn</a><a class="tags_container__link__1Ts3a" href="/tag/Selenium/1">Selenium</a><a class="tags_container__link__1Ts3a" href="/tag/Self-Dependency-Units%20(SDU)/1">Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/shared%20laughter/1">shared laughter</a><a class="tags_container__link__1Ts3a" href="/tag/SISR/1">SISR</a><a class="tags_container__link__1Ts3a" href="/tag/SLU/1">SLU</a><a class="tags_container__link__1Ts3a" href="/tag/Speech%20Disfluency/1">Speech Disfluency</a><a class="tags_container__link__1Ts3a" href="/tag/subprocess/1">subprocess</a><a class="tags_container__link__1Ts3a" href="/tag/Super-Resolution/1">Super-Resolution</a><a class="tags_container__link__1Ts3a" href="/tag/survey/1">survey</a><a class="tags_container__link__1Ts3a" href="/tag/tensorflow/1">tensorflow</a><a class="tags_container__link__1Ts3a" href="/tag/Tkinter/1">Tkinter</a><a class="tags_container__link__1Ts3a" href="/tag/Transfer%20Learning/1">Transfer Learning</a><a class="tags_container__link__1Ts3a" href="/tag/transformer/1">transformer</a><a class="tags_container__link__1Ts3a" href="/tag/Weight%20Interpolation/1">Weight Interpolation</a><a class="tags_container__link__1Ts3a" href="/tag/zsh/1">zsh</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91/1">オブジェクト指向</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%82%B3%E3%83%AC%E3%83%BC%E3%82%BF/1">デコレータ</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90/1">データ分析</a><a class="tags_container__link__1Ts3a" href="/tag/%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/1">特殊メソッド</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%81%9E%E3%81%8D%E6%89%8B%E5%8F%8D%E5%BF%9C/1">聞き手反応</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%AB%96%E6%96%87%E5%9F%B7%E7%AD%86/1">論文執筆</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%B6%85%E8%A7%A3%E5%83%8F/1">超解像</a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div id="TOC"></div></aside></div></div><footer class="footer_footer__WCChH"><div class="footer_footer__inner__287VQ"><div><a class="footer_footer__link__Ql5Ng" href="/privacy-policy">プライバシーポリシー</a></div><div class="footer_footer__title__PRn_u"><a href="/">ゆうぼうの書跡棚</a></div><div class="footer_footer__small__RlIHP"><small>Powered by <a target="_blank" class="footer_footer__small__link__u5kuV" href="https://twitter.com/Sloth65557166">ゆうぼう</a></small></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"contentHtml":"\u003cp\u003eTransformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．\u003c/p\u003e\n\u003cp\u003eこの時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変か?)\u003c/p\u003e\n\u003ch2\u003eやりたかったこと\u003c/h2\u003e\n\u003cp\u003eDataLoaderを定義するときに，\u003ccode\u003ecollate_fn\u003c/code\u003eのところで自作collate_fnを指定して，batch単位で流れてくるデータに対してエンコードすること．\u003c/p\u003e\n\u003cp\u003eこれがやりたいことになります．つまりこんな感じ\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e torch.utils \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Dataset, DataLoader\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMyDataset\u003c/span\u003e(\u003cspan class=\"hljs-title class_ inherited__\"\u003eDataset\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, *args, **kwargs\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__()\n        ...\n    \n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__len__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        ...\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__getitem__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, idx: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e\u003c/span\u003e):\n        ...\n\ndataloader = DataLoader(dataset=MyDataset(), batch_size=\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, shuffle=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n            num_workers=os.cpu_count(),\n            collate_fn=custom_collate_fn)  \u003cspan class=\"hljs-comment\"\u003e# \u0026#x3C;--- ここで自作collate_fnを指定して制御\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eやってうまくいかなかったこと\u003c/h2\u003e\n\u003cp\u003e先にやってうまくいかなかったことを共有しておきます．\u003c/p\u003e\n\u003cp\u003e自分が使っているのが，\u003ccode\u003epytorch-lightning\u003c/code\u003eなのでそのせいもあるかもしれません．なので，もしかしたら普通に素のPytorchならうまくいくかもしれません．\u003c/p\u003e\n\u003cp\u003e教えてください🙏\u003c/p\u003e\n\u003ch3\u003elambda式で制御する (functools.partialを使う)\u003c/h3\u003e\n\u003cp\u003eこんなことをしました．\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e torch.utils \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Dataset, DataLoader\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoTokenizer\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMyDataset\u003c/span\u003e(\u003cspan class=\"hljs-title class_ inherited__\"\u003eDataset\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, *args, **kwargs\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__()\n        ...\n    \n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__len__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        ...\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__getitem__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, idx: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e\u003c/span\u003e):\n        ...\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e text, label\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecustom_collate_fn\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edata, tokenizer, max_length\u003c/span\u003e):\n    texts, labels = \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(*data)\n    texts = \u003cspan class=\"hljs-built_in\"\u003elist\u003c/span\u003e(texts)\n    texts = tokenizer.batch_encode_plus(\n        texts,\n        padding=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n        truncation=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n        max_length=max_length,\n        return_tensors=\u003cspan class=\"hljs-string\"\u003e'pt'\u003c/span\u003e,\n    )\n    labels = torch.LongTensor(labels)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e\ndataloader = DataLoader(dataset=MyDataset(), batch_size=\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, shuffle=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n            num_workers=os.cpu_count(),\n            collate_fn=\u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e data: custom_collate_fn(data, tokenizer, max_length))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003epytorch-lightning\u003c/code\u003eの仕様だとは思うのですが，\u003ccode\u003epickle\u003c/code\u003eで圧縮するらしくそのタイミングでエラーを吐かれました．\u003c/p\u003e\n\u003cp\u003eなぜだろう...有識者の方教えてください...\u003c/p\u003e\n\u003ch2\u003e【解決策】 classで定義する\u003c/h2\u003e\n\u003cp\u003elambda式でダメだったので，もうクラスの内部に必要なものを保持させておこうということになりました．(僕の中では)\u003c/p\u003e\n\u003cp\u003e次のコードのような感じで解決しました．\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e torch.utils \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Dataset, DataLoader\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoTokenizer\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMyDataset\u003c/span\u003e(\u003cspan class=\"hljs-title class_ inherited__\"\u003eDataset\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, *args, **kwargs\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__()\n        ...\n    \n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__len__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        ...\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__getitem__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, idx: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e\u003c/span\u003e):\n        ...\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e text, label\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCollateFn\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, tokenizer, max_length: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        os.environ[\u003cspan class=\"hljs-string\"\u003e\"TOKENIZERS_PARALLELISM\"\u003c/span\u003e] = \u003cspan class=\"hljs-string\"\u003e\"true\"\u003c/span\u003e  \u003cspan class=\"hljs-comment\"\u003e# \u0026#x3C;--- 多分これを明示的に指定しないと怒られます (true|false)\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__call__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, data\u003c/span\u003e):\n        texts, labels = \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(*data)\n        texts = \u003cspan class=\"hljs-built_in\"\u003elist\u003c/span\u003e(texts)\n        texts = self.tokenizer.batch_encode_plus(\n            texts,\n            padding=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n            truncation=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n            max_length=self.max_length,\n            return_tensors=\u003cspan class=\"hljs-string\"\u003e'pt'\u003c/span\u003e,\n        )\n        labels = torch.LongTensor(labels)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e\ndataloader = DataLoader(dataset=MyDataset(), batch_size=\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, shuffle=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n            num_workers=os.cpu_count(),\n            collate_fn=CollateFn(tokenizer, max_length))\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e素のPytorchで組めば問題なかったのかもしれませんが，\u003ccode\u003epytorch-lightning\u003c/code\u003eを使っている方は同じ状況になるかもしれません．\u003c/p\u003e\n\u003cp\u003eその時は，ぜひ参考にclassでcollate_fnで実装してみて解決の一助となれたら幸いです．\u003c/p\u003e","Title":"collate_fnで複数の引数を取りたい!!","Date":"2021-12-24","Category":"Python","Tags":["ML","Python","Pytorch"],"Authors":"ゆうぼう","Slug":"pytorch-collate_fn-args","Thumbnail":"/images/thumbnails/pytorch-logo.jpg","Description":"Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変かも)","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Overleaf","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","論文執筆","超解像"]},"__N_SSG":true},"page":"/[slug]","query":{"slug":"pytorch-collate_fn-args"},"buildId":"SbY9LeOoZb-blH2R5VrxD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>