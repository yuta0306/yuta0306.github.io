<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>About SASAKI Yuta</title><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="4"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><link rel="alternate" type="application/rss+xml" href="https:/yuta0306.github.io/feed.xml" title="RSS2.0"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/75506965150cde8a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/75506965150cde8a.css" data-n-g=""/><link rel="preload" href="/_next/static/css/08cab5aedfedf73e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/08cab5aedfedf73e.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/pages/profile-42c2944121aeffff.js" defer=""></script><script src="/_next/static/ULwbJP9nZia2Ea0nrVo4J/_buildManifest.js" defer=""></script><script src="/_next/static/ULwbJP9nZia2Ea0nrVo4J/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Home_container__bCOhY"><div class="main_main__VZQGI"><div class="main_main__container_about__Zbqes"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__V_9fG"><h1>SASAKI Yuta<!-- --> (<!-- -->佐々木 裕多<!-- -->)</h1><div><h2>Bio</h2>
<p>I am a graduate student at Tokyo Institute of Technology.<br>
I would like to research into human-like dialogue systems, and build my career to develop the society where humans and AI cooperate with each other.<br>
I'm wating for the invitation for internships and collaborative research.<br>
Please feel free to contact me!</p>
<p>(日本語)<br>
東京工業大学で大学院生をしています．<br>
human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．<br>
インターンシップや共同研究のお誘いをお待ちしております．<br>
気軽な連絡お待ちしております！</p>
<h2>Research Interest</h2>
<ul>
<li>Dialogue System (対話システム)</li>
<li>Multi-modal Dialogue System (マルチモーダル対話システム)</li>
<li>Collaborative Dialogue System (共話システム？)</li>
<li>Speech Dialogue System (音声対話システム)</li>
<li>Human Interaction (ヒューマンインタラクション)</li>
<li>LLM (大規模言語モデル)</li>
<li>Humor Recognition (ユーモア検出)</li>
<li>Knowledge-Intensive NLP (知識に基づく自然言語処)</li>
</ul>
<h2>Publication</h2>
<h3>International Conference</h3>
<ol>
<li><strong>Yuta Sasaki</strong>, Jianwei Zhang, Yuhki Shiraishi. Commonsense-aware Attentive Modeling for Humor Recognition. <a href="https://www.dexa.org/dexa2023">The 34th International Conference on Database and Expert Systems Applications (DEXA 2023)</a>. [<a href="https://link.springer.com/chapter/10.1007/978-3-031-39847-6_3">paper</a>] [slide (in preparation)]
<ul>
<li>Acceptance rate ?% (FYI, 20.4% in DEXA 2019)</li>
<li>Online presentation</li>
</ul>
</li>
</ol>
<h3>Domestic Conference (Japanese)</h3>
<ol>
<li><strong>佐々木 裕多</strong>, 張 建偉, 白石 優旗. Commonsense生成モデルを用いたユーモア検出の性能評価. 2021年度情報処理学会東北支部研究会 (山形大学).</li>
<li>小田 大翔, <strong>佐々木 裕多</strong>, 張 建偉. 動的グラフにおけるGNNを用いたリンク予測. 2022年度情報処理学会東北支部研究会 (岩手大学).</li>
<li>佐藤 将太, <strong>佐々木 裕多</strong>, 張 建偉. 応答戦略と共感表現を考慮した感情支援対話の応答生成. 2022年度情報処理学会東北支部研究会 (山形大学).</li>
<li><strong>佐々木 裕多</strong>, 張 建偉, 白石 優旗. Commonsense-aware AttentionとDiscrepancy Resolution Lossを用いたユーモア検出手法の提案. <a href="https://event.dbsj.org/deim2023/">第15回データ工学と情報マネジメントに関するフォーラム (DEIM 2023)</a>. [<a href="https://proceedings-of-deim.github.io/DEIM2023/1b-3-2.pdf">paper</a>]</li>
<li>谷 聡馬, <strong>佐々木 裕多</strong>, 張 建偉. ニュースコンテンツとソーシャルコンテクストを用いたフェイクニュースの早期自動検出. <a href="https://event.dbsj.org/deim2023/">第15回データ工学と情報マネジメントに関するフォーラム (DEIM 2023)</a>. [<a href="https://proceedings-of-deim.github.io/DEIM2023/1a-8-2.pdf">paper</a>]</li>
<li><strong>佐々木 裕多</strong>, 張 建偉. 漫才対話の収集及び自動アノテーションのパイプラインの検討. <a href="https://www.anlp.jp/proceedings/annual_meeting/2023/">言語処理学会第29回年次大会 (NLP 2023)</a>. [<a href="https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q4-14.pdf">paper</a>]</li>
<li>片岸 祥帆, 小原 涼馬, <strong>佐々木 裕多</strong>, 荒木 健治. 漫才対話構造の分析手法の検討. NLP若手の会 (YANS) 第18回シンポジウム. [<a href="https://drive.google.com/file/d/1YNA8Wh2uIF4WnTgXJsglse9THV90aBZq/view?usp=sharing">poster</a>]</li>
</ol>
<h2>Education</h2>
<p>2023.4 - <strong>Present</strong><br>
<strong>Master of Tokyo Institute of Technology, Tokyo, Japan / 東京工業大学</strong><br>
<em>School of Engineering Department of Information and Communications Engineering / 工学院 情報通信系</em></p>
<p>2019.4 - 2023.3<br>
<strong>Bachelor of Iwate University, Iwate, Japan / 岩手大学</strong><br>
<em>School of Computer, Intelligence and Media Technology, Department of Systems Innovation Engineering, Faculty of Science and Engineering / 理工学部 システム創成工学科 知能メディア・情報コース</em></p>
<h2>Scholorship</h2>
<ol>
<li>岩手大学理工学部修学支援奨学金 (令和3年4月 - 令和4年3月)</li>
</ol>
<h2>Job Experience</h2>
<p>Comming soon...</p>
<h2>Internship</h2>
<p>2023.8 - <strong>Present</strong><br>
<strong>Research Development Internship / R&#x26;Dインターンシップ</strong><br>
<a href="https://research.reazon.jp/"><em>Reazon Human Interaction Lab</em></a><br>
マルチモーダル／対話システム／音声認識</p>
<p>2023.9<br>
<strong>Machine Learning Engineer Internship / MLエンジニアインターンシップ @ CA Tech JOB</strong><br>
<em>CyberAgent 極予測LP</em><br>
トレーナー：石上亮介<br>
LLMを用いた自動対話評価基盤の構築と検討 [リンク準備中] [リンク準備中]</p>
<h2>Other Experiences</h2>
<ul>
<li><a href="https://sites.google.com/view/dslc6">対話システムライブコンペティション6オーガナイザ</a></li>
<li><a href="https://icasspeech.connpass.com/event/292978/">INTERSPEECH2023論文読み会登壇</a></li>
</ul>
<h2>Software</h2>
<h3><strong>dslclib</strong> [<a href="https://github.com/yuta0306/dslclib">code</a>] [<a href="https://yuta0306.github.io/dslclib/">docs</a>] [<a href="https://pypi.org/project/dslclib/">PyPI</a>]</h3>
<p>対話システムライブコンペティション6で使用できるPythonライブラリ．ロボットコンペの方でも使えるようになっている．</p>
<h3><strong>py-arib-parser</strong> [<a href="https://github.com/yuta0306/py-arib-parser">code</a>]</h3>
<p>ARIB形式の字幕を抽出するため，arib対応ffmpegをインストールするdockerfileと字幕ファイルを読み込むクラスを記述したライブラリ．<br>
抽出した字幕を，付与されたカラーコードでターミナルに出力することができる．</p>
<h2>Certification</h2>
<ul>
<li>TOEIC® Listening &#x26; Reading Test: 865 / 990</li>
<li>Python3 エンジニア認定基礎試験合格者</li>
<li>Python3 エンジニア認定データ分析試験合格者</li>
</ul>
<h2>Skills</h2>
<h3>Main</h3>
<ul>
<li>Python</li>
<li>JavaScript</li>
<li>HTML5</li>
<li>CSS3 / SCSS</li>
<li>Docker</li>
</ul>
<h3>Sub</h3>
<ul>
<li>GCP</li>
<li>TypeScript</li>
<li>AWS</li>
<li>C#</li>
<li>PHP</li>
<li>ROS</li>
</ul>
<h3>Frameworks</h3>
<ul>
<li>Flask</li>
<li>Pytorch / Pytorch Lightning</li>
<li>Pelican (SSG with Python)</li>
<li>React / Next.js (These framework are used in this blog)</li>
</ul>
<h2>Contact</h2>
<p>Email: yubo1336[at]lr.pi.titech.ac.jp<br>
Twitter: <a href="https://twitter.com/Sloth65557166">@Sloth65557166</a><br>
GitHub: <a href="https://github.com/yuta0306">@yuta0306</a><br>
Kaggle: <a href="https://www.kaggle.com/yutasasaki">@yutasasaki</a><br>
LinkedIn: <a href="https://www.linkedin.com/in/yuta-sasaki-170472226/">Yuta SASAKI</a><br>
Blog: <a href="https://yuta0306.github.io/">ゆうぼうの書跡棚</a></p></div></div></main></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"aboutData":{"contentHtml":"\u003ch2\u003eBio\u003c/h2\u003e\n\u003cp\u003eI am a graduate student at Tokyo Institute of Technology.\u003cbr\u003e\nI would like to research into human-like dialogue systems, and build my career to develop the society where humans and AI cooperate with each other.\u003cbr\u003e\nI'm wating for the invitation for internships and collaborative research.\u003cbr\u003e\nPlease feel free to contact me!\u003c/p\u003e\n\u003cp\u003e(日本語)\u003cbr\u003e\n東京工業大学で大学院生をしています．\u003cbr\u003e\nhuman-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．\u003cbr\u003e\nインターンシップや共同研究のお誘いをお待ちしております．\u003cbr\u003e\n気軽な連絡お待ちしております！\u003c/p\u003e\n\u003ch2\u003eResearch Interest\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDialogue System (対話システム)\u003c/li\u003e\n\u003cli\u003eMulti-modal Dialogue System (マルチモーダル対話システム)\u003c/li\u003e\n\u003cli\u003eCollaborative Dialogue System (共話システム？)\u003c/li\u003e\n\u003cli\u003eSpeech Dialogue System (音声対話システム)\u003c/li\u003e\n\u003cli\u003eHuman Interaction (ヒューマンインタラクション)\u003c/li\u003e\n\u003cli\u003eLLM (大規模言語モデル)\u003c/li\u003e\n\u003cli\u003eHumor Recognition (ユーモア検出)\u003c/li\u003e\n\u003cli\u003eKnowledge-Intensive NLP (知識に基づく自然言語処)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePublication\u003c/h2\u003e\n\u003ch3\u003eInternational Conference\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eYuta Sasaki\u003c/strong\u003e, Jianwei Zhang, Yuhki Shiraishi. Commonsense-aware Attentive Modeling for Humor Recognition. \u003ca href=\"https://www.dexa.org/dexa2023\"\u003eThe 34th International Conference on Database and Expert Systems Applications (DEXA 2023)\u003c/a\u003e. [\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-031-39847-6_3\"\u003epaper\u003c/a\u003e] [slide (in preparation)]\n\u003cul\u003e\n\u003cli\u003eAcceptance rate ?% (FYI, 20.4% in DEXA 2019)\u003c/li\u003e\n\u003cli\u003eOnline presentation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eDomestic Conference (Japanese)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e佐々木 裕多\u003c/strong\u003e, 張 建偉, 白石 優旗. Commonsense生成モデルを用いたユーモア検出の性能評価. 2021年度情報処理学会東北支部研究会 (山形大学).\u003c/li\u003e\n\u003cli\u003e小田 大翔, \u003cstrong\u003e佐々木 裕多\u003c/strong\u003e, 張 建偉. 動的グラフにおけるGNNを用いたリンク予測. 2022年度情報処理学会東北支部研究会 (岩手大学).\u003c/li\u003e\n\u003cli\u003e佐藤 将太, \u003cstrong\u003e佐々木 裕多\u003c/strong\u003e, 張 建偉. 応答戦略と共感表現を考慮した感情支援対話の応答生成. 2022年度情報処理学会東北支部研究会 (山形大学).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e佐々木 裕多\u003c/strong\u003e, 張 建偉, 白石 優旗. Commonsense-aware AttentionとDiscrepancy Resolution Lossを用いたユーモア検出手法の提案. \u003ca href=\"https://event.dbsj.org/deim2023/\"\u003e第15回データ工学と情報マネジメントに関するフォーラム (DEIM 2023)\u003c/a\u003e. [\u003ca href=\"https://proceedings-of-deim.github.io/DEIM2023/1b-3-2.pdf\"\u003epaper\u003c/a\u003e]\u003c/li\u003e\n\u003cli\u003e谷 聡馬, \u003cstrong\u003e佐々木 裕多\u003c/strong\u003e, 張 建偉. ニュースコンテンツとソーシャルコンテクストを用いたフェイクニュースの早期自動検出. \u003ca href=\"https://event.dbsj.org/deim2023/\"\u003e第15回データ工学と情報マネジメントに関するフォーラム (DEIM 2023)\u003c/a\u003e. [\u003ca href=\"https://proceedings-of-deim.github.io/DEIM2023/1a-8-2.pdf\"\u003epaper\u003c/a\u003e]\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e佐々木 裕多\u003c/strong\u003e, 張 建偉. 漫才対話の収集及び自動アノテーションのパイプラインの検討. \u003ca href=\"https://www.anlp.jp/proceedings/annual_meeting/2023/\"\u003e言語処理学会第29回年次大会 (NLP 2023)\u003c/a\u003e. [\u003ca href=\"https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q4-14.pdf\"\u003epaper\u003c/a\u003e]\u003c/li\u003e\n\u003cli\u003e片岸 祥帆, 小原 涼馬, \u003cstrong\u003e佐々木 裕多\u003c/strong\u003e, 荒木 健治. 漫才対話構造の分析手法の検討. NLP若手の会 (YANS) 第18回シンポジウム. [\u003ca href=\"https://drive.google.com/file/d/1YNA8Wh2uIF4WnTgXJsglse9THV90aBZq/view?usp=sharing\"\u003eposter\u003c/a\u003e]\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eEducation\u003c/h2\u003e\n\u003cp\u003e2023.4 - \u003cstrong\u003ePresent\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eMaster of Tokyo Institute of Technology, Tokyo, Japan / 東京工業大学\u003c/strong\u003e\u003cbr\u003e\n\u003cem\u003eSchool of Engineering Department of Information and Communications Engineering / 工学院 情報通信系\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e2019.4 - 2023.3\u003cbr\u003e\n\u003cstrong\u003eBachelor of Iwate University, Iwate, Japan / 岩手大学\u003c/strong\u003e\u003cbr\u003e\n\u003cem\u003eSchool of Computer, Intelligence and Media Technology, Department of Systems Innovation Engineering, Faculty of Science and Engineering / 理工学部 システム創成工学科 知能メディア・情報コース\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003eScholorship\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e岩手大学理工学部修学支援奨学金 (令和3年4月 - 令和4年3月)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eJob Experience\u003c/h2\u003e\n\u003cp\u003eComming soon...\u003c/p\u003e\n\u003ch2\u003eInternship\u003c/h2\u003e\n\u003cp\u003e2023.8 - \u003cstrong\u003ePresent\u003c/strong\u003e\u003cbr\u003e\n\u003cstrong\u003eResearch Development Internship / R\u0026#x26;Dインターンシップ\u003c/strong\u003e\u003cbr\u003e\n\u003ca href=\"https://research.reazon.jp/\"\u003e\u003cem\u003eReazon Human Interaction Lab\u003c/em\u003e\u003c/a\u003e\u003cbr\u003e\nマルチモーダル／対話システム／音声認識\u003c/p\u003e\n\u003cp\u003e2023.9\u003cbr\u003e\n\u003cstrong\u003eMachine Learning Engineer Internship / MLエンジニアインターンシップ @ CA Tech JOB\u003c/strong\u003e\u003cbr\u003e\n\u003cem\u003eCyberAgent 極予測LP\u003c/em\u003e\u003cbr\u003e\nトレーナー：石上亮介\u003cbr\u003e\nLLMを用いた自動対話評価基盤の構築と検討 [リンク準備中] [リンク準備中]\u003c/p\u003e\n\u003ch2\u003eOther Experiences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sites.google.com/view/dslc6\"\u003e対話システムライブコンペティション6オーガナイザ\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://icasspeech.connpass.com/event/292978/\"\u003eINTERSPEECH2023論文読み会登壇\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSoftware\u003c/h2\u003e\n\u003ch3\u003e\u003cstrong\u003edslclib\u003c/strong\u003e [\u003ca href=\"https://github.com/yuta0306/dslclib\"\u003ecode\u003c/a\u003e] [\u003ca href=\"https://yuta0306.github.io/dslclib/\"\u003edocs\u003c/a\u003e] [\u003ca href=\"https://pypi.org/project/dslclib/\"\u003ePyPI\u003c/a\u003e]\u003c/h3\u003e\n\u003cp\u003e対話システムライブコンペティション6で使用できるPythonライブラリ．ロボットコンペの方でも使えるようになっている．\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003epy-arib-parser\u003c/strong\u003e [\u003ca href=\"https://github.com/yuta0306/py-arib-parser\"\u003ecode\u003c/a\u003e]\u003c/h3\u003e\n\u003cp\u003eARIB形式の字幕を抽出するため，arib対応ffmpegをインストールするdockerfileと字幕ファイルを読み込むクラスを記述したライブラリ．\u003cbr\u003e\n抽出した字幕を，付与されたカラーコードでターミナルに出力することができる．\u003c/p\u003e\n\u003ch2\u003eCertification\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTOEIC® Listening \u0026#x26; Reading Test: 865 / 990\u003c/li\u003e\n\u003cli\u003ePython3 エンジニア認定基礎試験合格者\u003c/li\u003e\n\u003cli\u003ePython3 エンジニア認定データ分析試験合格者\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSkills\u003c/h2\u003e\n\u003ch3\u003eMain\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eJavaScript\u003c/li\u003e\n\u003cli\u003eHTML5\u003c/li\u003e\n\u003cli\u003eCSS3 / SCSS\u003c/li\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSub\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGCP\u003c/li\u003e\n\u003cli\u003eTypeScript\u003c/li\u003e\n\u003cli\u003eAWS\u003c/li\u003e\n\u003cli\u003eC#\u003c/li\u003e\n\u003cli\u003ePHP\u003c/li\u003e\n\u003cli\u003eROS\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eFrameworks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFlask\u003c/li\u003e\n\u003cli\u003ePytorch / Pytorch Lightning\u003c/li\u003e\n\u003cli\u003ePelican (SSG with Python)\u003c/li\u003e\n\u003cli\u003eReact / Next.js (These framework are used in this blog)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eContact\u003c/h2\u003e\n\u003cp\u003eEmail: yubo1336[at]lr.pi.titech.ac.jp\u003cbr\u003e\nTwitter: \u003ca href=\"https://twitter.com/Sloth65557166\"\u003e@Sloth65557166\u003c/a\u003e\u003cbr\u003e\nGitHub: \u003ca href=\"https://github.com/yuta0306\"\u003e@yuta0306\u003c/a\u003e\u003cbr\u003e\nKaggle: \u003ca href=\"https://www.kaggle.com/yutasasaki\"\u003e@yutasasaki\u003c/a\u003e\u003cbr\u003e\nLinkedIn: \u003ca href=\"https://www.linkedin.com/in/yuta-sasaki-170472226/\"\u003eYuta SASAKI\u003c/a\u003e\u003cbr\u003e\nBlog: \u003ca href=\"https://yuta0306.github.io/\"\u003eゆうぼうの書跡棚\u003c/a\u003e\u003c/p\u003e","title":"About Me"}},"__N_SSG":true},"page":"/profile","query":{},"buildId":"ULwbJP9nZia2Ea0nrVo4J","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>