<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation<!-- --> | <!-- -->ゆうぼうの書跡棚</title><meta name="description" content="Multi-Hop Transformer for Document-Level Machine Translationのまとめ"/><meta name="og:description" content="Multi-Hop Transformer for Document-Level Machine Translationのまとめ"/><meta property="og:title" content="【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation"/><meta property="og:image" content="https://yuta0306.github.iohttps://s3.us-west-2.amazonaws.com/secure.notion-static.com/31894441-2dc1-4741-aa95-3d3a1d9b6411/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230521T180640Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=f3b170858b4feaeb830ff35992d79604941439a760b4f7426fe311b73b9298f5&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/><meta property="og:url" content="https://yuta0306.github.io/Multi-Hop-Transformer-for-Document-Level-Machine-Translation"/><script src="/js/toc.js"></script><meta name="next-head-count" content="9"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta property="og:title" content="ゆうぼうの書跡棚"/><meta property="og:type" content="website"/><meta property="og:url" content="https://yuta0306.github.io"/><meta property="og:image" content="https://yuta0306.github.io/images/default.png"/><meta property="og:site_name" content="ゆうぼうの書跡棚"/><meta property="og:description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/b31a7883dd1db9d4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b31a7883dd1db9d4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a6e19106a865540a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a6e19106a865540a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/664-60e06c839f82ba03.js" defer=""></script><script src="/_next/static/chunks/768-c61e8ddb09e59da7.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-cf387ad3e4ca362b.js" defer=""></script><script src="/_next/static/C3_RTYu8LDtRlpbNSHQHc/_buildManifest.js" defer=""></script><script src="/_next/static/C3_RTYu8LDtRlpbNSHQHc/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Home_container__bCOhY"><div class="header_container__IoqX_"><div class="header_container__inner__pDDDU"><header class="header_header__pKEQL"><a href="/"><div class="header_header__title__uoTF0">ゆうぼうの書跡棚</div></a></header><div class="header_hamburger__kYfxY"><div><img src="/icons/hamburger.png"/></div></div><nav class="header_nav__closed__1h469"><ul class="header_nav__list__eqFqF"><li class="header_nav__item__FNSzb"><a href="/about">About</a></li><li class="header_nav__item_active__qVXxE"><a href="/">Blog</a></li><li class="header_nav__item__FNSzb"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdgyok9pi697ZJvVizRNEw0qghDWz517k1FrbcRmfvvERlraA/viewform">Contact</a></li></ul></nav></div></div><div class="header_category__WFSrL"><ul class="header_category__items____MmN"></ul></div><div class="main_main__VZQGI"><div class="main_main__container__PFqpL"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__V_9fG"><div itemscope="" itemType="http://schema.org/BlogPosting"><div style="background:url(https://s3.us-west-2.amazonaws.com/secure.notion-static.com/31894441-2dc1-4741-aa95-3d3a1d9b6411/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230521T180640Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=f3b170858b4feaeb830ff35992d79604941439a760b4f7426fe311b73b9298f5&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject);overflow:hidden"><div class="Home_thumbnail__xs1Hd" itemscope="" itemProp="image" itemType="https://schema.org/ImageObject"><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/31894441-2dc1-4741-aa95-3d3a1d9b6411/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230521T180640Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=f3b170858b4feaeb830ff35992d79604941439a760b4f7426fe311b73b9298f5&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject" alt="【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation" loading="lazy" style="height:100%;width:auto;margin:0 auto;display:block"/></div></div><time dateTime="2023-05-21" style="color:rgb(144, 144, 144)">2023-05-21</time><h1>【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation</h1><div id="TOC__mobile"></div><article style="margin-top:4rem" itemscope="" itemProp="text"><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css">
</head>
<body>
<h2>概要</h2>
<p>Document-level neural machine translationにおいて，Multi-Hopなアーキテクチャを導入することにより，従来手法と比べて精度の高い文脈を考慮した機械翻訳を実現</p>
<p>翻訳者のように，頭の中に翻訳のドラフトを作り，文脈に合わせて適切に修正する流れ（human-like draft-editing）を明示的にモデリング</p>
<p>大きな事前学習済みモデルを使うことなく，使用に足る機械翻訳モデルを実現</p>
<h2>提案手法</h2>
<p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c2619e4c-99b6-4874-8d32-2f7a07bb54a3/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180650Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=4a9ebbe96f6c5752e6fc236e0119656f4b12b60887449c2c7aa040705e510b1b&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject" alt=""></p>
<p>アーキテクチャ周りのこと</p>
<h3>Sentence Encoder</h3>
<p>source-sideとtarget-sideでそれぞれPretrained Encoderがあり，source contextとtarget draftの分散表現をそれぞれ得る</p>
<h3>Multi-Hop Encoder</h3>
<p>source-contextにおいて文章ごとのreasoningをして，現在の文章の分散表現を得る</p>
<h3>Multi-Hop Decoder</h3>
<p>target-side draftから情報を取得して，翻訳の確率分布を得る</p>
<p>そのほかアーキテクチャの工夫</p>
<h3>Contet Gating</h3>
<p>contextual informationを過剰にutilizeしすぎないように，context gating machanismを採用</p>
<p>contextと現在の文章間の重みを動的にコントロールする</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mi>a</mi></msub><msubsup><mi>A</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msub><mi>W</mi><mi>b</mi></msub><msubsup><mi>B</mi><mrow><mi>s</mi><mo>−</mo><mi>i</mi></mrow><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha = \sigma(W_a A_s^{(n)} + W_b B_{s-i}^{(n)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2948em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.5834em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1166em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.38em;vertical-align:-0.3352em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3352em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> where sigma is logistic sigmoid function</p>
<h2>新規性</h2>
<p>Docment-level NMTにおける従来手法の問題点</p>
<ol>
<li>文章間のreasoningの特徴づけを明示的に行うことなく，単純にcontextの分散表現を導入</li>
<li>推論時にはアクセスできないのに，訓練時には追加入力としてのtarget contextにground-truthなデータを入力</li>
</ol>
<p>↑　訓練時と推論時において状況が異なる</p>
<p>Document-level NMTにおいてMulti-Hop reasoningをモデリングしたMulti-Hop Transformerの提案と提案モデルによるDocument-level NMTの大きな性能改善</p>
<p>target contextにground-truthで訓練すると推論時にはアクセスできないため，他の翻訳モデルの翻訳結果を使用することで，訓練時と推論時の状況を同じにした</p>
<h2>実験</h2>
<p>Baseline</p>
<p>Transformer</p>
<p>CA-Transformer</p>
<p>CA-HAN</p>
<p>CADec</p>
<p>計算量のオーバーヘッドを改善するためSentence Encoderはそれぞれのsideでパラメータを共有</p>
<h2>まとめ</h2>
<p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e9aa2659-c723-4d78-b619-a2fca604a864/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.26.30.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180722Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=3c66aee5b1251f84f761a12c1b07c5f26d1114707c08ad8eb206be319a3d6204&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject" alt=""></p>
<p>large-scaleな事前学習済み言語モデルを使用することなく，SoTA翻訳クオリティを達成</p>
<p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/648e061a-037a-47b0-9ccd-cb5e34f0584a/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.28.39.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180725Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=d8e366424556a63c360223cb870221afa415f0600f1de00904ef774da473a490&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject" alt=""></p>
<p>contextを付与するためのAttentionの構造は，ConcatやHierarchicalよりもMulti-HopなAttentionが効果があり</p>
<p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/7b8e8b86-406c-4fca-ac8d-f024c3d62def/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.30.06.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180728Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=02d6ca507eda285f489eb36d05990ce6b4189bcdd66b43c4cf724d22483fd766&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject" alt=""></p>
<p>contextを考慮する幅のwindow sizeは大きくするほど効果が上がるわけではなく．3が最も良かった</p>
<p>4以上にすると悪化傾向らしく，本研究ではwindow size = 3 を採用</p>
<p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/38aeab8f-9c30-446e-b0a5-c01920ef5946/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.36.58.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180731Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=27f587002275197d83d43ed3fcc6ac2f71ea54f41e42d6d313e5619a7b726883&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject" alt=""></p>
<p>contextにおいてreasoningするときの方向は，一般的な読み順の通りleft-to-rightで順方向にreasoningさせた方が結果は良かった</p>
<p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4f9ed4db-e0ac-48b0-9f5d-44af8ae2d2c5/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.38.40.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180733Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=5819e87b937d230adb78032278ea562724af2a498fd6ffff524e7b434f34dd8b&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject" alt=""></p>
<p>訓練時と推論時にtarget draftに与える文章が異なる問題への対処に関する実験結果</p>
<p>Referenceはground-truthをtarget draftとして与えて訓練，Draftはpre-trained MT systemが生成した翻訳結果をtarget draftとして与えて訓練したモデル</p>
<p>Draftの方が結果がよく，pre-trained MT systemの生成結果をtarget draftとする方法によって訓練時と推論時のギャップの橋渡しになることを示唆する結果</p>
<h2>その他（なぜ通ったか？等）</h2>
<h2>次読みたい論文</h2>
<p><a href="/5955ca444629476ebf23e66629a2413f">Context-Aware Self-Attention Networks</a></p>
</body>
</html>
</article><div class="socialshare_container__SSXJE"><h3>タメになったらSHARE!!!</h3><div class="socialshare_container__links__JZs4j"><a target="_blank" href="https://twitter.com/share?url=https://yuta0306.github.io/Multi-Hop-Transformer-for-Document-Level-Machine-Translation"><img src="/icons/twitter.png" loading="lazy" alt="https://yuta0306.github.io/Multi-Hop-Transformer-for-Document-Level-Machine-TranslationをTwitterに共有する"/></a><a target="_blank" href="https://www.facebook.com/share.php?u=https://yuta0306.github.io/Multi-Hop-Transformer-for-Document-Level-Machine-Translation"><img src="/icons/facebook.png" loading="lazy" alt="https://yuta0306.github.io/Multi-Hop-Transformer-for-Document-Level-Machine-TranslationをFacebookに共有する"/></a><a target="_blank" href="http://b.hatena.ne.jp/entry/https:/yuta0306.github.io/Multi-Hop-Transformer-for-Document-Level-Machine-Translation"><img src="/icons/hatenablog.png" loading="lazy" alt="https://yuta0306.github.io/Multi-Hop-Transformer-for-Document-Level-Machine-Translationをはてなブログに共有する"/></a></div></div></div></div></main><aside class="main_sidebar__tM28d"><div class="shortbio_container__4psan" itemscope="" itemProp="author" itemType="http://schema.org/Person"><div class="shortbio_container__image__eljVd"><img src="/images/profile.jpeg" alt="ゆうぼう" loading="lazy"/></div><h3 class="shortbio_author__A2bKB" itemscope="" itemProp="name">ゆうぼう</h3><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">国立大学院M1のナマケモノです．</p></div><div><p class="shortbio_container__paragraph__EJbWG">human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．</p></div><div><p class="shortbio_container__paragraph__EJbWG">学部時代はコモンセンスを利用したユーモア検出の研究を行っていました(Knowledge-intensive NLP)．</p></div><div><p class="shortbio_container__paragraph__EJbWG">このブログはNext.jsで書いてます．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">Kaggle等のデータ分析コンペは活動休止中．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div></div><div class="followme_container__T1oVi"><h3 class="followme_container__header__Pt5SP">Follow Me</h3><div class="followme_container__links__b3XW5"><a target="_blank" href="https://github.com/yuta0306"><img src="/icons/github.png" alt="GitHub"/></a><a target="_blank" href="https://kaggle.com/yutasasaki"><img src="/icons/kaggle.png" alt="Kaggle"/></a><a target="_blank" href="https://twitter.com/Sloth65557166"><img src="/icons/twitter.png" alt="Twitter"/></a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="categories_container__J8nCF"><h3 class="categories_container__header__zt836">Categories</h3><div class="categories_container__links__MFrVK"><a class="categories_container__link__AuvVr" href="/category/%E8%AB%96%E6%96%87/1">論文</a><a class="categories_container__link__AuvVr" href="/category/Web/1">Web</a><a class="categories_container__link__AuvVr" href="/category/JavaScript/1">JavaScript</a><a class="categories_container__link__AuvVr" href="/category/Competition/1">Competition</a><a class="categories_container__link__AuvVr" href="/category/Cloud/1">Cloud</a><a class="categories_container__link__AuvVr" href="/category/Linux/1">Linux</a><a class="categories_container__link__AuvVr" href="/category/Python/1">Python</a><a class="categories_container__link__AuvVr" href="/category/ML/1">ML</a><a class="categories_container__link__AuvVr" href="/category/Go/1">Go</a><a class="categories_container__link__AuvVr" href="/category/SQL/1">SQL</a></div></div><div class="tags_container___e3ez"><h3 class="tags_container__header__hxPW8">Tags</h3><div class="tags_container__links__X38Ga"><a class="tags_container__link__1Ts3a" href="/tag/Apache/1">Apache</a><a class="tags_container__link__1Ts3a" href="/tag/Appium/1">Appium</a><a class="tags_container__link__1Ts3a" href="/tag/atmaCup/1">atmaCup</a><a class="tags_container__link__1Ts3a" href="/tag/AWS/1">AWS</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS7/1">CentOS7</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS8/1">CentOS8</a><a class="tags_container__link__1Ts3a" href="/tag/Colab/1">Colab</a><a class="tags_container__link__1Ts3a" href="/tag/COMET,mental%20health,NLP,mental%20state%20knowledge,mentalisation,Contrasive%20Learning,MentalRoBERTa,KC-Net/1">COMET,mental health,NLP,mental state knowledge,mentalisation,Contrasive Learning,MentalRoBERTa,KC-Net</a><a class="tags_container__link__1Ts3a" href="/tag/conda/1">conda</a><a class="tags_container__link__1Ts3a" href="/tag/CSS/1">CSS</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system/1">dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,Internet-Augmented/1">dialogue system,Internet-Augmented</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,knowledge-base/1">dialogue system,knowledge-base</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,NLI/1">dialogue system,NLI</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,persona,Prompt-Tuning/1">dialogue system,persona,Prompt-Tuning</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system,survey,DST/1">dialogue system,survey,DST</a><a class="tags_container__link__1Ts3a" href="/tag/DST/1">DST</a><a class="tags_container__link__1Ts3a" href="/tag/ESPNet/1">ESPNet</a><a class="tags_container__link__1Ts3a" href="/tag/ffmpeg/1">ffmpeg</a><a class="tags_container__link__1Ts3a" href="/tag/Flask/1">Flask</a><a class="tags_container__link__1Ts3a" href="/tag/Gating%20Mechanism/1">Gating Mechanism</a><a class="tags_container__link__1Ts3a" href="/tag/Go/1">Go</a><a class="tags_container__link__1Ts3a" href="/tag/Google%20Colaboratory/1">Google Colaboratory</a><a class="tags_container__link__1Ts3a" href="/tag/Heroku/1">Heroku</a><a class="tags_container__link__1Ts3a" href="/tag/Highway%20Transformer/1">Highway Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/HTML/1">HTML</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection/1">humor detection</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection,multi-modal/1">humor detection,multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/JavaScript/1">JavaScript</a><a class="tags_container__link__1Ts3a" href="/tag/JSON/1">JSON</a><a class="tags_container__link__1Ts3a" href="/tag/Kaggle/1">Kaggle</a><a class="tags_container__link__1Ts3a" href="/tag/laughter,shared%20laughter/1">laughter,shared laughter</a><a class="tags_container__link__1Ts3a" href="/tag/Linux/1">Linux</a><a class="tags_container__link__1Ts3a" href="/tag/Mac/1">Mac</a><a class="tags_container__link__1Ts3a" href="/tag/make/1">make</a><a class="tags_container__link__1Ts3a" href="/tag/map/1">map</a><a class="tags_container__link__1Ts3a" href="/tag/MeCab/1">MeCab</a><a class="tags_container__link__1Ts3a" href="/tag/ML/1">ML</a><a class="tags_container__link__1Ts3a" href="/tag/MT,transformer,Multi-Hop%20Transformer/1">MT,transformer,Multi-Hop Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/multi-modal/1">multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/MySQL/1">MySQL</a><a class="tags_container__link__1Ts3a" href="/tag/NLP/1">NLP</a><a class="tags_container__link__1Ts3a" href="/tag/Node/1">Node</a><a class="tags_container__link__1Ts3a" href="/tag/node.js/1">node.js</a><a class="tags_container__link__1Ts3a" href="/tag/npm/1">npm</a><a class="tags_container__link__1Ts3a" href="/tag/Pandas/1">Pandas</a><a class="tags_container__link__1Ts3a" href="/tag/Poetry/1">Poetry</a><a class="tags_container__link__1Ts3a" href="/tag/Python/1">Python</a><a class="tags_container__link__1Ts3a" href="/tag/Pytorch/1">Pytorch</a><a class="tags_container__link__1Ts3a" href="/tag/pytorch-lightning/1">pytorch-lightning</a><a class="tags_container__link__1Ts3a" href="/tag/Scikit-learn/1">Scikit-learn</a><a class="tags_container__link__1Ts3a" href="/tag/Selenium/1">Selenium</a><a class="tags_container__link__1Ts3a" href="/tag/Self-Dependency-Units%20(SDU)/1">Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/SISR/1">SISR</a><a class="tags_container__link__1Ts3a" href="/tag/subprocess/1">subprocess</a><a class="tags_container__link__1Ts3a" href="/tag/Super-Resolution/1">Super-Resolution</a><a class="tags_container__link__1Ts3a" href="/tag/survey/1">survey</a><a class="tags_container__link__1Ts3a" href="/tag/survey,dialogue%20system/1">survey,dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive%20NLP/1">survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP</a><a class="tags_container__link__1Ts3a" href="/tag/tensorflow/1">tensorflow</a><a class="tags_container__link__1Ts3a" href="/tag/Tkinter/1">Tkinter</a><a class="tags_container__link__1Ts3a" href="/tag/transformer/1">transformer</a><a class="tags_container__link__1Ts3a" href="/tag/transformer,Highway%20Transformer,Gating%20Mechanism,Self-Dependency-Units%20(SDU)/1">transformer,Highway Transformer,Gating Mechanism,Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/zsh/1">zsh</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91/1">オブジェクト指向</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%82%B3%E3%83%AC%E3%83%BC%E3%82%BF/1">デコレータ</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90/1">データ分析</a><a class="tags_container__link__1Ts3a" href="/tag/%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/1">特殊メソッド</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%B6%85%E8%A7%A3%E5%83%8F/1">超解像</a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div id="TOC"></div></aside></div></div><footer class="footer_footer__WCChH"><div class="footer_footer__inner__287VQ"><div><a class="footer_footer__link__Ql5Ng" href="/privacy-policy">プライバシーポリシー</a></div><div class="footer_footer__title__PRn_u"><a href="/">ゆうぼうの書跡棚</a></div><div class="footer_footer__small__RlIHP"><small>Powered by <a target="_blank" class="footer_footer__small__link__u5kuV" href="https://twitter.com/Sloth65557166">ゆうぼう</a></small></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"contentHtml":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eDocument-level neural machine translationにおいて，Multi-Hopなアーキテクチャを導入することにより，従来手法と比べて精度の高い文脈を考慮した機械翻訳を実現\u003c/p\u003e\n\u003cp\u003e翻訳者のように，頭の中に翻訳のドラフトを作り，文脈に合わせて適切に修正する流れ（human-like draft-editing）を明示的にモデリング\u003c/p\u003e\n\u003cp\u003e大きな事前学習済みモデルを使うことなく，使用に足る機械翻訳モデルを実現\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c2619e4c-99b6-4874-8d32-2f7a07bb54a3/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180650Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=4a9ebbe96f6c5752e6fc236e0119656f4b12b60887449c2c7aa040705e510b1b\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eアーキテクチャ周りのこと\u003c/p\u003e\n\u003ch3\u003eSentence Encoder\u003c/h3\u003e\n\u003cp\u003esource-sideとtarget-sideでそれぞれPretrained Encoderがあり，source contextとtarget draftの分散表現をそれぞれ得る\u003c/p\u003e\n\u003ch3\u003eMulti-Hop Encoder\u003c/h3\u003e\n\u003cp\u003esource-contextにおいて文章ごとのreasoningをして，現在の文章の分散表現を得る\u003c/p\u003e\n\u003ch3\u003eMulti-Hop Decoder\u003c/h3\u003e\n\u003cp\u003etarget-side draftから情報を取得して，翻訳の確率分布を得る\u003c/p\u003e\n\u003cp\u003eそのほかアーキテクチャの工夫\u003c/p\u003e\n\u003ch3\u003eContet Gating\u003c/h3\u003e\n\u003cp\u003econtextual informationを過剰にutilizeしすぎないように，context gating machanismを採用\u003c/p\u003e\n\u003cp\u003econtextと現在の文章間の重みを動的にコントロールする\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003c/msub\u003e\u003cmsubsup\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msubsup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/msub\u003e\u003cmsubsup\u003e\u003cmi\u003eB\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msubsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha = \\sigma(W_a A_s^{(n)} + W_b B_{s-i}^{(n)})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.0037em;\"\u003eα\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2948em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1514em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ea\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.0448em;\"\u003e\u003cspan style=\"top:-2.5834em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003es\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2198em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003en\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1166em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.38em;vertical-align:-0.3352em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3361em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.0448em;\"\u003e\u003cspan style=\"top:-2.4231em;margin-left:-0.0502em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003es\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2198em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003en\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3352em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e where sigma is logistic sigmoid function\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003eDocment-level NMTにおける従来手法の問題点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e文章間のreasoningの特徴づけを明示的に行うことなく，単純にcontextの分散表現を導入\u003c/li\u003e\n\u003cli\u003e推論時にはアクセスできないのに，訓練時には追加入力としてのtarget contextにground-truthなデータを入力\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e↑　訓練時と推論時において状況が異なる\u003c/p\u003e\n\u003cp\u003eDocument-level NMTにおいてMulti-Hop reasoningをモデリングしたMulti-Hop Transformerの提案と提案モデルによるDocument-level NMTの大きな性能改善\u003c/p\u003e\n\u003cp\u003etarget contextにground-truthで訓練すると推論時にはアクセスできないため，他の翻訳モデルの翻訳結果を使用することで，訓練時と推論時の状況を同じにした\u003c/p\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eBaseline\u003c/p\u003e\n\u003cp\u003eTransformer\u003c/p\u003e\n\u003cp\u003eCA-Transformer\u003c/p\u003e\n\u003cp\u003eCA-HAN\u003c/p\u003e\n\u003cp\u003eCADec\u003c/p\u003e\n\u003cp\u003e計算量のオーバーヘッドを改善するためSentence Encoderはそれぞれのsideでパラメータを共有\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e9aa2659-c723-4d78-b619-a2fca604a864/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.26.30.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180722Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=3c66aee5b1251f84f761a12c1b07c5f26d1114707c08ad8eb206be319a3d6204\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003elarge-scaleな事前学習済み言語モデルを使用することなく，SoTA翻訳クオリティを達成\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/648e061a-037a-47b0-9ccd-cb5e34f0584a/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.28.39.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180725Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=d8e366424556a63c360223cb870221afa415f0600f1de00904ef774da473a490\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003econtextを付与するためのAttentionの構造は，ConcatやHierarchicalよりもMulti-HopなAttentionが効果があり\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/7b8e8b86-406c-4fca-ac8d-f024c3d62def/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.30.06.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180728Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=02d6ca507eda285f489eb36d05990ce6b4189bcdd66b43c4cf724d22483fd766\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003econtextを考慮する幅のwindow sizeは大きくするほど効果が上がるわけではなく．3が最も良かった\u003c/p\u003e\n\u003cp\u003e4以上にすると悪化傾向らしく，本研究ではwindow size = 3 を採用\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/38aeab8f-9c30-446e-b0a5-c01920ef5946/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.36.58.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180731Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=27f587002275197d83d43ed3fcc6ac2f71ea54f41e42d6d313e5619a7b726883\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003econtextにおいてreasoningするときの方向は，一般的な読み順の通りleft-to-rightで順方向にreasoningさせた方が結果は良かった\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4f9ed4db-e0ac-48b0-9f5d-44af8ae2d2c5/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.38.40.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026#x26;X-Amz-Date=20230521T180733Z\u0026#x26;X-Amz-Expires=3600\u0026#x26;X-Amz-Signature=5819e87b937d230adb78032278ea562724af2a498fd6ffff524e7b434f34dd8b\u0026#x26;X-Amz-SignedHeaders=host\u0026#x26;x-id=GetObject\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e訓練時と推論時にtarget draftに与える文章が異なる問題への対処に関する実験結果\u003c/p\u003e\n\u003cp\u003eReferenceはground-truthをtarget draftとして与えて訓練，Draftはpre-trained MT systemが生成した翻訳結果をtarget draftとして与えて訓練したモデル\u003c/p\u003e\n\u003cp\u003eDraftの方が結果がよく，pre-trained MT systemの生成結果をtarget draftとする方法によって訓練時と推論時のギャップの橋渡しになることを示唆する結果\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/5955ca444629476ebf23e66629a2413f\"\u003eContext-Aware Self-Attention Networks\u003c/a\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n","Title":"【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation","Date":"2023-05-21","Category":"論文","Tags":"MT,transformer,Multi-Hop Transformer","Authos":"ゆうぼう","Slug":"Multi-Hop-Transformer-for-Document-Level-Machine-Translation","Thumbnail":"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/31894441-2dc1-4741-aa95-3d3a1d9b6411/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Content-Sha256=UNSIGNED-PAYLOAD\u0026X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230521T180640Z\u0026X-Amz-Expires=3600\u0026X-Amz-Signature=f3b170858b4feaeb830ff35992d79604941439a760b4f7426fe311b73b9298f5\u0026X-Amz-SignedHeaders=host\u0026x-id=GetObject","Description":"Multi-Hop Transformer for Document-Level Machine Translationのまとめ","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET,mental health,NLP,mental state knowledge,mentalisation,Contrasive Learning,MentalRoBERTa,KC-Net","conda","CSS","dialogue system","dialogue system,Internet-Augmented","dialogue system,knowledge-base","dialogue system,NLI","dialogue system,persona,Prompt-Tuning","dialogue system,survey,DST","DST","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","humor detection,multi-modal","JavaScript","JSON","Kaggle","laughter,shared laughter","Linux","Mac","make","map","MeCab","ML","MT,transformer,Multi-Hop Transformer","multi-modal","MySQL","NLP","Node","node.js","npm","Pandas","Poetry","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","SISR","subprocess","Super-Resolution","survey","survey,dialogue system","survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP","tensorflow","Tkinter","transformer","transformer,Highway Transformer,Gating Mechanism,Self-Dependency-Units (SDU)","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"]},"__N_SSG":true},"page":"/[slug]","query":{"slug":"Multi-Hop-Transformer-for-Document-Level-Machine-Translation"},"buildId":"C3_RTYu8LDtRlpbNSHQHc","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>