<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>「<!-- -->論文<!-- -->」<!-- -->1<!-- -->ページ目 | <!-- -->ゆうぼうの書跡棚</title><meta name="next-head-count" content="3"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><link rel="alternate" type="application/rss+xml" href="https:/yuta0306.github.io/feed.xml" title="RSS2.0"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/75506965150cde8a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/75506965150cde8a.css" data-n-g=""/><link rel="preload" href="/_next/static/css/71fbbc4c2f48f099.css" as="style"/><link rel="stylesheet" href="/_next/static/css/71fbbc4c2f48f099.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/664-60e06c839f82ba03.js" defer=""></script><script src="/_next/static/chunks/768-c61e8ddb09e59da7.js" defer=""></script><script src="/_next/static/chunks/pages/category/%5Bcategory%5D/%5Bpage%5D-6f58068add1ae00e.js" defer=""></script><script src="/_next/static/qAoBOyZoxCIoSGdt5co7N/_buildManifest.js" defer=""></script><script src="/_next/static/qAoBOyZoxCIoSGdt5co7N/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="header_container__IoqX_"><div class="header_container__inner__pDDDU"><header class="header_header__pKEQL"><a href="/"><h1 class="header_header__title__uoTF0">ゆうぼうの書跡棚</h1></a></header><div class="header_hamburger__kYfxY"><div><img src="/icons/hamburger.png"/></div></div><nav class="header_nav__closed__1h469"><ul class="header_nav__list__eqFqF"><li class="header_nav__item__FNSzb"><a href="/about">About</a></li><li class="header_nav__item_active__qVXxE"><a href="/">Blog</a></li><li class="header_nav__item__FNSzb"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdgyok9pi697ZJvVizRNEw0qghDWz517k1FrbcRmfvvERlraA/viewform">Contact</a></li></ul></nav></div></div><div class="header_category__WFSrL"><ul class="header_category__items____MmN"><a class="header_category__item__0CmfH" href="/category/%E8%AB%96%E6%96%87/1"><li>論文</li></a><a class="header_category__item__0CmfH" href="/category/Web/1"><li>Web</li></a><a class="header_category__item__0CmfH" href="/category/JavaScript/1"><li>JavaScript</li></a><a class="header_category__item__0CmfH" href="/category/Competition/1"><li>Competition</li></a><a class="header_category__item__0CmfH" href="/category/Cloud/1"><li>Cloud</li></a><a class="header_category__item__0CmfH" href="/category/Linux/1"><li>Linux</li></a><a class="header_category__item__0CmfH" href="/category/Python/1"><li>Python</li></a><a class="header_category__item__0CmfH" href="/category/ML/1"><li>ML</li></a><a class="header_category__item__0CmfH" href="/category/Go/1"><li>Go</li></a><a class="header_category__item__0CmfH" href="/category/SQL/1"><li>SQL</li></a></ul></div><div class="main_main__VZQGI"><div class="main_main__container__PFqpL"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__grid__Bpzhl"><div class="card_card__container__PrCEE"><a href="/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/default.png" alt="【論文まとめ】対話システムはどのように話すべきか" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-07-10" itemProp="published">2023-07-10</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】対話システムはどのように話すべきか</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation.png" alt="【論文まとめ】CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-07-10" itemProp="published">2023-07-10</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey.png" alt="【論文まとめ】Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-22" itemProp="published">2023-05-22</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey.png" alt="【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-22" itemProp="published">2023-05-22</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/A-survey-on-empathetic-dialogue-systems"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/A-survey-on-empathetic-dialogue-systems.png" alt="【論文まとめ】A survey on empathetic dialogue systems" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-22" itemProp="published">2023-05-22</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】A survey on empathetic dialogue systems</h2></div></div></a></div><ins class="adsbygoogle " style="display:block;border-bottom:1px dashed rgba(240, 240, 240, 0.6)" data-ad-client="ca-pub-4998278830587376" data-ad-slot="3060159795" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="card_card__container__PrCEE"><a href="/%E9%81%A0%E9%9A%94%E6%93%8D%E4%BD%9C%E3%82%A2%E3%83%B3%E3%83%89%E3%83%AD%E3%82%A4%E3%83%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E8%AA%AC%E5%BE%97%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AE%E5%8F%8E%E9%9B%86%E3%81%A8%E5%88%86%E6%9E%90"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析.png" alt="【論文まとめ】遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/%E7%9F%A5%E8%AD%98%E6%BA%90%E3%81%A8%E3%81%AE%E4%B8%80%E5%AF%BE%E5%A4%9A%E9%96%A2%E4%BF%82%E3%82%92%E6%9C%89%E3%81%99%E3%82%8B%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AB%E3%82%88%E3%82%8B%E7%99%BA%E8%A9%B1%E7%94%9F%E6%88%90"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/知識源との一対多関係を有する対話コーパスによる発話生成.png" alt="【論文まとめ】知識源との一対多関係を有する対話コーパスによる発話生成" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】知識源との一対多関係を有する対話コーパスによる発話生成</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/%E5%88%86%E9%A1%9E%E3%83%A2%E3%83%87%E3%83%ABBERT%E3%81%AB%E3%82%88%E3%82%8B%E4%B8%8D%E6%95%B4%E5%90%88%E7%94%9F%E6%88%90%E6%96%87%E3%81%AE%E6%A4%9C%E5%87%BA%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/分類モデルBERTによる不整合生成文の検出について.png" alt="【論文まとめ】分類モデルBERTによる不整合生成文の検出について" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】分類モデルBERTによる不整合生成文の検出について</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Transformer%E3%81%AB%E3%82%88%E3%82%8Bhallucination-error%E3%81%AE%E4%BA%8B%E5%BE%8C%E4%BF%AE%E6%AD%A3"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Transformerによるhallucination-errorの事後修正.png" alt="【論文まとめ】Transformerによるhallucination errorの事後修正" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Transformerによるhallucination errorの事後修正</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Prompt-Tuning-%E3%81%AB%E3%82%88%E3%82%8B%E5%80%8B%E6%80%A7%E3%82%92%E6%8C%81%E3%81%A3%E3%81%9F%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Prompt-Tuning-による個性を持った対話システムの構築.png" alt="【論文まとめ】Prompt-Tuning による個性を持った対話システムの構築" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Prompt-Tuning による個性を持った対話システムの構築</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue.png" alt="【論文まとめ】Prediction of Shared Laughter for Human-Robot Dialogue" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Prediction of Shared Laughter for Human-Robot Dialogue</h2></div></div></a></div><ins class="adsbygoogle " style="display:block;border-bottom:1px dashed rgba(240, 240, 240, 0.6)" data-ad-client="ca-pub-4998278830587376" data-ad-slot="3060159795" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="card_card__container__PrCEE"><a href="/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms.png" alt="【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Multi-Hop-Transformer-for-Document-Level-Machine-Translation"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Multi-Hop-Transformer-for-Document-Level-Machine-Translation.png" alt="【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Internet-Augmented-Dialogue-Generation"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Internet-Augmented-Dialogue-Generation.png" alt="【論文まとめ】Internet-Augmented Dialogue Generation" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Internet-Augmented Dialogue Generation</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png" alt="【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/A-mental-state-Knowledge–aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media.png" alt="【論文まとめ】A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media</h2></div></div></a></div><div class="card_card__container__PrCEE"><a href="/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models"><div class="card_card__s4JKv"><div class="card_card__thumbnail__XavfB"><img src="/images/thumbnails/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models.png" alt="【論文まとめ】A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models" class="card_card__thumbnail__img__fusOc" loading="lazy"/></div><div><div class="card_card__meta__NjslI"><time dateTime="2023-05-21" itemProp="published">2023-05-21</time><a itemscope="" role="author" itemType="http://schema.org/Person" href="/about"><span itemProp="name">ゆうぼう</span></a></div><h2 class="card_card__title__Nn7hR">【論文まとめ】A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models</h2></div></div></a></div><ins class="adsbygoogle " style="display:block;border-bottom:1px dashed rgba(240, 240, 240, 0.6)" data-ad-client="ca-pub-4998278830587376" data-ad-slot="3060159795" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="paginager_container____vsL"><ul class="paginager_container__pagers__1zmK9"><a class="paginager_container__pager__GaEzu" href="/category/%E8%AB%96%E6%96%87/1"><li class="paginager_container__pager__page__2YOZs">&lt;&lt;</li></a><li class="paginager_container__pager_deactive__Gjmav">&lt;</li><li class="paginager_container__pager_active__k3Sib">1</li><li class="paginager_container__pager_deactive__Gjmav">&gt;</li><a class="paginager_container__pager__GaEzu" href="/category/%E8%AB%96%E6%96%87/1"><li class="paginager_container__pager__page__2YOZs">&gt;&gt;</li></a></ul></div></div></main><aside class="main_sidebar__tM28d"><div class="shortbio_container__4psan" itemscope="" itemProp="author" itemType="http://schema.org/Person"><div class="shortbio_container__image__eljVd"><img src="/images/profile.jpeg" alt="ゆうぼう" loading="lazy"/></div><h3 class="shortbio_author__A2bKB" itemscope="" itemProp="name">ゆうぼう</h3><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">国立大学院M1のナマケモノです．</p></div><div><p class="shortbio_container__paragraph__EJbWG">human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．</p></div><div><p class="shortbio_container__paragraph__EJbWG">学部時代はコモンセンスを利用したユーモア検出の研究を行っていました(Knowledge-intensive NLP)．</p></div><div><p class="shortbio_container__paragraph__EJbWG">このブログはNext.jsで書いてます．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">Kaggle等のデータ分析コンペは活動休止中．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div></div><div class="followme_container__T1oVi"><h3 class="followme_container__header__Pt5SP">Follow Me</h3><div class="followme_container__links__b3XW5"><a target="_blank" href="https://github.com/yuta0306"><img src="/icons/github.png" alt="GitHub"/></a><a target="_blank" href="https://kaggle.com/yutasasaki"><img src="/icons/kaggle.png" alt="Kaggle"/></a><a target="_blank" href="https://twitter.com/Sloth65557166"><img src="/icons/twitter.png" alt="Twitter"/></a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="categories_container__J8nCF"><h3 class="categories_container__header__zt836">Categories</h3><div class="categories_container__links__MFrVK"><a class="categories_container__link__AuvVr" href="/category/%E8%AB%96%E6%96%87/1">論文</a><a class="categories_container__link__AuvVr" href="/category/Web/1">Web</a><a class="categories_container__link__AuvVr" href="/category/JavaScript/1">JavaScript</a><a class="categories_container__link__AuvVr" href="/category/Competition/1">Competition</a><a class="categories_container__link__AuvVr" href="/category/Cloud/1">Cloud</a><a class="categories_container__link__AuvVr" href="/category/Linux/1">Linux</a><a class="categories_container__link__AuvVr" href="/category/Python/1">Python</a><a class="categories_container__link__AuvVr" href="/category/ML/1">ML</a><a class="categories_container__link__AuvVr" href="/category/Go/1">Go</a><a class="categories_container__link__AuvVr" href="/category/SQL/1">SQL</a></div></div><div class="tags_container___e3ez"><h3 class="tags_container__header__hxPW8">Tags</h3><div class="tags_container__links__X38Ga"><a class="tags_container__link__1Ts3a" href="/tag/Apache/1">Apache</a><a class="tags_container__link__1Ts3a" href="/tag/Appium/1">Appium</a><a class="tags_container__link__1Ts3a" href="/tag/atmaCup/1">atmaCup</a><a class="tags_container__link__1Ts3a" href="/tag/AWS/1">AWS</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS7/1">CentOS7</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS8/1">CentOS8</a><a class="tags_container__link__1Ts3a" href="/tag/Colab/1">Colab</a><a class="tags_container__link__1Ts3a" href="/tag/COMET/1">COMET</a><a class="tags_container__link__1Ts3a" href="/tag/commonsense/1">commonsense</a><a class="tags_container__link__1Ts3a" href="/tag/conda/1">conda</a><a class="tags_container__link__1Ts3a" href="/tag/Contrasive%20Learning/1">Contrasive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/Contrastive%20Learning/1">Contrastive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/CSS/1">CSS</a><a class="tags_container__link__1Ts3a" href="/tag/Dialogue%20Structure%20Learning/1">Dialogue Structure Learning</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system/1">dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/DST/1">DST</a><a class="tags_container__link__1Ts3a" href="/tag/empathetic%20dialogue%20system/1">empathetic dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/encyclopedic/1">encyclopedic</a><a class="tags_container__link__1Ts3a" href="/tag/ESPNet/1">ESPNet</a><a class="tags_container__link__1Ts3a" href="/tag/ffmpeg/1">ffmpeg</a><a class="tags_container__link__1Ts3a" href="/tag/Flask/1">Flask</a><a class="tags_container__link__1Ts3a" href="/tag/Gating%20Mechanism/1">Gating Mechanism</a><a class="tags_container__link__1Ts3a" href="/tag/Go/1">Go</a><a class="tags_container__link__1Ts3a" href="/tag/Google%20Colaboratory/1">Google Colaboratory</a><a class="tags_container__link__1Ts3a" href="/tag/Heroku/1">Heroku</a><a class="tags_container__link__1Ts3a" href="/tag/Highway%20Transformer/1">Highway Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/HTML/1">HTML</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection/1">humor detection</a><a class="tags_container__link__1Ts3a" href="/tag/Internet-Augmented/1">Internet-Augmented</a><a class="tags_container__link__1Ts3a" href="/tag/JavaScript/1">JavaScript</a><a class="tags_container__link__1Ts3a" href="/tag/JSON/1">JSON</a><a class="tags_container__link__1Ts3a" href="/tag/Kaggle/1">Kaggle</a><a class="tags_container__link__1Ts3a" href="/tag/KC-Net/1">KC-Net</a><a class="tags_container__link__1Ts3a" href="/tag/knowledge-base/1">knowledge-base</a><a class="tags_container__link__1Ts3a" href="/tag/Knowledge-Intensive%20NLP/1">Knowledge-Intensive NLP</a><a class="tags_container__link__1Ts3a" href="/tag/laughter/1">laughter</a><a class="tags_container__link__1Ts3a" href="/tag/Linux/1">Linux</a><a class="tags_container__link__1Ts3a" href="/tag/Mac/1">Mac</a><a class="tags_container__link__1Ts3a" href="/tag/make/1">make</a><a class="tags_container__link__1Ts3a" href="/tag/map/1">map</a><a class="tags_container__link__1Ts3a" href="/tag/MeCab/1">MeCab</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20health/1">mental health</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20state%20knowledge/1">mental state knowledge</a><a class="tags_container__link__1Ts3a" href="/tag/mentalisation/1">mentalisation</a><a class="tags_container__link__1Ts3a" href="/tag/MentalRoBERTa/1">MentalRoBERTa</a><a class="tags_container__link__1Ts3a" href="/tag/ML/1">ML</a><a class="tags_container__link__1Ts3a" href="/tag/MT/1">MT</a><a class="tags_container__link__1Ts3a" href="/tag/Multi-Hop%20Transformer/1">Multi-Hop Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/multi-modal/1">multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/MySQL/1">MySQL</a><a class="tags_container__link__1Ts3a" href="/tag/NLG/1">NLG</a><a class="tags_container__link__1Ts3a" href="/tag/NLI/1">NLI</a><a class="tags_container__link__1Ts3a" href="/tag/NLP/1">NLP</a><a class="tags_container__link__1Ts3a" href="/tag/Node/1">Node</a><a class="tags_container__link__1Ts3a" href="/tag/node.js/1">node.js</a><a class="tags_container__link__1Ts3a" href="/tag/npm/1">npm</a><a class="tags_container__link__1Ts3a" href="/tag/Pandas/1">Pandas</a><a class="tags_container__link__1Ts3a" href="/tag/persona/1">persona</a><a class="tags_container__link__1Ts3a" href="/tag/PLMKE/1">PLMKE</a><a class="tags_container__link__1Ts3a" href="/tag/Poetry/1">Poetry</a><a class="tags_container__link__1Ts3a" href="/tag/Prompt-Tuning/1">Prompt-Tuning</a><a class="tags_container__link__1Ts3a" href="/tag/Python/1">Python</a><a class="tags_container__link__1Ts3a" href="/tag/Pytorch/1">Pytorch</a><a class="tags_container__link__1Ts3a" href="/tag/pytorch-lightning/1">pytorch-lightning</a><a class="tags_container__link__1Ts3a" href="/tag/Scikit-learn/1">Scikit-learn</a><a class="tags_container__link__1Ts3a" href="/tag/Selenium/1">Selenium</a><a class="tags_container__link__1Ts3a" href="/tag/Self-Dependency-Units%20(SDU)/1">Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/shared%20laughter/1">shared laughter</a><a class="tags_container__link__1Ts3a" href="/tag/SISR/1">SISR</a><a class="tags_container__link__1Ts3a" href="/tag/subprocess/1">subprocess</a><a class="tags_container__link__1Ts3a" href="/tag/Super-Resolution/1">Super-Resolution</a><a class="tags_container__link__1Ts3a" href="/tag/survey/1">survey</a><a class="tags_container__link__1Ts3a" href="/tag/tensorflow/1">tensorflow</a><a class="tags_container__link__1Ts3a" href="/tag/Tkinter/1">Tkinter</a><a class="tags_container__link__1Ts3a" href="/tag/transformer/1">transformer</a><a class="tags_container__link__1Ts3a" href="/tag/zsh/1">zsh</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91/1">オブジェクト指向</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%82%B3%E3%83%AC%E3%83%BC%E3%82%BF/1">デコレータ</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90/1">データ分析</a><a class="tags_container__link__1Ts3a" href="/tag/%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/1">特殊メソッド</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%81%9E%E3%81%8D%E6%89%8B%E5%8F%8D%E5%BF%9C/1">聞き手反応</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%B6%85%E8%A7%A3%E5%83%8F/1">超解像</a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div id="TOC"></div></aside></div></div><footer class="footer_footer__WCChH"><div class="footer_footer__inner__287VQ"><div><a class="footer_footer__link__Ql5Ng" href="/privacy-policy">プライバシーポリシー</a></div><div class="footer_footer__title__PRn_u"><a href="/">ゆうぼうの書跡棚</a></div><div class="footer_footer__small__RlIHP"><small>Powered by <a target="_blank" class="footer_footer__small__link__u5kuV" href="https://twitter.com/Sloth65557166">ゆうぼう</a></small></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"CategoricalPostData":[{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: 対話システムはどのように話すべきか\u003c/p\u003e\n\u003cp\u003e研究会: 日本音響学会誌\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: 聞き手反応, dialogue system\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://www.jstage.jst.go.jp/article/jasj/78/5/78_283/_pdf\"\u003ehttps://www.jstage.jst.go.jp/article/jasj/78/5/78_283/_pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"https://doi.org/10.20697/jasj.78.5_283\"\u003ehttps://doi.org/10.20697/jasj.78.5_283\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e「\u003cstrong\u003e聞き手反応\u003c/strong\u003e」をキーワードに，対話システムが発する音声をどのように生成するべきかという観点から音声対話システムとの自然な対話の実現に迫った研究の紹介．\u003c/p\u003e\n\u003ch2\u003e聞き手反応\u003c/h2\u003e\n\u003cp\u003e会話は話し手と聞き手の相互行為により産出されるもの\u003c/p\u003e\n\u003cp\u003e聞き手の反応を「相槌表現」として次の分類（Den et al., 2011）\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/ake32nmz.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e聞き手の行動の観察は分析上不可欠\u003c/strong\u003eであるが，音声対話システム研究ではほとんど考慮されない\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e人はシステムに対して，（人同士の対話と異なり）ほとんど聞き手反応をしない\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e人は，どのようにして声でシステムを操作するかを学\u003c/p\u003e\n\u003cp\u003e→ 機械は機械であり，人と同じようにコミュニケーションができないことを知っている\u003c/p\u003e\n\u003ch2\u003e聞き手反応を誘発する話し方\u003c/h2\u003e\n\u003cp\u003e音声対話システム vs 人における違い\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e発話タイミングを含むターンテイキング\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e話し方\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e人同士の対話における発話は，必ずしも明瞭かつ流暢ではない．\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e高津ら（2018）の研究では，人同士の対話にのみ存在する話し方の特徴を要因に分解し，次のコードで表す．\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/3w6tk316.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/acfpbbc4.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eそして，以下の3条件でWoZ対話的なことを行う\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/hqhzjfod.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/lyovs6wf.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e発話計画例と実験の結果は次のとおり\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/yodnhyta.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/r2at9ofw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e機械的な発話をすると，聞き手反応が減る\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e自発音声コーパスを元にした音声で話すエージェント（飯塚ら, 2019）\u003c/h2\u003e\n\u003cp\u003e読み上げ音声コーパスではなく，自然な会話コーパスを用いて音声合成を訓練することで，読み上げと会話の音声のギャップを埋めうる\u003c/p\u003e\n\u003cp\u003eしかし，そのような試みは少ない\u003c/p\u003e\n\u003cp\u003e以下が考えうる要因\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e声のプロと違い，普通の人々の音声は正確性，明瞭性に欠けるから，音声合成に相応しくないという思い込み\u003c/li\u003e\n\u003cli\u003e会話音声の多様性．テキストだけでは説明できない音声のパラ言語的な特徴の変動が読み上げ音声に比べて大きく，モデル化が相対的に困難なこと\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e会話コーパスの小ささ．音声合成ではデータりょうが品質に直結するが，会話コーパスの場合，話者一人当たりのデータ量が少なく，音声のモデル化が困難なこと\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e筆者ら研究チームでは，とりわけこれがネックだった\u003c/li\u003e\n\u003cli\u003e→ ニューラルボコーダ（高木, 2019）の登場によりインパクトに\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e実験として，読み上げ独話音声コーパスJSUT vs 自発対話音声コーパスUUDB（宇都宮大学パラ言語情報研究向け音声対話データベース）\u003c/p\u003e\n\u003cp\u003e音声合成にはTacotron 2，スペクトルからの波形生成にはMelGAN\u003c/p\u003e\n\u003cp\u003eUUDBでは女性話者1名のデータでfine-tuning\u003c/p\u003e\n\u003cp\u003eMMDAgentを利用して音声対話システムを構築（研究仮説を検証するため）\u003c/p\u003e\n\u003cp\u003e国当てクイズの対話シナリオ\u003c/p\u003e\n\u003cp\u003e実験結果とその対話動画視聴から得たアンケート結果\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/863qbviy.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/gi2f1kw7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e自然な会話コーパス（ここではUUDB）を元にした合成音声で話すシステムとの対話の方が，人同士の対話に近い振る舞いをしている．\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e感情表出系感動詞及び笑いの数に有意な差は認められず\u003c/p\u003e\n\u003cp\u003e「不気味の谷」？？？\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e自然な音声対話の実現を望むならば，もっと本物の音声コミュニケーションと真剣に向き合わなければならないのではないか？\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@article{2022,\nauthor = {森 大毅},\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003edoi = {10.20697/jasj.78.5_283},\njournal = {日本音響学会誌},\nnumber = {5},\npages = {283-288},\ntitle = {対話システムはどのように話すべきか},\nvolume = {78},\nyear = {2022},\nbdsk-url-1 = {\u003ca href=\"https://doi.org/10.20697/jasj.78.5_283\"\u003ehttps://doi.org/10.20697/jasj.78.5_283\u003c/a\u003e}}\u003c/p\u003e","Title":"【論文まとめ】対話システムはどのように話すべきか","Date":"2023-07-10","Category":"論文","Tags":["聞き手反応","dialogue system"],"Authos":"ゆうぼう","Slug":"対話システムはどのように話すべきか","Description":"対話システムはどのように話すべきかのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation\u003c/p\u003e\n\u003cp\u003e研究会: WWW\u003c/p\u003e\n\u003cp\u003e年度: 2023\u003c/p\u003e\n\u003cp\u003eキーワード: Dialogue Structure Learning, dialogue system, Contrastive Learning, NLG\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://arxiv.org/pdf/2303.01094.pdf\"\u003ehttps://arxiv.org/pdf/2303.01094.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"https://doi.org/10.1145/3543507.3583285\"\u003ehttps://doi.org/10.1145/3543507.3583285\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eコード: \u003ca href=\"https://github.com/lemonsis/CTRLStruct\"\u003ehttps://github.com/lemonsis/CTRLStruct\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: PERSONA-CHAT, DailyDialog\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e対話構造の発見は対話生成において重要\u003c/p\u003e\n\u003cp\u003eよく構造化されたトピックのフローは背景情報を利用でき，未来のトピックを予測でき，コントローラブルで説明可能な応答生成に役立つ\u003c/p\u003e\n\u003cp\u003eラベルなしの情報を用いて，トピックレベルの対話クラスターとその遷移を効果的に見つける対話構造学習のフレームワーク\u003cstrong\u003eCTRLStruct\u003c/strong\u003eを提案\u003c/p\u003e\n\u003cp\u003ePersonachatとDailyDialogの二つのデータセットを用いて，提案モデルはより一貫した応答を生成し，対話の発話のrepresentationにおいて，いくつかの一般的なsentence embedding手法の性能を超えることを示した．\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/5xkyz45g.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eContrastive Utterance Representation Learning\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eAbsolute Correlation\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/njccm74r.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSimCLRに従った自己教師なし対照学習を行う\u003c/p\u003e\n\u003cp\u003e対象サンプルをデータ拡張して，同サンプルから拡張されたサンプルを近づけるように学習\u003c/p\u003e\n\u003cp\u003eこの時以下の拡張がランダムで行われる（実装より）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e単語の追加（ContextualWordEmbsAug: insert）\u003c/li\u003e\n\u003cli\u003e単語の置換（ContextualWordEmbsAug: substitute）\u003c/li\u003e\n\u003cli\u003e同義語による置換（SynonymAug）\u003c/li\u003e\n\u003cli\u003e元サンプルのまま\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eRelative Correlation\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eある時点の発話は前の発話と意味的に近く，次の発話とは近い関係性を持つが，その関係性は弱いという想定で実装される\u003c/p\u003e\n\u003cp\u003eそれぞれは，\u003cstrong\u003eStrong Relativity\u003c/strong\u003e，\u003cstrong\u003eWeak Relativity\u003c/strong\u003eで，対照学習として計算される．\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/v161lamo.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/4ehbyyyf.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.375ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.108ex\" height=\"1.945ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 1373.8 859.6\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-1-TEX-I-1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D459\" xlink:href=\"#MJX-1-TEX-I-1D459\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(331,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D446\" xlink:href=\"#MJX-1-TEX-I-1D446\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(645,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-1-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eは発話ペア間で強い関係性を学習し，\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.375ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.753ex\" height=\"1.945ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 1658.7 859.6\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-2-TEX-I-1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-2-TEX-I-1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-2-TEX-I-1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D459\" xlink:href=\"#MJX-2-TEX-I-1D459\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(331,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D44A\" xlink:href=\"#MJX-2-TEX-I-1D44A\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(1048,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-2-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eは\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.027ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.319ex\" height=\"1.597ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 583 706\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-3-TEX-I-1D706\" d=\"M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D706\" xlink:href=\"#MJX-3-TEX-I-1D706\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eを係数として導入することで，発話と次の発話間で弱い関係性を学習する\u003c/p\u003e\n\u003cp\u003eこれらは\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.375ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"15.936ex\" height=\"1.945ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 7043.6 859.6\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-4-TEX-I-1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-N-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-N-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D459\" xlink:href=\"#MJX-4-TEX-I-1D459\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(331,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-4-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(759,0)\"\u003e\u003cuse data-c=\"1D436\" xlink:href=\"#MJX-4-TEX-I-1D436\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(1732.9,0)\"\u003e\u003cuse data-c=\"3D\" xlink:href=\"#MJX-4-TEX-N-3D\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"msub\" transform=\"translate(2788.7,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D459\" xlink:href=\"#MJX-4-TEX-I-1D459\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(331,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D446\" xlink:href=\"#MJX-4-TEX-I-1D446\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(645,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-4-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(4384.7,0)\"\u003e\u003cuse data-c=\"2B\" xlink:href=\"#MJX-4-TEX-N-2B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"msub\" transform=\"translate(5384.9,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D459\" xlink:href=\"#MJX-4-TEX-I-1D459\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(331,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D44A\" xlink:href=\"#MJX-4-TEX-I-1D44A\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(1048,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-4-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eとして計算される．\u003c/p\u003e\n\u003cp\u003eバッチサイズNに対しては以下のように計算され，最終的な対照学習を用いたエンコーダが構成される．\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/1vxutkwi.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/cc2u35dd.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eDialogue Structure Modeling\u003c/h3\u003e\n\u003cp\u003e対話トピックの遷移確率の計算のため，imitation learningを適応する\u003c/p\u003e\n\u003cp\u003eトピックレベルはエンコーダの埋め込みのクラスタで表現できるが，このクラスタが強化学習における状態となる\u003c/p\u003e\n\u003cp\u003e元の状態はutterance representationで連続値だが，遷移先の状態をクラスタidとすると離散値になるため，クラスタ中心のベクトルを状態遷移先とすることで，状態を連続値として表現する\u003c/p\u003e\n\u003cp\u003eタスク設定としては，\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.025ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"0.817ex\" height=\"1.441ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -626 361 637\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-5-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-5-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e時刻における行動を\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.471ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.789ex\" height=\"1.471ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 1674.9 650\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-6-TEX-I-1D450\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-6-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-6-TEX-N-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-6-TEX-N-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D450\" xlink:href=\"#MJX-6-TEX-I-1D450\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(466,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-6-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(361,0)\"\u003e\u003cuse data-c=\"2B\" xlink:href=\"#MJX-6-TEX-N-2B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mn\" transform=\"translate(1139,0)\"\u003e\u003cuse data-c=\"31\" xlink:href=\"#MJX-6-TEX-N-31\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eとし，この行動は\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.471ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"4.113ex\" height=\"2.041ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 1817.9 902\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-7-TEX-I-210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"210E\" xlink:href=\"#MJX-7-TEX-I-210E\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(609,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-7-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(361,0)\"\u003e\u003cuse data-c=\"2B\" xlink:href=\"#MJX-7-TEX-N-2B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mn\" transform=\"translate(1139,0)\"\u003e\u003cuse data-c=\"31\" xlink:href=\"#MJX-7-TEX-N-31\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eが含まれるクラスタ中心のベクトルで表現される．\u003c/p\u003e\n\u003cp\u003eこの設定を踏まえ，MLEによって，ポリシー\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.355ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.228ex\" height=\"1.33ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -431 984.6 588.1\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-8-TEX-I-1D70B\" d=\"M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-8-TEX-I-1D703\" d=\"M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D70B\" xlink:href=\"#MJX-8-TEX-I-1D70B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(603,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D703\" xlink:href=\"#MJX-8-TEX-I-1D703\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eを推定する\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/omgk0ge7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e連続値の行動と状態におけるポリシーを表現するために正規分布を適応し，状態\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.446ex\" height=\"1.57ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 639 694\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-9-TEX-B-1D421\" d=\"M40 686L131 690Q222 694 223 694H229V533L230 372L238 381Q248 394 264 407T317 435T398 450Q428 450 448 447T491 434T529 402T551 346Q553 335 554 198V62H623V0H614Q596 3 489 3Q374 3 365 0H356V62H425V194V275Q425 348 416 373T371 399Q326 399 288 370T238 290Q236 281 235 171V62H304V0H295Q277 3 171 3Q64 3 46 0H37V62H106V332Q106 387 106 453T107 534Q107 593 105 605T91 620Q77 624 50 624H37V686H40Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D421\" xlink:href=\"#MJX-9-TEX-B-1D421\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eに対して，次で推定するポリシーが定義される\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/3aqswy0q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e(9)と(10)から，以下の式に変形される\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/qdb1e32h.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eA2CNet (Actor-to-Critice network)を用いて，\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.508ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 2434.6 1000\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-10-TEX-I-1D707\" d=\"M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-10-TEX-I-1D703\" d=\"M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-10-TEX-N-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-10-TEX-B-1D421\" d=\"M40 686L131 690Q222 694 223 694H229V533L230 372L238 381Q248 394 264 407T317 435T398 450Q428 450 448 447T491 434T529 402T551 346Q553 335 554 198V62H623V0H614Q596 3 489 3Q374 3 365 0H356V62H425V194V275Q425 348 416 373T371 399Q326 399 288 370T238 290Q236 281 235 171V62H304V0H295Q277 3 171 3Q64 3 46 0H37V62H106V332Q106 387 106 453T107 534Q107 593 105 605T91 620Q77 624 50 624H37V686H40Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-10-TEX-N-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D707\" xlink:href=\"#MJX-10-TEX-I-1D707\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(636,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D703\" xlink:href=\"#MJX-10-TEX-I-1D703\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(1017.6,0)\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-10-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(1406.6,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D421\" xlink:href=\"#MJX-10-TEX-B-1D421\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(2045.6,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-10-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eを推定する\u003c/p\u003e\n\u003cp\u003e報酬は得られないので，これは考慮していない\u003c/p\u003e\n\u003cp\u003e分散\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.73ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.485ex\" height=\"2.617ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -833.9 2424.6 1156.5\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-11-TEX-I-1D70E\" d=\"M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-11-TEX-N-32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-11-TEX-I-1D703\" d=\"M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-11-TEX-N-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-11-TEX-B-1D421\" d=\"M40 686L131 690Q222 694 223 694H229V533L230 372L238 381Q248 394 264 407T317 435T398 450Q428 450 448 447T491 434T529 402T551 346Q553 335 554 198V62H623V0H614Q596 3 489 3Q374 3 365 0H356V62H425V194V275Q425 348 416 373T371 399Q326 399 288 370T238 290Q236 281 235 171V62H304V0H295Q277 3 171 3Q64 3 46 0H37V62H106V332Q106 387 106 453T107 534Q107 593 105 605T91 620Q77 624 50 624H37V686H40Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-11-TEX-N-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msubsup\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D70E\" xlink:href=\"#MJX-11-TEX-I-1D70E\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mn\" transform=\"translate(604,363) scale(0.707)\"\u003e\u003cuse data-c=\"32\" xlink:href=\"#MJX-11-TEX-N-32\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(604,-315.5) scale(0.707)\"\u003e\u003cuse data-c=\"1D703\" xlink:href=\"#MJX-11-TEX-I-1D703\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(1007.6,0)\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-11-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(1396.6,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D421\" xlink:href=\"#MJX-11-TEX-B-1D421\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(2035.6,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-11-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eをパラメータ\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.023ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.061ex\" height=\"1.618ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -705 469 715\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-12-TEX-I-1D703\" d=\"M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D703\" xlink:href=\"#MJX-12-TEX-I-1D703\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eと独立した定数とおくことで，最終的な目的関数は平均二乗誤差の回帰問題に帰着できる\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/mz9dw4b8.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eこれをニューラルネットで解き，ポリシーを予測する\u003c/p\u003e\n\u003ch3\u003eControlled Generation\u003c/h3\u003e\n\u003cp\u003eNLLロスとKLダイバージェンスロスによってデコーダを最適化する\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/vl3pmhqt.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/cdfb2pis.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/j7onid54.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eラベルなしのコーパスを用いて，対話構造を見つけ，応答生成をコントロールするフレームワークを提案\n\u003col\u003e\n\u003cli\u003e対話構造の学習は，これまではタスク指向対話で主に取り組まれていたため，オープンドメイン対話に取り組んだことも新しい\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e対照学習を行うエンコーダによって，トピックレベルの対話構造モデリングの性能を改善するためにutterance representationの学習を増強する\u003c/li\u003e\n\u003cli\u003eimitation learning手法を提案し，トピックレベルの対話遷移確率の推定を行い，高品質な対話構造学習を行う．\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003e次の疑問を実験により明らかにする\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e他の強い対話生成モデルに対して，どのようにしてCTRLStructはオープンドメイン対話において動作するか？\u003c/li\u003e\n\u003cli\u003e本当に提案モデルは応答のトピックをコントロールするか？発見された対話構造は応答生成に役立つか？\u003c/li\u003e\n\u003cli\u003e他のsentence embedding手法に対して，提案モデルはutterance representationにおいてどのように動作するか？\u003c/li\u003e\n\u003cli\u003e提案モデルの汎化性能は？他のタイプのバックボーンを用いたモデルにも適応可能？\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eデータセットには，PersonaChatとDailyDialog\u003c/p\u003e\n\u003cp\u003e比較モデル\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSeq2Seq\u003c/li\u003e\n\u003cli\u003eCVAE\u003c/li\u003e\n\u003cli\u003eBART (large)\u003c/li\u003e\n\u003cli\u003eDialoGPT (medium)\u003c/li\u003e\n\u003cli\u003eBlenderBot (2.7B)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/1xkprovo.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eCTRLStructは全体的に他のモデルの性能を超える\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/15gvgtph.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eHTHA (Hard Topic Hit Accuracy)，STHA (Soft Topic Hit Accuracy)\u003c/p\u003e\n\u003cp\u003e与えられたkカテゴリと生成されたサンプルのマルチラベル分類で定式化され，トピック分類の性能を評価\u003c/p\u003e\n\u003cp\u003e（真のラベルは存在しないため，CTRLStructのクラスタリングのプロセスで得られるラベルを擬似ラベルとして使用．）\u003c/p\u003e\n\u003cp\u003ePersonaChatはDailyDialogと違い，トピックの多様性が低い\u003c/p\u003e\n\u003cp\u003e→ 実際に持っているトピックよりも多いトピックカテゴリを設定してしまったため（トピックのクラスタ数？），多くのノイズが現れて，CTRLStructが動作しなかったと考えられる\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/io3fzt35.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/qspsm2n1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eutterance representationの評価\u003c/p\u003e\n\u003cp\u003ePersonaChatでは，SimCSEがBERTよりも低い性能\u003c/p\u003e\n\u003cp\u003eCTRLStructはどの指標，どのデータセットでも比較手法を超える性能\u003c/p\u003e\n\u003cp\u003eFigure3から，CTRLStructは，クラスタ数に対してロバストと言えるらしい\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/bbhow0b4.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eGPT2を用いて，他のバックボーンに対するCTRLStructの効果を検証\u003c/p\u003e\n\u003cp\u003eGPT2に対して適応させても性能が向上していることがわかった．\u003c/p\u003e\n\u003cp\u003eただし，この手法はTransformerを想定しているため，Transformer構造を持たないモデルに対しては適応できない可能性がある\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/pj5s4u39.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation/qboygo1u.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@inproceedings{10.1145/3543507.3583285,\nauthor = {Yin, Congchi and Li, Piji and Ren, Zhaochun},\ntitle = {CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation},\nyear = {2023},\nisbn = {9781450394161},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {\u003ca href=\"https://doi.org/10.1145/3543507.3583285\"\u003ehttps://doi.org/10.1145/3543507.3583285\u003c/a\u003e},\ndoi = {10.1145/3543507.3583285},\nabstract = {Dialogue structure discovery is essential in dialogue generation. Well-structured topic flow can leverage background information and predict future topics to help generate controllable and explainable responses. However, most previous work focused on dialogue structure learning in task-oriented dialogue other than open-domain dialogue which is more complicated and challenging. In this paper, we present a new framework CTRLStruct for dialogue structure learning to effectively explore topic-level dialogue clusters as well as their transitions with unlabelled information. Precisely, dialogue utterances encoded by bi-directional Transformer are further trained through a special designed contrastive learning task to improve representation. Then we perform clustering to utterance-level representations and form topic-level clusters that can be considered as vertices in dialogue structure graph. The edges in the graph indicating transition probability between vertices are calculated by mimicking expert behavior in datasets. Finally, dialogue structure graph is integrated into dialogue model to perform controlled response generation. Experiments on two popular open-domain dialogue datasets show our model can generate more coherent responses compared to some excellent dialogue models, as well as outperform some typical sentence embedding methods in dialogue utterance representation. Code is available in GitHub1.},\nbooktitle = {Proceedings of the ACM Web Conference 2023},\npages = {1539–1550},\nnumpages = {12},\nkeywords = {Dialogue Structure Learning, Imitation Learning, Utterance Representation, Open-Domain Dialogue Generation, Contrastive Learning},\nlocation = {Austin, TX, USA},\nseries = {WWW '23}\n}\u003c/p\u003e\n\u003c/blockquote\u003e\u003cstyle\u003e\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] \u003e g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] \u003e rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e line[data-line], svg[data-table] \u003e g \u003e line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e rect[data-frame], svg[data-table] \u003e g \u003e rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dashed, svg[data-table] \u003e g \u003e .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dotted, svg[data-table] \u003e g \u003e .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e g \u003e svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool \u003e mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool \u003e mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n\u003c/style\u003e","Title":"【論文まとめ】CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation","Date":"2023-07-10","Category":"論文","Tags":["Dialogue Structure Learning","dialogue system","Contrastive Learning","NLG"],"Authos":"ゆうぼう","Slug":"CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation","Thumbnail":"/images/thumbnails/CTRLStruct-Dialogue-Structure-Learning-for-Open-Domain-Response-Generation.png","Description":"CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generationのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey\u003c/p\u003e\n\u003cp\u003e研究会: ACL SIGDIAL\u003c/p\u003e\n\u003cp\u003e年度: 2021\u003c/p\u003e\n\u003cp\u003eキーワード: dialogue system, survey, DST\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://aclanthology.org/2021.sigdial-1.25.pdf\"\u003ehttps://aclanthology.org/2021.sigdial-1.25.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003cp\u003eData State Tracking (以下DST) on Task-Oriented Dialogue Systemに焦点を当てたsurvey\u003c/p\u003e\n\u003ch2\u003eAbstract\u003c/h2\u003e\n\u003cp\u003e触れること\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eタスク\u003c/li\u003e\n\u003cli\u003eデータセット\u003c/li\u003e\n\u003cli\u003eevaluation metrics\u003c/li\u003e\n\u003cli\u003eアプローチ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本論文では，二つのDSTモデルをしっかり区別する．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estatic ontology DST models\n\u003cul\u003e\n\u003cli\u003e固定された対話状況集合を予測する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003edynamic ontology DST models\n\u003cul\u003e\n\u003cli\u003eオントロジーが変化した時でも対話状況を予測する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDefinition of ontology\u003c/p\u003e\n\u003cp\u003ea set of concepts and categories in a subject area or domain that shows their properties and the relations between them.\u003c/p\u003e\n\u003cp\u003e単一ドメインでも複数ドメインでもトラックすることや新しいドメインにスケーリングすることのモデルの性能について議論する\u003c/p\u003e\n\u003cp\u003eTerms: knowledge transfer, zero-shot learning\u003c/p\u003e\n\u003cp\u003eカバーしている年代は2013~2020\u003c/p\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eTask-oriented dialogue system:\u003c/p\u003e\n\u003cp\u003eユーザーがタスクを成し遂げるようにするシステム\u003c/p\u003e\n\u003cp\u003eチケット予約，レストラン予約，カスタマーサポートなど\u003c/p\u003e\n\u003cp\u003eユーザの要求を正確にトラッキングする性能は，一貫していて効果的な対話を可能にする\u003c/p\u003e\n\u003cp\u003e対話状況をslot-valueで表現するDSTコンポーネントを使った情報をトラッキングする\u003c/p\u003e\n\u003cp\u003e↑この精度がとても重要で，下流のコンポーネントがこの状況を利用して，次のactionを決定する\u003c/p\u003e\n\u003cp\u003eDSTタスクは，実際Natural Language Understanding (以下NLU)のタスクを統合している\u003c/p\u003e\n\u003cp\u003eただし，単なるslot filling taskよりも複雑になっている\u003c/p\u003e\n\u003cp\u003eDST\u003c/p\u003e\n\u003cp\u003e現在のturnまで，対話レベルでslot-valueを予測\u003c/p\u003e\n\u003cp\u003eSlot Filling\u003c/p\u003e\n\u003cp\u003e特定のturnのみ考慮してslot-valueを予測すれば良い\u003c/p\u003e\n\u003cp\u003eモデルとしては以下が提案されている\u003c/p\u003e\n\u003cp\u003eRNN-based models\u003c/p\u003e\n\u003cp\u003eAttention-based models\u003c/p\u003e\n\u003cp\u003eTransformer-based models\u003c/p\u003e\n\u003cp\u003eここ最近では，単一ドメインではなく，マルチドメインやflexibleにドメインの移行をするモデリングの研究が盛んらしい\u003c/p\u003e\n\u003ch2\u003eDialogue State Tracking\u003c/h2\u003e\n\u003cp\u003eそもそもDSTとは\u003c/p\u003e\n\u003ch3\u003eDialogue State\u003c/h3\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.357ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.152ex\" height=\"1.952ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -705 951.3 862.8\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-1-TEX-I-1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D446\" xlink:href=\"#MJX-1-TEX-I-1D446\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(646,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-1-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e: dialogue state\u003c/p\u003e\n\u003cp\u003e→turn \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.025ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"0.817ex\" height=\"1.441ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -626 361 637\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-2-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-2-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e までにおける対話履歴のsummary\u003c/p\u003e\n\u003cp\u003e次の行動を決定するための全ての十分な情報を含んでいる\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.025ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"0.817ex\" height=\"1.441ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -626 361 637\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-3-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-3-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e   : turn\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"11.732ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 5185.7 1000\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-4-TEX-N-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-N-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D463\" d=\"M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D462\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-I-1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-4-TEX-N-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mo\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-4-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(389,0)\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-4-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(858,0)\"\u003e\u003cuse data-c=\"1D459\" xlink:href=\"#MJX-4-TEX-I-1D459\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(1156,0)\"\u003e\u003cuse data-c=\"1D45C\" xlink:href=\"#MJX-4-TEX-I-1D45C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(1641,0)\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-4-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(2002,0)\"\u003e\u003cuse data-c=\"2C\" xlink:href=\"#MJX-4-TEX-N-2C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(2446.7,0)\"\u003e\u003cuse data-c=\"1D463\" xlink:href=\"#MJX-4-TEX-I-1D463\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(2931.7,0)\"\u003e\u003cuse data-c=\"1D44E\" xlink:href=\"#MJX-4-TEX-I-1D44E\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(3460.7,0)\"\u003e\u003cuse data-c=\"1D459\" xlink:href=\"#MJX-4-TEX-I-1D459\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(3758.7,0)\"\u003e\u003cuse data-c=\"1D462\" xlink:href=\"#MJX-4-TEX-I-1D462\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(4330.7,0)\"\u003e\u003cuse data-c=\"1D452\" xlink:href=\"#MJX-4-TEX-I-1D452\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(4796.7,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-4-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e: このペアで，ユーザの目的を捉える\u003c/p\u003e\n\u003cp\u003eslotはOntology \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.05ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.726ex\" height=\"1.643ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -704 763 726\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-5-TEX-I-1D442\" d=\"M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D442\" xlink:href=\"#MJX-5-TEX-I-1D442\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e の中で事前に定義されていて (ドメイン依存であるが)，\u003c/p\u003e\n\u003cp\u003evalueはユーザによって与えられた各スロット \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.023ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.061ex\" height=\"1.023ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 469 452\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-6-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-6-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e で決められる\u003c/p\u003e\n\u003cp\u003eレストランの例で言えば\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"48.816ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 21576.8 1000\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-7-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-7B\" d=\"M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D439\" d=\"M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D442\" d=\"M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D434\" d=\"M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D438\" d=\"M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-I-1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-7-TEX-N-7D\" d=\"M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-7-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(502,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-7-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(1085,0)\"\u003e\u003cuse data-c=\"3D\" xlink:href=\"#MJX-7-TEX-N-3D\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(2140.8,0)\"\u003e\u003cuse data-c=\"7B\" xlink:href=\"#MJX-7-TEX-N-7B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(2640.8,0)\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-7-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(3029.8,0)\"\u003e\u003cuse data-c=\"1D439\" xlink:href=\"#MJX-7-TEX-I-1D439\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(3778.8,0)\"\u003e\u003cuse data-c=\"1D442\" xlink:href=\"#MJX-7-TEX-I-1D442\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(4541.8,0)\"\u003e\u003cuse data-c=\"1D442\" xlink:href=\"#MJX-7-TEX-I-1D442\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(5304.8,0)\"\u003e\u003cuse data-c=\"1D437\" xlink:href=\"#MJX-7-TEX-I-1D437\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(6132.8,0)\"\u003e\u003cuse data-c=\"2C\" xlink:href=\"#MJX-7-TEX-N-2C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(6577.5,0)\"\u003e\u003cuse data-c=\"1D43C\" xlink:href=\"#MJX-7-TEX-I-1D43C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(7081.5,0)\"\u003e\u003cuse data-c=\"1D447\" xlink:href=\"#MJX-7-TEX-I-1D447\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(7785.5,0)\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-7-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(8535.5,0)\"\u003e\u003cuse data-c=\"1D43F\" xlink:href=\"#MJX-7-TEX-I-1D43F\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(9216.5,0)\"\u003e\u003cuse data-c=\"1D43C\" xlink:href=\"#MJX-7-TEX-I-1D43C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(9720.5,0)\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-7-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(10470.5,0)\"\u003e\u003cuse data-c=\"1D441\" xlink:href=\"#MJX-7-TEX-I-1D441\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(11358.5,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-7-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(11747.5,0)\"\u003e\u003cuse data-c=\"2C\" xlink:href=\"#MJX-7-TEX-N-2C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(12192.2,0)\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-7-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(12581.2,0)\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-7-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(13331.2,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-7-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(14090.2,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-7-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(14854.2,0)\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-7-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(15604.2,0)\"\u003e\u003cuse data-c=\"2C\" xlink:href=\"#MJX-7-TEX-N-2C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(16048.8,0)\"\u003e\u003cuse data-c=\"1D436\" xlink:href=\"#MJX-7-TEX-I-1D436\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(16808.8,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-7-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(17572.8,0)\"\u003e\u003cuse data-c=\"1D441\" xlink:href=\"#MJX-7-TEX-I-1D441\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(18460.8,0)\"\u003e\u003cuse data-c=\"1D447\" xlink:href=\"#MJX-7-TEX-I-1D447\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(19164.8,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-7-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(19923.8,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-7-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(20687.8,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-7-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(21076.8,0)\"\u003e\u003cuse data-c=\"7D\" xlink:href=\"#MJX-7-TEX-N-7D\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eのようになる\u003c/p\u003e\n\u003cp\u003eslotのタイプは二つ\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003einformable\n対話から得られる→FOODやAREA\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erequestable\nシステムが与える→ADRRESSやPHONE\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eDialogue State Tracker\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eturn-level prediction\n各ターンで与えられるslot-valueを予測\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003edialogue-level prediction\n各ターンでの完全な対話状況を予測\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eTurn-level prediction\u003c/h3\u003e\n\u003cp\u003e直近のturn \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.025ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"0.817ex\" height=\"1.441ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -626 361 637\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-8-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-8-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e からslot-valueを予測する\u003c/p\u003e\n\u003cp\u003erule-basedの場合は，そのルールに従って，\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.471ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.871ex\" height=\"1.471ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 1710.9 650\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-9-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-9-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-9-TEX-N-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-9-TEX-N-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-9-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(502,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-9-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(361,0)\"\u003e\u003cuse data-c=\"2212\" xlink:href=\"#MJX-9-TEX-N-2212\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mn\" transform=\"translate(1139,0)\"\u003e\u003cuse data-c=\"31\" xlink:href=\"#MJX-9-TEX-N-31\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eに統合して\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.357ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.826ex\" height=\"1.357ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 807.3 599.8\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-10-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-10-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-10-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(502,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-10-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eを得る\u003c/p\u003e\n\u003cp\u003eturn \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.025ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"0.817ex\" height=\"1.441ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -626 361 637\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-11-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-11-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e を優先したり，\u003c/p\u003e\n\u003cp\u003e確率を利用して統合したり\u003c/p\u003e\n\u003cp\u003elearning to updateの場合は，turn-levelの予測を入力として，対話状況を予測する方法を学習する\u003c/p\u003e\n\u003ch3\u003eDialogue level prediction\u003c/h3\u003e\n\u003cp\u003e各turn \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.025ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"0.817ex\" height=\"1.441ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -626 361 637\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-12-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-12-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e において，完全な対話履歴を入力として，完全な対話状況 \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.357ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.826ex\" height=\"1.357ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 807.3 599.8\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-13-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-13-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-13-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(502,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-13-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e を予測する\u003c/p\u003e\n\u003cp\u003e直前の対話状況 \u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.471ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.871ex\" height=\"1.471ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 1710.9 650\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-14-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-14-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-14-TEX-N-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-14-TEX-N-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-14-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(502,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-14-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(361,0)\"\u003e\u003cuse data-c=\"2212\" xlink:href=\"#MJX-14-TEX-N-2212\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mn\" transform=\"translate(1139,0)\"\u003e\u003cuse data-c=\"31\" xlink:href=\"#MJX-14-TEX-N-31\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e を考慮しないため，\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.471ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.871ex\" height=\"1.471ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 1710.9 650\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-15-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-15-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-15-TEX-N-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-15-TEX-N-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-15-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(502,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-15-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(361,0)\"\u003e\u003cuse data-c=\"2212\" xlink:href=\"#MJX-15-TEX-N-2212\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mn\" transform=\"translate(1139,0)\"\u003e\u003cuse data-c=\"31\" xlink:href=\"#MJX-15-TEX-N-31\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eと\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.357ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.826ex\" height=\"1.357ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -442 807.3 599.8\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-16-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-16-TEX-I-1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"msub\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-16-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(502,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D461\" xlink:href=\"#MJX-16-TEX-I-1D461\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eに一貫性がないこともある\u003c/p\u003e\n\u003ch2\u003eDatasets\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/p9w4y463.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDialog State Tracking Challenge (DSTC)\u003c/li\u003e\n\u003cli\u003eDSTC2 and DSTC3\u003c/li\u003e\n\u003cli\u003eWoZ2.0\u003c/li\u003e\n\u003cli\u003eMultiWoZ\u003c/li\u003e\n\u003cli\u003eSchema-Guided Dataset (SGD)\u003c/li\u003e\n\u003cli\u003eTreeDST\u003c/li\u003e\n\u003cli\u003eMachine-to-Machine (M2M)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eEvaluation Metrics\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAverage Goal Accuracy\u003c/li\u003e\n\u003cli\u003eJoint Goal Accuracy\u003c/li\u003e\n\u003cli\u003eRequested  Slots F1\u003c/li\u003e\n\u003cli\u003eTime Complexity\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eStatic Ontology DST Models\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/qegkxt8s.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eslot-valueは事前に定義されている\u003c/p\u003e\n\u003cp\u003e→\u003c/p\u003e\n\u003cp\u003eoutput layerは\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efeed-forward layer\n- slotとvalueが固定なので，それらはembeddingされているため可能\u003c/li\u003e\n\u003cli\u003esoftmax\n- 全てのslot-valueのペアの確率を求める\u003c/li\u003e\n\u003cli\u003esigmoid\n- それぞれのslot-valueの確率を求める\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eDelexicalization\u003c/h3\u003e\n\u003cp\u003eimbalanced training data for slot-valuesに対処する効果的なアプローチ\u003c/p\u003e\n\u003cp\u003e入力のslot valuesをラベルの名前に置き換える\u003c/p\u003e\n\u003cp\u003eI want Chinese food.\u003c/p\u003e\n\u003cp\u003e→ I want F.VALUE F.SLOT.\u003c/p\u003e\n\u003ch3\u003eData-driven DST\u003c/h3\u003e\n\u003cp\u003edelexicalizationは確かに効果的だが，手作業でのfeature engineeringが必要になる\u003c/p\u003e\n\u003cp\u003e→ data-drivenな手法が提案された\u003c/p\u003e\n\u003ch3\u003eParameter sharing\u003c/h3\u003e\n\u003cp\u003e昔のモデルはslotごとにエンコーダが分かれていた\u003c/p\u003e\n\u003cp\u003e→そのため全てのslotに対してパラメータを共有する手法が提案された\u003c/p\u003e\n\u003cp\u003eStateNet？\u003c/p\u003e\n\u003ch3\u003eRNN and latency in DST\u003c/h3\u003e\n\u003cp\u003e予測時間が問題だったため，それに対する対策の提案\u003c/p\u003e\n\u003ch3\u003eEncoder based on pre-trained LM\u003c/h3\u003e\n\u003cp\u003eBERTなどを使うことで，捕捉できるslot valueが増えた\u003c/p\u003e\n\u003ch2\u003eDynamic Ontology DST Models\u003c/h2\u003e\n\u003cp\u003eオントロジーが事前定義されていなくてもslot-valueをトラッキングする必要がある\u003c/p\u003e\n\u003cp\u003eアプローチは2種\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eユーザの入力からslot-valueをコピー\u003c/li\u003e\n\u003cli\u003eoutputにslot-valueを生成\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下図は2種のアプローチを合わせたアーキテクチャ\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/l30un8m9.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003estatic ontology vs dynamic ontology\u003c/p\u003e\n\u003cp\u003estaticだとvalueが有限だが，\u003c/p\u003e\n\u003cp\u003edynamicだとoutputの語彙数がとても大きくなる\u003c/p\u003e\n\u003ch3\u003eCopy and pointer networks\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/7vvgrpy7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003ecopy mechanismとpointer networkがメインのアプローチ\u003c/p\u003e\n\u003cp\u003eどちらもattention-based\u003c/p\u003e\n\u003cp\u003eXu氏とHu氏が提案したpointer networkベースのアーキテクチャだと，すべてのslotには適用できず，postprocessingが必要だった\u003c/p\u003e\n\u003cp\u003e→ Wu氏がTRADEというモデルを提案\u003c/p\u003e\n\u003cp\u003e全てのslotとdomainに関する全てのパラメータを共有していて，domain transferができるらしい\u003c/p\u003e\n\u003cp\u003ezero-shotアプローチと言える\u003c/p\u003e\n\u003ch3\u003eCategorical and non-categorical slot-values\u003c/h3\u003e\n\u003cp\u003enon-categoricalなslotは，オープンなvalue集合を受け入れることができる\u003c/p\u003e\n\u003cp\u003eZhang氏が提案した手法によれば\u003c/p\u003e\n\u003cp\u003eもしcategoricalのラベルがついていれば，outputは事前定義されたvalueに対する確率のスコアを出力\u003c/p\u003e\n\u003cp\u003enon-categoricalであれば，outputにはinput tokenからデコードされたものを出力\u003c/p\u003e\n\u003cp\u003eHeck氏は，TripPy (triple copy strategy)を提案\u003c/p\u003e\n\u003cp\u003eシナリオに応じてslot-valueを予測する\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eユーザに明示的に示された\u003c/li\u003e\n\u003cli\u003eシステムに示され，ユーザによって言及された\u003c/li\u003e\n\u003cli\u003e別のドメインのslotのために前の対話ターンにおいて示された\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eFunction-baed update\u003c/h3\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"55.351ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 24465 1000\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-17-TEX-N-7B\" d=\"M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D434\" d=\"M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D44C\" d=\"M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D442\" d=\"M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D449\" d=\"M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D438\" d=\"M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-N-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D448\" d=\"M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-I-1D443\" d=\"M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-17-TEX-N-7D\" d=\"M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mo\"\u003e\u003cuse data-c=\"7B\" xlink:href=\"#MJX-17-TEX-N-7B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(500,0)\"\u003e\u003cuse data-c=\"1D436\" xlink:href=\"#MJX-17-TEX-I-1D436\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(1260,0)\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-17-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(2010,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-17-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(2769,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-17-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(3528,0)\"\u003e\u003cuse data-c=\"1D44C\" xlink:href=\"#MJX-17-TEX-I-1D44C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(4291,0)\"\u003e\u003cuse data-c=\"1D442\" xlink:href=\"#MJX-17-TEX-I-1D442\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(5054,0)\"\u003e\u003cuse data-c=\"1D449\" xlink:href=\"#MJX-17-TEX-I-1D449\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(5823,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-17-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(6587,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-17-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(7346,0)\"\u003e\u003cuse data-c=\"2C\" xlink:href=\"#MJX-17-TEX-N-2C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(7790.7,0)\"\u003e\u003cuse data-c=\"1D437\" xlink:href=\"#MJX-17-TEX-I-1D437\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(8618.7,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-17-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(9382.7,0)\"\u003e\u003cuse data-c=\"1D43F\" xlink:href=\"#MJX-17-TEX-I-1D43F\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(10063.7,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-17-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(10827.7,0)\"\u003e\u003cuse data-c=\"1D447\" xlink:href=\"#MJX-17-TEX-I-1D447\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(11531.7,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-17-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(12295.7,0)\"\u003e\u003cuse data-c=\"2C\" xlink:href=\"#MJX-17-TEX-N-2C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(12740.3,0)\"\u003e\u003cuse data-c=\"1D437\" xlink:href=\"#MJX-17-TEX-I-1D437\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(13568.3,0)\"\u003e\u003cuse data-c=\"1D442\" xlink:href=\"#MJX-17-TEX-I-1D442\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(14331.3,0)\"\u003e\u003cuse data-c=\"1D441\" xlink:href=\"#MJX-17-TEX-I-1D441\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(15219.3,0)\"\u003e\u003cuse data-c=\"1D447\" xlink:href=\"#MJX-17-TEX-I-1D447\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(15923.3,0)\"\u003e\u003cuse data-c=\"1D436\" xlink:href=\"#MJX-17-TEX-I-1D436\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(16683.3,0)\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-17-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(17433.3,0)\"\u003e\u003cuse data-c=\"1D445\" xlink:href=\"#MJX-17-TEX-I-1D445\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(18192.3,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-17-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(18956.3,0)\"\u003e\u003cuse data-c=\"2C\" xlink:href=\"#MJX-17-TEX-N-2C\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(19401,0)\"\u003e\u003cuse data-c=\"1D448\" xlink:href=\"#MJX-17-TEX-I-1D448\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(20168,0)\"\u003e\u003cuse data-c=\"1D443\" xlink:href=\"#MJX-17-TEX-I-1D443\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(20919,0)\"\u003e\u003cuse data-c=\"1D437\" xlink:href=\"#MJX-17-TEX-I-1D437\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(21747,0)\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-17-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(22497,0)\"\u003e\u003cuse data-c=\"1D447\" xlink:href=\"#MJX-17-TEX-I-1D447\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(23201,0)\"\u003e\u003cuse data-c=\"1D438\" xlink:href=\"#MJX-17-TEX-I-1D438\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(23965,0)\"\u003e\u003cuse data-c=\"7D\" xlink:href=\"#MJX-17-TEX-N-7D\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eを使う\u003c/p\u003e\n\u003cp\u003eCARRYOVER: 前の対話状況を引き継ぐ\u003c/p\u003e\n\u003cp\u003eDELETE        : slot-valueを戻す\u003c/p\u003e\n\u003cp\u003eUPDATE       : slot-valueの予測を必要とし，対話状況を更新する\u003c/p\u003e\n\u003ch2\u003eTake-away Points\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e各スロットに多様なモデルを使うのは，汎化性能や効果的な表現を学習するのに限りがある\u003c/li\u003e\n\u003cli\u003eスロット間のパラメータシェアリングは効果的で，全てのスロットに対するパフォーマンスを改善する\u003c/li\u003e\n\u003cli\u003e大規模データセットを使うと，RNNはSOTAの性能が出る\u003c/li\u003e\n\u003cli\u003eRNNは，encoderとdecoderを両方使うと時間がかかる問題がある\u003c/li\u003e\n\u003cli\u003eattention-basedのcopying mechanismは効果的なアプローチであり，多くのSOTAモデルで採用されているアプローチ\u003c/li\u003e\n\u003cli\u003e小資源のドメインに対しては，事前学習済みの言語モデルを使用することで性能がよくなる\u003c/li\u003e\n\u003cli\u003e統計的な更新関数はルールベースの更新関数を超える性能を出す\u003c/li\u003e\n\u003cli\u003eドメインのスケーラビリティとモデルの柔軟性が問題の時，scheme-basedアプローチを使うとscheme内での変更を入れることが可能になる\n\u003col\u003e\n\u003cli\u003ezero-shotを含むtransfer learningが可能に\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eDSTモデルの大半は，事前学習済み言語モデルが使われている\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eDST Challenges and Future Directions\u003c/h2\u003e\n\u003cp\u003e現実世界の会話アプリにおいて新たなslotやdomainの追加は避けられない\u003c/p\u003e\n\u003ch3\u003eFew-shot and Zero-shot Models\u003c/h3\u003e\n\u003ch3\u003eData Augmentation and Data-efficient Models\u003c/h3\u003e\n\u003ch3\u003eDiverse Datasets\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/3pbaksol.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@inproceedings{balaraman-etal-2021-recent,\ntitle = \"Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey\",\nauthor = \"Balaraman, Vevake and\nSheikhalishahi, Seyedmostafa and\nMagnini, Bernardo\",\nbooktitle = \"Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue\",\nmonth = jul,\nyear = \"2021\",\naddress = \"Singapore and Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"\u003ca href=\"https://aclanthology.org/2021.sigdial-1.25\"\u003ehttps://aclanthology.org/2021.sigdial-1.25\u003c/a\u003e\",\npages = \"239--251\",\nabstract = \"This paper aims at providing a comprehensive overview of recent developments in dialogue state tracking (DST) for task-oriented conversational systems. We introduce the task, the main datasets that have been exploited as well as their evaluation metrics, and we analyze several proposed approaches. We distinguish between static ontology DST models, which predict a fixed set of dialogue states, and dynamic ontology models, which can predict dialogue states even when the ontology changes. We also discuss the model{'}s ability to track either single or multiple domains and to scale to new domains, both in terms of knowledge transfer and zero-shot learning. We cover a period from 2013 to 2020, showing a significant increase of multiple domain methods, most of them utilizing pre-trained language models.\",\n}\u003c/p\u003e\n\u003c/blockquote\u003e\u003cstyle\u003e\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] \u003e g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] \u003e rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e line[data-line], svg[data-table] \u003e g \u003e line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e rect[data-frame], svg[data-table] \u003e g \u003e rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dashed, svg[data-table] \u003e g \u003e .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dotted, svg[data-table] \u003e g \u003e .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e g \u003e svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool \u003e mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool \u003e mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n\u003c/style\u003e","Title":"【論文まとめ】Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey","Date":"2023-05-22","Category":"論文","Tags":["dialogue system","survey","DST"],"Authos":"ゆうぼう","Slug":"Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey","Thumbnail":"/images/thumbnails/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey.png","Description":"Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Surveyのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey\u003c/p\u003e\n\u003cp\u003e研究会: arxiv\u003c/p\u003e\n\u003cp\u003e年度: 2021\u003c/p\u003e\n\u003cp\u003eキーワード: survey, dialogue system\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://arxiv.org/pdf/2105.04387.pdf\"\u003ehttps://arxiv.org/pdf/2105.04387.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ry2fz8tn.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e対話システムに関するサーベイ論文\u003c/p\u003e\n\u003cp\u003e対話システムはNLPタスクの一種\u003c/p\u003e\n\u003cp\u003e研究の価値が高いNLPタスクを多く含むため，対話システムは複雑と言える．\u003c/p\u003e\n\u003cp\u003eここ最近で良い成果をあげているもののほとんどがDL\u003c/p\u003e\n\u003cp\u003eメインは，モデルタイプとシステムタイプについて述べられる．\u003c/p\u003e\n\u003cp\u003eシステムタイプ\u003c/p\u003e\n\u003cp\u003eタスク指向型\u003c/p\u003e\n\u003cp\u003eオープンドメイン型\u003c/p\u003e\n\u003ch3\u003eKeywords\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/8575dpgt.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eサーベイの主張の流れ\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/hpk33ao6.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003ch3\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003e対話システムはNLPにおいてホットな話題であり，産業においても需要が非常に高い．\u003c/p\u003e\n\u003cp\u003eタスク指向型とオープンドメイン型の対話システムが存在する．\u003c/p\u003e\n\u003cp\u003e昔ながらのタスク指向型は，Natural Language Understanding, Dialogue State Tracking, Policy Learning, Natural Language Generationの4つからなっていた\u003c/p\u003e\n\u003cp\u003e⇒\u003c/p\u003e\n\u003cp\u003e最近のSoTAモデルでは，E2Eのタスク指向型の対話システムが多い．\u003c/p\u003e\n\u003cp\u003eオープンドメイン型\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egenerative systems\n\u003cul\u003e\n\u003cli\u003eseq2seqなモデル\u003c/li\u003e\n\u003cli\u003eユーザのメッセージや対話履歴を返答系列にマッピングする(Trainingデータに存在しないであろうものも含む)\u003c/li\u003e\n\u003cli\u003e柔軟でコンテクストを読んだ返答をするが，時々主張が一貫しない返答や鈍感で面白くない返答を返す．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eretrieval-based systems (検索)\n\u003cul\u003e\n\u003cli\u003e返答の集合の中から，すでに存在する適した返答を探す．\u003c/li\u003e\n\u003cli\u003e表面上では良い返答をする．ただし，返答集合は有限集合なので，対話上のコンテクストに対しては関係性があまりみられないこともある．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eensemble systems\n\u003cul\u003e\n\u003cli\u003e上記二つを含む\u003c/li\u003e\n\u003cli\u003eGeneratie systemsは検索システムをよくするために使われる．\u003c/li\u003e\n\u003cli\u003e検索システムはより適した返答を選ぶために使われる．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e古典的な対話システムとして，finite state-basedとstatistical learningとmachine learning-basedが挙げられる．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFinite State-based\n\u003cul\u003e\n\u003cli\u003e対話の流れはあらかじめ決められている\u003c/li\u003e\n\u003cli\u003e決まったシナリオの中でしか対応ができない．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStatistical Learning-based\n\u003cul\u003e\n\u003cli\u003eFinite State-basedよりは柔軟である．あらかじめ対応が決められていないから．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003emachine learning-based\n\u003cul\u003e\n\u003cli\u003eDeep learningが主流？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNLPの中には対話システムに近い領域がある．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQ \u0026#x26; A\u003c/li\u003e\n\u003cli\u003ereading comprehension\u003c/li\u003e\n\u003cli\u003edialogue disentanglement\u003c/li\u003e\n\u003cli\u003evisual dialogue\u003c/li\u003e\n\u003cli\u003evisual Q \u0026#x26; A\u003c/li\u003e\n\u003cli\u003edialogue reasoning\u003c/li\u003e\n\u003cli\u003econversational semantic parsing\u003c/li\u003e\n\u003cli\u003edialogue relation extraction\u003c/li\u003e\n\u003cli\u003edialogue sentiment analysis\u003c/li\u003e\n\u003cli\u003ehate speech detection\u003c/li\u003e\n\u003cli\u003eMISC detection (???)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eNeural Models in Dialogue Sustems\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCNN\n\u003cul\u003e\n\u003cli\u003eここ数年NLPの分野での応用も多いらしい\u003c/li\u003e\n\u003cli\u003eフレーズや文章，パラグラフには意味づけをするのに有用でCNNがヒラルキーなモデルになる\u003c/li\u003e\n\u003cli\u003eCNNは一条に乏しいため，最近のSoTAにおいては，テキストをencoderにかけたのちにCNNを用いてヒエラルキーな特徴抽出を行っている．\u003c/li\u003e\n\u003cli\u003e欠点として入力系列の長さは固定長のため以下の使用例\n\u003cul\u003e\n\u003cli\u003eencoderの出力をCNNでベクトル化\u003c/li\u003e\n\u003cli\u003econtextと返答の候補を行列にして，CNNで近さを図ることによって，妥当な候補を選び出す\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e基本的にCNNとencoderはセットか？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/37cmjuij.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRNN and Vanilla seq2seq\n\u003cul\u003e\n\u003cli\u003e系列として扱えるのが利点と考えるべき\u003c/li\u003e\n\u003cli\u003eHMMや古典的な系列モデルだと，推論時のアルゴリズムの複雑さや考えるべき状態空間の増大に合わせて行列サイズが大きくなりすぎて，大きな状態空間を必要とするデータには対応しがたい．\u003c/li\u003e\n\u003cli\u003eマルコフモデルは限られた条件下においては強力なモデルになりうる．\u003c/li\u003e\n\u003cli\u003eRNNは最近では提案されないが，NLPタスクにおいては未だ現役として活躍することもある\u003c/li\u003e\n\u003cli\u003eJordan-Type \u0026#x26; Elman-Type RNN\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ts6afg6g.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-graphql\"\u003e\t- Jordan-\u003cspan class=\"hljs-keyword\"\u003eType\u003c/span\u003e RNN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/mz0mr5oj.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 最新の隠れ層の状態は，Input\u003cspan class=\"hljs-emphasis\"\u003e_tとOutput_\u003c/span\u003et-1による\n\u003cspan class=\"hljs-code\"\u003e\t\t\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Elman-Type RNN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/10bbby3m.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 最新の隠れ層の状態は，Input\u003cspan class=\"hljs-emphasis\"\u003e_tとHidden_\u003c/span\u003et-1による\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e いずれにしてもシンプルなRNNは勾配消失か勾配爆発が大抵おこる\n\u003cspan class=\"hljs-code\"\u003e\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e LSTM\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/k21pyf3t.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Gates\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 入力ゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 忘却ゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 出力ゲート\n\n\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e GRU; Gated Recurrent Unit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/fs4fug4f.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Gates\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 更新ゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e リセットゲート\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e パラメータが少ないため，\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 早い\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 汎化性がみられる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e ただし，\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 大きなデータセットには対応しきれないこともある\n\u003cspan class=\"hljs-code\"\u003e\t\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Bi-directional RNN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/vrdvputk.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 双方向を考慮したRNN\n\u003cspan class=\"hljs-code\"\u003e\t\t\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e seq2seq; Encoder-Decoder model\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 初めは機械翻訳のために提案された手法\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Encoderにより入力系列をベクトル化，その隠れ状態をDecodeして生成することを目指す\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/56q5hqna.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Encode時\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e t時刻のinputとt-1時刻のhiddenによって，t時刻のhiddenが決まる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Decode時\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e t時刻のhiddenとt-1時刻のoutputによって，t時刻のoutputをデコードする\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 入力系列と出力系列の長さが固定長である必要はない．\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e その代わり，適応させる系列長と出力される系列長は同じになることは保証されない\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eHierarchical Recurrent Encoder-Decoder; HRED\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/4s0olxfm.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-diff\"\u003e\u003cspan class=\"hljs-deletion\"\u003e- コンテクストを理解するためのseq2seqモデル\u003c/span\u003e\n\u003cspan class=\"hljs-deletion\"\u003e- クエリの履歴を理解する？\u003c/span\u003e\n\u003cspan class=\"hljs-deletion\"\u003e- トークンレベルとターンレベルで学習する\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eMemory Networks\u003c/li\u003e\n\u003cli\u003eAttention and Transformer\n\u003cul\u003e\n\u003cli\u003eAttention\u003c/li\u003e\n\u003cli\u003eTransformer\n\u003cul\u003e\n\u003cli\u003eMuti-head Attention\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePointer Net and CopyNet\n\u003cul\u003e\n\u003cli\u003ePointer Net\u003c/li\u003e\n\u003cli\u003eCopyNet\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDeep RL and GANs\n\u003cul\u003e\n\u003cli\u003eDeep Q-Networks\u003c/li\u003e\n\u003cli\u003eREINFORCE\u003c/li\u003e\n\u003cli\u003eGANs\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKnowledge Graph Augmented Neural Networks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2章は途中から読むのやめた．使用されるネットワークよりも課題感の方が知りたい．\u003c/p\u003e\n\u003ch3\u003eタスク指向型対話システム\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/y29vzu3h.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eドメインの決まったタスクにおいて特定の問題を解決する．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNatural Language Understanding\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/1mx5wsm3.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 3つのタスクを持つ\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e ドメイン分類\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 意図の理解\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e スロット埋め\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e IOB; Inside Outside Beginning\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NER; Named Entity Recognition\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e intent detectionにおいては，Task-Oriented Dialogue BERTがSoTA?\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Domain classification \u0026#x26; intent detectionは同カテゴリタスク\n\n\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e slot filling task = semantic tagging\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NLUタスクを解く際に，音声データをそのままInputとして与える研究事例も出ているらしい\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e エラーが少なくロバストなモデルになったらしい？\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Natural Language UnderstandingとNatural Language Generationは逆のプロセスをふむ\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 同時にタスクを学習結果が得られるというアプローチも\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eDialogue State Tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/aao7x0r4.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e ユーザの目的と対話履歴を追跡する\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NLUとDSTのタスクは近い関係にある．\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e NLUは単語にtagを割り振っていくイメージ\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e DSTはtagのplaceholderを会話の内容から埋めていくイメージ\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Dialogue Stateには3つの要素からなる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Goal constraint corresponding with informable slots\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e 特別なvalueの制約で，ユーザによって言及されるか特別な値をとる\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e DontcareやNoneが特別な値にあたる\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Requested slots\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Search method of current turn\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 古典的な手法でいくと，\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e ルールベースはエラーが多く，ドメイン適応が大変\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 統計的手法はノイジーな状態や曖昧性に弱い\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e ニューラルネットな手法\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e slot-valueのペアを事前定義して学習\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e valueが大きくなると複雑性が増す\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e slot-valueのペアを読むだけでよく，2値分類タスクとして解ける\n\u003cspan class=\"hljs-bullet\"\u003e\t\t-\u003c/span\u003e モデルの複雑性は避けられるが，反応速度が遅くなる可能性がある．\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e slot-valueのペアを定義せずに，対話の中から直接選ぶ\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePolicy Learning\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDSTモジュールの出力結果からどう行動をとるか\u003c/li\u003e\n\u003cli\u003e教師あり学習or 強化学習\u003c/li\u003e\n\u003cli\u003e教師ありだとアノテショーンデータセットを作るのがとても大変\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNatural Language Generation; NLG\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eタスク指向型対話システムにおける最終層のモジュール\u003c/li\u003e\n\u003cli\u003e最終的な自然言語表現を生成するシステム\u003c/li\u003e\n\u003cli\u003e4つのコンポーネントからなる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/9ybi8yfr.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Content Determination\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Sentence Planning\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Surface Realization\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e Lexicalization, Referring expression, aggregation\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e RNNに基づいた統計言語モデルにおいて，意味的制約や文法構造による返答生成を行うなど\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e コンテクストを理解した返答を生成することは重要である\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e タスク指向型においては，返答の多様性というよりも信頼性のほうが重要視されがち\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 意味解析をビームサーチを使うことで，意味の正しさを改善する手法も提案された\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eE2E Methods\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eend-to-endのパイプラインを組むことで高いパフォーマンスを発揮することがあるが，\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e多くのモジュールを組み込むため，バックプロパゲーションで誤差が伝播しないこともある．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eすべてのモジュールが，返答の精度を向上するために，対等に重要であるとは限らない．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e違うドメインに差し替えるとき，オントロジーを事前学習させる必要があるため，困難が生じることもある\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eやり方は大きく分けて2つ\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eすべてのモジュールを展開して誤差逆伝播させる？\u003c/li\u003e\n\u003cli\u003e知識ベースの検索システムと返答生成の双方を用いてパイプラインを組む\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eタスク指向型においては，外部の知識源が必要なことが多い\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eオープンドメイン型対話システム\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e雑談対話システム，或いはタスク思考型ではない対話システムのこと\u003c/li\u003e\n\u003cli\u003eSoTAを示しているオープンドメインは大抵ニューラルネットで解決している\u003c/li\u003e\n\u003cli\u003e完全なるデータドリブンなものが多い\u003c/li\u003e\n\u003cli\u003eオープンドメイン型対話システムは，大まか3つに分けられる\n\u003cul\u003e\n\u003cli\u003e生成システム\u003c/li\u003e\n\u003cli\u003e検索ベースシステム\u003c/li\u003e\n\u003cli\u003eアンサンブルシステム\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e３つの話が以下\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e生成システム\n\u003cul\u003e\n\u003cli\u003e訓練コーパスに出てこないような返答に対して，ユーザのメッセージや対話履歴をマッピングするために，seq2seqなモデルを適用する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e検索システム\n\u003cul\u003e\n\u003cli\u003e決まった返答集合の中からすでに存在する返答を探そうとする\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eアンサンブルシステム\n\u003cul\u003e\n\u003cli\u003e生成手法と検索手法を合わせる．\u003c/li\u003e\n\u003cli\u003e生成された返答と検索された返答とを比べる．\u003c/li\u003e\n\u003cli\u003e生成も，検索された返答を洗練するために用いられる．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特徴として，\u003c/p\u003e\n\u003cp\u003e生成モデルは\u003c/p\u003e\n\u003cp\u003e柔軟でコンテクストを読んだ返答をできるが，ときには理解に欠けていたり，怠けた返答を見せることがある\u003c/p\u003e\n\u003cp\u003e検索ベースのモデルは\u003c/p\u003e\n\u003cp\u003e人の返答の集合から実際の返答を選ぶため，返答の集合は有限集合であり，コンテクストと相関がないことがある．\u003c/p\u003e\n\u003cp\u003eただし，表面上のレベルでは，首尾一貫した返答することも多い\u003c/p\u003e\n\u003cp\u003e以下は，オープンドメイン型対話システムにおける，難しさとホットなトピックをまとめる\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eContext Awareness\n\u003cul\u003e\n\u003cli\u003e対話コンテクストは会話のトピックを決定したり，ユーザの目標を決定したりと重要\u003c/li\u003e\n\u003cli\u003eコンテクストを解釈した対話エージェントは，現メッセージだけではなく，対話履歴からももとにして返答する\u003c/li\u003e\n\u003cli\u003e生成モデルも検索ベースも，どちらも対話コンテクストモデリングに依存する\u003c/li\u003e\n\u003cli\u003eいくつかのモデルではAttentionが使用されているらしい\u003c/li\u003e\n\u003cli\u003e構造化されたAttentionを用いることでコンテクストを読み取れる？\u003c/li\u003e\n\u003cli\u003e対話をリライトする問題があるらしい\n\u003cul\u003e\n\u003cli\u003e複数のメッセージから単一のメッセージに変換する目標\u003c/li\u003e\n\u003cli\u003eここではコンテクストを理解させることが重要\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eResponse Coherence\n\u003cul\u003e\n\u003cli\u003e首尾一貫した返答は，良い生成器としての一つのクオリティ\u003c/li\u003e\n\u003cli\u003e対話の中で，論理的で首尾一貫しているか？という指標\u003c/li\u003e\n\u003cli\u003e生成モデルにおいてホットなトピックとなっている（検索ベースはすでに人の返答をりようするのでもともと一貫性はあるという主張）\u003c/li\u003e\n\u003cli\u003e一貫性のない文の順序を見つけるタスクを解くことで，返答の一貫性を改善した事例もあり\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eResponse Diversity\n\u003cul\u003e\n\u003cli\u003e人が多用するような表現は訓練コーパスにも多く含まれ，それらばかりを返答してしまうことが問題となりうる\u003c/li\u003e\n\u003cli\u003eかつては条件付き確率において，尤度関数を解くことで尤もらしい返答をもとめていた．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/00fwhths.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e この手法では，返答の精度の安全性と適切さはトレードオフになっていた？\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e ビームサーチを提案されたことも\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eSpeaker Consistency and Personality-based Response\n\u003cul\u003e\n\u003cli\u003eシステムは，訓練コーパスからサンプリングされた分布に対して学習\n\u003cul\u003e\n\u003cli\u003e対話者の趣味といった一貫性のないものに対する返答は．．．\u003c/li\u003e\n\u003cli\u003e対話者の役割を理解し，その個人に合わせた返答が必要になる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e1ステージではなく，3ステージで個人的な嗜好に対応した事例がある\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEmpathetic Response\n\u003cul\u003e\n\u003cli\u003e同情する対話システムは，ユーザの感情の変化や感情に伴った適切な返答をする\u003c/li\u003e\n\u003cli\u003e雑談チャットについて，このトピックは重要\u003c/li\u003e\n\u003cli\u003eCortanaやAlexaなどの製品にもモジュールが含まれている\u003c/li\u003e\n\u003cli\u003eCoBERTのモデルなど\u003c/li\u003e\n\u003cli\u003e感情対話システムのデータセットはとぼしいが，新たなデータセットとベンチマークが提供されたらしい\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eConversation Topics\n\u003cul\u003e\n\u003cli\u003eトピックや目的は，会話に参加した人と会話を続けるための重要な役割を果たす\u003c/li\u003e\n\u003cli\u003eトピックを理解させることが重要\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKnowledge-Grounded System\n\u003cul\u003e\n\u003cli\u003e人は，会話のコンテクストと経験や記憶といったものとを関連付けて，返答をする（機会には難しい）\u003c/li\u003e\n\u003cli\u003e生成モデルは，単なる機械翻訳よりも複雑\n\u003cul\u003e\n\u003cli\u003eより自由度が高く，制約が曖昧なため\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e故に，雑談チャットは，外部から得られる常識と結びつけて，seq2seqなモデルによって生成する\u003c/li\u003e\n\u003cli\u003eメモリーネットワークなどで，知識をグラウンディングする手法\u003c/li\u003e\n\u003cli\u003e知識グラフは外部の情報をソースにするものもある．\u003c/li\u003e\n\u003cli\u003egraph attentionを用いて，常識をグラフベースで学習する手法も\u003c/li\u003e\n\u003cli\u003e主な考え方は，外部の知識グラフを使って，会話の論理の流れをモデリングする指標の一部として扱う\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eInteractive Training\n\u003cul\u003e\n\u003cli\u003e別名；human-in-loop training\u003c/li\u003e\n\u003cli\u003eアノテーションされたデータセットは限られている\n\u003cul\u003e\n\u003cli\u003eすべての状況をカバーすることは不可能\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eユーザとの対話の中で，システムを改善する\u003c/li\u003e\n\u003cli\u003e強化学習における逐次学習を提案\u003c/li\u003e\n\u003cli\u003e対話相手と話して，その相手からフィードバックを得る\u003c/li\u003e\n\u003cli\u003e教師あり学習をした後，Interactive Trainingによってファインチューニングする\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eVisual Dialogue\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/v3baquyk.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e Visual Q \u0026#x26; Aなど\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e 画像あり対話システムのほか，映像あり対話システムも面白いトピックだが難題でもある\n\u003cspan class=\"hljs-bullet\"\u003e\t-\u003c/span\u003e 特徴量抽出の複雑さも増す\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e visual dialogueのアノテーションは重労働であり，データセットに乏しいので，現在はデータの不十分さに悩まされている\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e評価のアプローチ\u003c/h3\u003e\n\u003cp\u003e評価の仕方も重要なパートとなっている\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eタスク指向型対話システムにおける評価\n\u003cul\u003e\n\u003cli\u003eBLEUスコアを用いて，システムの返答と人の返答を比べるなど\u003c/li\u003e\n\u003cli\u003eTask Completion Rate\n\u003cul\u003e\n\u003cli\u003eすべてのタスクの試行に対して，いくつのイベントが成功したかの割合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTask Completion Cost\n\u003cul\u003e\n\u003cli\u003eタスクをこなすのに使われたリソース\u003c/li\u003e\n\u003cli\u003e解決までの時間が重視されるタスクにおいて用いられる\u003c/li\u003e\n\u003cli\u003eなるべく短いターン数で完遂するのが良しとされる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHuman-based Evaluation\n\u003cul\u003e\n\u003cli\u003eユーザの対話とユーザの満足度のスコアを提供\u003c/li\u003e\n\u003cli\u003e方法はふたつ\n\u003cul\u003e\n\u003cli\u003eクラウドソーシングで労働を雇う\u003c/li\u003e\n\u003cli\u003e実際にローンチしてからユーザのフィードバックで評価する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eオープンドメイン型対話システムにおける評価\n\u003cul\u003e\n\u003cli\u003e明確なメトリックはない\u003c/li\u003e\n\u003cli\u003e長らくHuman Evaluationを使ってきた\u003c/li\u003e\n\u003cli\u003eWord-overlap Metrics\n\u003cul\u003e\n\u003cli\u003e生成された系列と実際の系列の近さを計算する\u003c/li\u003e\n\u003cli\u003e機械翻訳や要約タスクにおいて用いられる\u003c/li\u003e\n\u003cli\u003en-gramのものとして\n\u003cul\u003e\n\u003cli\u003eBLEU\u003c/li\u003e\n\u003cli\u003eROUGE\u003c/li\u003e\n\u003cli\u003eMETEOR(BLUEの改良版)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNeural Metrics\n\u003cul\u003e\n\u003cli\u003eニューラルモデルによって計算させる\u003c/li\u003e\n\u003cli\u003eRNNやCNN,GANの識別器を使うなどして，ターンレベルの特徴量抽出を行うなど\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e今もホットなトピックになっている\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eデータセット\u003c/h3\u003e\n\u003cp\u003eタスク指向型対話システム\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/4xcx432d.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/bvaa6edt.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/5nqvspiq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eオープンドメイン型対話システム\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ja1g5vzq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ki5ab45o.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ikzadi1i.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/nuxspw6o.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/1u3frgui.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003e結論とトレンド\u003c/h3\u003e\n\u003cp\u003eココ最近のトレンド\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMutlimodal dialogue systems\n\u003cul\u003e\n\u003cli\u003e異なるモダリティを組み合わせる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMultitask dialogue systems\n\u003cul\u003e\n\u003cli\u003eタスク指向型と知識グラウンディングさせたオープンドメイン型を組み合わせて，一つのフレームワークまたはシングルモデルとして完結させる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCorpus exploration on Internet\n\u003cul\u003e\n\u003cli\u003ereal-timeなコーパスをインターネットから取り出せるようになれば，期待がもてる\u003c/li\u003e\n\u003cli\u003e研究に値するのでは？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUser modeling\n\u003cul\u003e\n\u003cli\u003e生成と評価の双方でホットなトピック\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDialogue generation with a long-term goal\n\u003cul\u003e\n\u003cli\u003e日常的な雑談は特に目的はない\u003c/li\u003e\n\u003cli\u003eしかし，会話が意図的にある特定の目的に向かうときは，ほんの少しでも状況があるはず\u003c/li\u003e\n\u003cli\u003e現在のオープンドメイン型は，長期的な目的を除いてモデリングされがち\n\u003cul\u003e\n\u003cli\u003e十分な知性を備えていない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey","Date":"2023-05-22","Category":"論文","Tags":["survey","dialogue system"],"Authos":"ゆうぼう","Slug":"Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey","Thumbnail":"/images/thumbnails/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey.png","Description":"Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Surveyのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: A survey on empathetic dialogue systems\u003c/p\u003e\n\u003cp\u003e研究会: Information Fusion 64\u003c/p\u003e\n\u003cp\u003e年度: 2020\u003c/p\u003e\n\u003cp\u003eキーワード: survey, dialogue system, empathetic dialogue system\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://sentic.net/empathetic-dialogue-systems.pdf\"\u003ehttps://sentic.net/empathetic-dialogue-systems.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"https://doi.org/10.1016/j.inffus.2020.06.011\"\u003ehttps://doi.org/10.1016/j.inffus.2020.06.011\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKeywords\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eArtificial Intelligence,\u003c/p\u003e\n\u003cp\u003eAffective computing,\u003c/p\u003e\n\u003cp\u003eDialogue system\u003c/p\u003e\n\u003cp\u003e共感的対話システム構築の最終目的\u003c/p\u003e\n\u003cp\u003e→ユーザの疑問や悩みに応えること\u003c/p\u003e\n\u003cp\u003eどのような機能が対話システムの共感的な振る舞いを可能にしたのかという，機能の観点から対話システムのユニークな側面に注目する．\u003c/p\u003e\n\u003cp\u003ePersonalization：　システムの一貫性と整合性を高める働き．\u003c/p\u003e\n\u003cp\u003e→ユーザ固有の情報\u003c/p\u003e\n\u003cp\u003eemotion，personalization，knowledge の3要素が重要\u003c/p\u003e\n\u003cp\u003eEmpathetic Dialogue System\u003c/p\u003e\n\u003cp\u003e感情の状態の感受や表現，個人的な嗜好，知識を強化する\u003c/p\u003e\n\u003cp\u003e3つの重要な特徴について\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eemotional-awareness\u003c/li\u003e\n\u003cli\u003ePersonality-awareness\u003c/li\u003e\n\u003cli\u003eKnowledge-accessibility\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/jj8cey5y.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e3つのサブトピックを扱う\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ePerceiving and expressing emotion\u003c/li\u003e\n\u003cli\u003eCaring each individual\u003c/li\u003e\n\u003cli\u003eCasting into knowledge\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e過去10年ぶんほどをカバー\u003c/p\u003e\n\u003ch2\u003ePropaedeutic background\u003c/h2\u003e\n\u003cp\u003ebackboneとして使われているアーキテクチャの紹介\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNeural language model\n1. RNN (LSTM, GRUなど)\n\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/ab1tzbka.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-css\"\u003e  \t\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e. Sequence-\u003cspan class=\"hljs-selector-tag\"\u003eto\u003c/span\u003e-sequence model\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/vxnsfxcb.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/j79vo4ch.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRNN\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003elong short-term memory, LSTM\n入力・忘却・出力の3つのゲート\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003egated recurrent unit, GRU\ngated関数は通常シグモイド関数．勾配のスケールを制限し，複数回の時間ステップの後に爆発するようにする．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSeq2Seq\nmodualizedなシステムは，通常以下の4パートからなる：\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNatural Language Understanding, NLU\n入力から構造情報を抽出する\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ea Dialogue State Tracker, DST\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ea Dialogue Polich, DP\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ea response generator\n先行モジュールすべての出力に基づいた応答を生成する．\u003c/p\u003e\n\u003cp\u003e別名，エンコーダ・デコーダモデル．\u003c/p\u003e\n\u003cp\u003e条件付き対話生成のモデル化にはおそらく最も広く使われているニューラルアーキテクチャ．\u003c/p\u003e\n\u003cp\u003eエンコーダ，デコーダはそれぞれ，通常RNNをベースとしている．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eAttention mechanism\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/ccvc5cuj.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eエンコーダが符号化できる最大ワード数の制限．入力単語数が大きくなると適切に符号化できない．\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e→デコーダが文脈の最も関連性の高い位置にアクセスすることが，この問題に効果的\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eRNNやseq2seqなどの，入力単語数の限界に対して対処できると，RNNやseq2seqでの問題の解消に一役買ったと紹介．\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eMemory networks; MMN\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/4dbq5m5d.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-wasm\"\u003eRNNの隠れ空間ではメモリは時間と共に更新されるものであるが，このメモリは小さかったり離れすぎていたりする．対話のような，文脈を理解するために長期的な記憶が必要な分野では上手くいかないことも．\n\n内部に必要な情報を保持できないため，外部メモリの機能を実装したのがこのMMN\n\n外部の\u003cspan class=\"hljs-keyword\"\u003ememory\u003c/span\u003e slotsに対して，attentionをかけてslotを更新するなどをする\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eVAE, Variational AutoEncoder\n条件付き確率分布に基づき，データ分布に近いように生成をする.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e通常のオートエンコーダ：\u003c/p\u003e\n\u003cp\u003e　入力→エンコーダが潜在変数を生成→デコーダ→出力（入力に似たものを生成）\u003c/p\u003e\n\u003cp\u003eVAE：\u003c/p\u003e\n\u003cp\u003e　潜在変数がN(0,1)の確率分布に従うと仮定する．\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-scss\"\u003e条件付き\u003cspan class=\"hljs-built_in\"\u003eVAE\u003c/span\u003e(CVAE)：条件付き確率分布 \u003cspan class=\"hljs-selector-tag\"\u003eP\u003c/span\u003e(出力応答 | 入力)をモデル化する．\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGAN, Generative Adversarial Network\nGenerator G, Discriminator D　からなる．\u003c/p\u003e\n\u003cp\u003e画像生成から伝達学習までさまざまなタスクで大きな成果をあげている．\u003c/p\u003e\n\u003cp\u003eGenerator vs Discriminator：\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e生成器（G）は分類誤差を最大にして識別器（D）を欺くように訓練され，Dは分類誤差を最小にするように訓練される．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRL, Reinforcement Learning（強化学習）\n以下のような一般的に用いられる目的関数は，対話システムの現実的な目標と明確な関連性を持っていない．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e尤度\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eELBO\nELBOを解く．\u003c/p\u003e\n\u003cp\u003e対話における各タイミングでの学習のフィードバックは，単語ごとではなく，まとまった文章が生成されたのちに与えられる．このため遅延報酬関数を使用できる．\u003c/p\u003e\n\u003cp\u003e論文の中でも，RLの重要性が何度も紹介されていた\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAffective dialogue system\u003c/h2\u003e\n\u003cp\u003e感情は，反応と社会的行動で文化的な作用であり，これは人と環境の関係によって連続的に発展していくもの\u003c/p\u003e\n\u003cp\u003e感情のカテゴライズは，心理学者と哲学者の間でせわしく，長らく議論されてきた\u003c/p\u003e\n\u003cp\u003e感情は社会的な機能も持つ．そして情動は意思決定に関連する尺度である可能性が示唆されている．人間の会話行動のエミュレートだけでなく，システムとユーザとの感情的なつながりを強化することができる．\u003c/p\u003e\n\u003cp\u003e本書における affective dialogue system\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eperceiving emotion\u003c/li\u003e\n\u003cli\u003eunderstanding emotion\u003c/li\u003e\n\u003cli\u003eexpressing and regulating emotion\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e↓\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eemotion-awareness\n文脈の中の感情の表現に関係する，対話中のユーザの感情状態を検出できなければならない．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eemotion-expressiveness\n生成された応答に感情情報を取り入れることに関係する．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e感情に関する理論は，感情の挿入がユーザとの感情の結びつきを強くするという利点をサポートしてくれる\u003c/p\u003e\n\u003ch3\u003eEmotion analysis\u003c/h3\u003e\n\u003cp\u003e一般的には，多くのcomputational modelは3つのカテゴリーに分けられる\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003edimensional approach（次元的）\u003c/li\u003e\n\u003cli\u003ediscrete approach（離散的）\u003c/li\u003e\n\u003cli\u003eappraisal approach（評価的）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eDimensional approach\u003c/p\u003e\n\u003cp\u003e感情をベクトル（[覚醒]と[静寂]を表すもの）として表現する．\u003c/p\u003e\n\u003cp\u003e次元空間を持つことで，異なる感情の間でも類似度を計算できるのが利点\u003c/p\u003e\n\u003cp\u003eDiscrete approace\u003c/p\u003e\n\u003cp\u003e感情をいくつかのカテゴリーに分類する．\u003c/p\u003e\n\u003cp\u003eカテゴリ数は設定によって異なってくる．（2，32，64，など．emojiで表したり）\u003c/p\u003e\n\u003cp\u003eAppraisal approach\u003c/p\u003e\n\u003cp\u003e感情と引き起こされたリアクションの関係について学習する\u003c/p\u003e\n\u003cp\u003e分布型\u003c/p\u003e\n\u003cp\u003e感情の別の表現方法．embeddingを使う．\u003c/p\u003e\n\u003cp\u003eメリット：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e感情の種類が連続的になり，補完が可能になる\u003c/li\u003e\n\u003cli\u003eDLの入力として直接利用できる\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこの利点は，感情のタイプを連続値として扱えること\u003c/p\u003e\n\u003cp\u003eDeep learningのinputとして扱えること\u003c/p\u003e\n\u003cp\u003eもう一つのタイプは感情を，実際の効果として重視する\u003c/p\u003e\n\u003cp\u003esatisfactionやpolitenessとして分類する\u003c/p\u003e\n\u003cp\u003e文や文脈から感情・感情を予測するタスク\u003c/p\u003e\n\u003cp\u003e会話が与えられた時，感情ラベルを事前に予測する＝条件付き確率分布の学習と同義\u003c/p\u003e\n\u003cp\u003easpect-base分析\u003c/p\u003e\n\u003cp\u003e目的：アスペクトと文の両方から感情ラベルを予測することを学習する．\u003c/p\u003e\n\u003cp\u003e文に複数のアスペクトが付与されているとして，その種類に応じて予測を行う？\u003c/p\u003e\n\u003cp\u003e対話システムによる感情ラベル予測では，現在の時間ステップまでの対話履歴しか見えないことがることに注意\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChallenges\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e感情は曖昧な方法で表現される\n\u003cul\u003e\n\u003cli\u003eコンテクストから理由づけを必要とする\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e対話で現れた感情は，過去から継続していて，文脈的な感情の状態にとても依存している\n\u003cul\u003e\n\u003cli\u003e発話者自身もだが，そのパーティにも影響を受ける\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eさまざまなモダリティを合わせて感情を表している\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eEmotion-aware encoders\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/zrlg6ntb.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eEmotion-aware encoderは，感情に関連した情報をエンコードする\u003c/p\u003e\n\u003cp\u003e得られる文脈ベクトルにも感情に関連した情報が含まれる．\u003c/p\u003e\n\u003cp\u003eモジュール化されたフレームワークは，POMDPとしてモデル化したものとして扱える\u003c/p\u003e\n\u003cp\u003e追加の特徴量として感情のラベルを与えることで機能する\u003c/p\u003e\n\u003cp\u003eただし，テスト時は感情ラベルがない\u003c/p\u003e\n\u003cp\u003e→emotion detector（＝追加の感情検出器）を加えて，暗示的に感情のラベルを推測することで機能させる\u003c/p\u003e\n\u003ch3\u003eEmotion-expressive decoder\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/w0fopt7s.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e感情的なレスポンスを促進する目的で使われる\u003c/p\u003e\n\u003cp\u003econtrollable variableとして直接感情を与える\u003c/p\u003e\n\u003cp\u003eモデルはCVAEやGAN，RLなどを使うことが多いらしい\u003c/p\u003e\n\u003cp\u003econtrollable variableの想定\u003c/p\u003e\n\u003cp\u003e一つまたは複数の潜在的な変数が応答の生成に対して強制力を持っていること\u003c/p\u003e\n\u003cp\u003eそしてそのような変数が存在していること\u003c/p\u003e\n\u003cp\u003e潜在的な対話状態をモデリングするのに自然なアーキテクチャはCVAE\u003c/p\u003e\n\u003cp\u003e学習の際，微分できないことが多いので，誤差をフィードバックするにはRLを使うのが重要\u003c/p\u003e\n\u003ch3\u003eDiscussion\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eChallenges\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e感情のラベルの不足\n\u003cul\u003e\n\u003cli\u003e対話のアノテーション処理に時間がかかるため，人手が不足．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eweak supervisions を用いることで緩和可能：事前に学習された感情ラベルを使うとか，複数のデータソースを組み合わせて規模を拡大するとか\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e感情の評価\n\u003cul\u003e\n\u003cli\u003e単語レベルでは感情の手がかりが微妙なこともある．\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eユーザの本質的な感情と実際の認識にギャップがある可能性　→ユーザの誘導とギャップをノイズとして扱うこと．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e他の目的における感情のcompliance\u003c/li\u003e\n\u003cli\u003eターンレベルでのcontrollable variableと生成される単語の依存性\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePersonalized dialogue system; PDS\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-survey-on-empathetic-dialogue-systems/kip5j4e8.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003epersonalized informationは，話者の意図や継続的な状態を知覚したり，結果的に適したレスポンスを生成するのを成功させる鍵になる\u003c/p\u003e\n\u003ch3\u003eUser modeling\u003c/h3\u003e\n\u003cp\u003eパーソナリティを表現する方法は，多くのパーソナリティ理論で重要になっている関心ごとである\u003c/p\u003e\n\u003cp\u003eこのサーベイでは，user modelingの方法として，二つに分類される\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eidentity-based\u003c/li\u003e\n\u003cli\u003eknowledge-based\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eIdentity-based user modeling\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eもっともシンプルな方法で，identityを静的な属性として与える\u003c/p\u003e\n\u003cp\u003eidentity-basedの特徴量は，信頼性があり，情報抽出するための追加のステップを必要とせずに直接的に扱うことができる\u003c/p\u003e\n\u003cp\u003eidentity-basedの特徴量のソースは主に，registrationで収集したメタデータである\u003c/p\u003e\n\u003cp\u003epersona factsとidentity featuresはパーソナライズされた応答の生成に効果があるため，unstructured dataとstructured dataの双方を使う\u003c/p\u003e\n\u003cp\u003eidentity-basedをニューラルネットに入れるときは，embedding layerを用いて，連続値のdense vectorにする\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKnowledge-based user modeling\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003estructured dataとpredefined rulesを用いる\u003c/p\u003e\n\u003cp\u003eidentity-basedと比べると，これはユーザのメタデータの制限がない\u003c/p\u003e\n\u003cp\u003estructuredとunstructured information data sourceを両方使用できる\u003c/p\u003e\n\u003cp\u003ePersonalized reasoningというタスク\u003c/p\u003e\n\u003cp\u003eknowledge baseから事実を取り出すことを目的にしている\u003c/p\u003e\n\u003ch3\u003ePersonalized response generation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003egenerative methods\u003c/li\u003e\n\u003cli\u003eretrieval-based methods (ranking methods)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePDSのメインの目標は，適した応答だけでなく，ユーザのじゅう雨よう（重要？）な知識に基づいた応答を生成すること\u003c/p\u003e\n\u003cp\u003eここでは二つのサブトピックの紹介があった\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epersonality-aware model\u003c/li\u003e\n\u003cli\u003epersonality-infused model\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePersonality-aware model\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eユーザのパーソナリティ，もしくは会話のパーティに適応した応答を生成する\u003c/p\u003e\n\u003cp\u003eその応答には，ユーザの嗜好が含まれるということである\u003c/p\u003e\n\u003cp\u003eユーザのプロファイルや会話履歴は，話者の記憶の中で異なる役割をはたす→メモリ(MMNの話など)\u003c/p\u003e\n\u003cp\u003eシステムの中で多くのユーザの参加する大規模な環境においては，それぞれのユーザのタイプに十分なデータを持つのが難しくなりうる\u003c/p\u003e\n\u003cp\u003e→ユーザの知識を収集したり，転移することは可能\u003c/p\u003e\n\u003cp\u003e以降はtransfer learningの話がなされていた．RLも同様に使えるとのこと\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePersonality-infused agent dialogue systems\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e会話をスムーズで，柔軟で自然に行うために，システムにpersonalityを与える\u003c/p\u003e\n\u003cp\u003e3つのコンポーネントからなる\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProfiler Detector\u003c/li\u003e\n\u003cli\u003eBidirectional Decoder\u003c/li\u003e\n\u003cli\u003ePosition Detector\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eProfile Detector\nどのprofileのvalueが生成された応答の中で言及されるべきかを選ぶ\u003c/p\u003e\n\u003cp\u003eMLPを使う\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBidirectional Decoder\nprofile valueが言及される中で応答を生成する目的のデコーダ\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePosition Detector\ndecoding positionのスタート位置を予測する\u003c/p\u003e\n\u003cp\u003eここで使うコンポーネントはbidirectional decoderで監視するように設計される\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePosition Detectorは，training dataをかえる性能があるらしい\u003c/p\u003e\n\u003cp\u003epre-specificなエージェントのprofileに沿った応答生成ができるモデルを提供してくれる\u003c/p\u003e\n\u003cp\u003epersona representationの後，以下の提案があった\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePersona Aware Attention\nそれぞれのdecoding positionに対するAttention weightsを生成する\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePersona Aware Bias\nデコーダのoutput layerの分散表現を差し込むことで生成分布を評価する\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAttentive Memory Network; AMNの提案\u003c/p\u003e\n\u003cp\u003eおそらく個人だけでなく，所属するグループの影響を加味するためのモデルだと思う\u003c/p\u003e\n\u003cp\u003eコンポーネント二つ\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAttentive Encoders\u003c/li\u003e\n\u003cli\u003eKnowledge-Store Memory Module\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eKnowledge-based dialogue system\u003c/h3\u003e\n\u003cp\u003ecurrent dialogue, personal background, external knowledge sourceからきた知識から探したり，コミュニケーションをとるプロセスを経る\u003c/p\u003e\n\u003cp\u003e→ knowledge graphなど\u003c/p\u003e\n\u003cp\u003eexternal knowledgeは重要な役割をはたすことができる\u003c/p\u003e\n\u003cp\u003eこのシステムはたいてい二つの追加のコンポーネントを持つ\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eknowledge encoder\u003c/li\u003e\n\u003cli\u003eknowledge-aware decoder\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれらによりcontextとexternal knowledgeの両方で応答に条件付けできる\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKnowledge encoding\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eStructured knowledge\nlanguage understandingで重要な役割\u003c/p\u003e\n\u003cp\u003e扱うモデルの変遷\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBoW→Sequence→Data cell→Recursive graph\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e人の前処理やルールベースなどでフィルターをかけるステップが必要\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003eUnstructured knowledge\n制約が少ないため，扱えるデータの量が多い\u003c/p\u003e\n\u003cp\u003e分散表現に変換できるので，end-to-endのモデル\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eKnowledge-aware decoding\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ehistorical knowledgeとしてknowledge source→contextなどとinputを一緒に入力にかける\u003c/p\u003e\n\u003cp\u003einputはembeddingされたもの\u003c/p\u003e\n\u003cp\u003e応答生成における2種類の知識ソース：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e対話の履歴から得られるもの\u003c/li\u003e\n\u003cli\u003e事前予知的知識\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eKnowledge attention\nknowledge encoderの出力であるknowledge embeddingを使って，応答の生成に条件付けをする\u003c/p\u003e\n\u003cp\u003e文脈とknowledge embeddingのセットが与えられたら，関連する知識を読み取るか再認識する必要があり，それが応答性性の条件付けに使われる．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopy\nattention mechanismをベースにしている\u003c/p\u003e\n\u003cp\u003eattentionを入力から単語を選び，コピーするためのポインターに使う\u003c/p\u003e\n\u003cp\u003eseq2seqをコピー機構で拡張することで，検索ベースの手法より優れた性能を発揮する事が示された．\u003c/p\u003e\n\u003cp\u003e単語は決められた単語の分布から取るか，knowledge baseからの単語をコピーすることで生成される\u003c/p\u003e\n\u003cp\u003e高階層のmemory architectureの学習の提案をしている人もいる\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eFuture direction\u003c/h2\u003e\n\u003cp\u003eempathetic dialogue systemに残る研究課題：\u003c/p\u003e\n\u003cp\u003epersonalization, knowledge, and emotion の要素の組み合わせによる包括的な共感システムの構築なんかはあまり行われていなかった．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eMulti-goal Management\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eコミュニケーションには多くのobjective（目的）が乗っている\n→複数の目的によって過負荷になる事がある．感情や性格，知識を取り入れることでさらに顕著に．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003edialogue agentは全ての異なる側面に取り入れるべき\nすべての異なる側面を考慮する必要があるから↓\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003euser's inherent states, communicating information, minimizing the communicative effortsなど\nこれらを同時に達成するための最適解をいかに効率的に探索するかが問題となる\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExplicit Affective Policy\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e感情は明示的な行動と考えられる\u003c/li\u003e\n\u003cli\u003eagentが他の人の感情をミラーリングしたり，共感を示したりする\n並列共感（相手の感情のミラーリング）と反応的共感に対して異なる戦略を取る事ができる．\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLong-term Empathy Modeling\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e対話中での共感はlong-termである\u003c/li\u003e\n\u003cli\u003eemotion, personality, knowledgeを静的，動的の双方で評価して，long-termで対応する\n静的で動的：安定的なベースを持ちながら，変化もしやすい．長期的なデータ収集において変化に適応する会話モデルの構築が課題．\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDialogue Generation with Target-dependent Emotion\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e感情は，話者と会話の参加者にアタッチされた特定の次元であるとして，target-dependent emotionをuser modelingに合わせる\n感情とターゲットの依存関係が省略されてきた．ターゲットに依存する感情をユーザモデリングと組み合わせることが望まれる（感情と人格の2次元の相関　←共同でモデル化する必要性）．\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDialogue Generation with Emotion Knowledge\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003esentimental, emotionalな知識を使って，感情の状態を認識する\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIncorporate Cues from Multimodal Input\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e複数のモダリティを使って共感を示す\u003c/li\u003e\n\u003cli\u003ei.e. audio signals, body gestures\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePersonalized Diversifying Dialogue Generation\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eユーザに合わせて，生成する応答や検索する応答をカスタマイズする\u003c/li\u003e\n\u003cli\u003eグループごとに多様性はあるが，同グループ内での多様性はかけるのが問題\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDeeper Conversation and User modeling\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e与えられたユーザからのクエリに対して，統計的にもっともらしい回答を取り出すのがシンプルなメインの目標なのが現在\u003c/li\u003e\n\u003cli\u003e将来的には，会話ごとにモデルを作ったり，どのようにユーザの感情が変わるかを理解したり，重要な会話や嗜好を覚えたり，ユーザのニーズや意図を汲み取る以上のことをするようになる(?)\u003c/li\u003e\n\u003cli\u003e↑そのためのサブタスク\n\u003col\u003e\n\u003cli\u003esarcasm detection（皮肉検出）\u003c/li\u003e\n\u003cli\u003etime expression（時間表現）\u003c/li\u003e\n\u003cli\u003enamed entity recognition（固有表現）\u003c/li\u003e\n\u003cli\u003eanaphora resolution\u003c/li\u003e\n\u003cli\u003emicrotext normalization\u003c/li\u003e\n\u003cli\u003eetc\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@article{MA202050,\ntitle = {A survey on empathetic dialogue systems},\njournal = {Information Fusion},\nvolume = {64},\npages = {50-70},\nyear = {2020},\nissn = {1566-2535},\ndoi = {\u003ca href=\"https://doi.org/10.1016/j.inffus.2020.06.011%7D\"\u003ehttps://doi.org/10.1016/j.inffus.2020.06.011}\u003c/a\u003e,\nurl = {\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S1566253520303092%7D\"\u003ehttps://www.sciencedirect.com/science/article/pii/S1566253520303092}\u003c/a\u003e,\nauthor = {Yukun Ma and Khanh Linh Nguyen and Frank Z. Xing and Erik Cambria},\nkeywords = {Artificial intelligence, Affective computing, Dialogue systems},\nabstract = {Dialogue systems have achieved growing success in many areas thanks to the rapid advances of machine learning techniques. In the quest for generating more human-like conversations, one of the major challenges is to learn to generate responses in a more empathetic manner. In this review article, we focus on the literature of empathetic dialogue systems, whose goal is to enhance the perception and expression of emotional states, personal preference, and knowledge. Accordingly, we identify three key features that underpin such systems: emotion-awareness, personality-awareness, and knowledge-accessibility. The main goal of this review is to serve as a comprehensive guide to research and development on empathetic dialogue systems and to suggest future directions in this domain.}\n}\u003c/p\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】A survey on empathetic dialogue systems","Date":"2023-05-22","Category":"論文","Tags":["survey","dialogue system","empathetic dialogue system"],"Authos":"ゆうぼう","Slug":"A-survey-on-empathetic-dialogue-systems","Thumbnail":"/images/thumbnails/A-survey-on-empathetic-dialogue-systems.png","Description":"A survey on empathetic dialogue systemsのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: 遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析\u003c/p\u003e\n\u003cp\u003e研究会: NLP\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: dialogue system\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-2.pdf\"\u003ehttps://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-2.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e対話を通じてユーザに行動変容を促す説得対話システムの研究を行うため，マルチモーダル情報を含む説得対話コーパスの収録\u003c/p\u003e\n\u003cp\u003e音声と顔画像から得られる特徴量を含む説得対話コーパス\u003c/p\u003e\n\u003cp\u003e収集したコーパスと合わせて前後にアンケート\u003c/p\u003e\n\u003cp\u003e被験者に対して追跡調査を行い，実際に説得により行動が変容したかも調査\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e5〜8分の対話を収録\u003c/p\u003e\n\u003cp\u003e3つのドメインについて収集：「運動の促し」「インターネット依存の改善」「慈善事業団体への募金の促し」\u003c/p\u003e\n\u003cp\u003eWoz対話であることを伏せて，ERICAで開発された新しいシステムと対話をしてもらうという目的と被験者に伝えた\u003c/p\u003e\n\u003cp\u003e効果的な説得対話の戦略（論文参照）に基づいてERICAが発話\u003c/p\u003e\n\u003cp\u003eまた以下の流れで対話\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eお互いに挨拶\u003c/li\u003e\n\u003cli\u003e会話テーマの提示（ドメイン）\u003c/li\u003e\n\u003cli\u003e会話テーマに対する被験者の意識を尋ねる\u003c/li\u003e\n\u003cli\u003e相手の反応に応じて説得\u003c/li\u003e\n\u003cli\u003e5分経過後，流れに応じて対話終了\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003e説得対話における非言語情報の活用を指向し，遠隔操作Androidを用いたWoz対話によるcleanマルチモーダル説得対話コーパスの収集\u003c/p\u003e\n\u003cp\u003e先行研究では主に対話中の説得にフォーカスしているが，本論文では追跡調査による実際の行動変容を調査\u003c/p\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e収録環境\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e説得マイクでステレオ録音／Webカメラで正面から顔を撮影→OpenFaceでAction Unitを抽出／音声は書き起こし→訓練されたアノテーたによって拡張ISO-24617-2対話行為タグに基づく対話行為タグ付与を\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e事前アンケート\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e性格，意思決定の傾向，説得対象ドメインへの意識，現在の状況など\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e事後アンケート\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e年齢，性別，学歴，パートナーの有無，政治的見解など\u003c/p\u003e\n\u003cp\u003eERICAへの印象，対話を通じての意識の変化なども用意\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e追跡調査\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e説得に対して合意した被験者に対して，1週間後にその合意を履行したか追跡調査\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E9%81%A0%E9%9A%94%E6%93%8D%E4%BD%9C%E3%82%A2%E3%83%B3%E3%83%89%E3%83%AD%E3%82%A4%E3%83%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E8%AA%AC%E5%BE%97%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AE%E5%8F%8E%E9%9B%86%E3%81%A8%E5%88%86%E6%9E%90/h2xv1id7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E9%81%A0%E9%9A%94%E6%93%8D%E4%BD%9C%E3%82%A2%E3%83%B3%E3%83%89%E3%83%AD%E3%82%A4%E3%83%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E8%AA%AC%E5%BE%97%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AE%E5%8F%8E%E9%9B%86%E3%81%A8%E5%88%86%E6%9E%90/6nfsjg5n.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e実際の行動の変容や意識の変容に関わらず，多くの人が対話中に説得へ合意\u003c/p\u003e\n\u003cp\u003e→実際に説得が効果的であったかを対話上の振る舞いから判定することは困難\u003c/p\u003e\n\u003cp\u003e意識変容から行動変容に映るにはハードルがある\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eロジスティック回帰による説得に有効な要素の分析\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E9%81%A0%E9%9A%94%E6%93%8D%E4%BD%9C%E3%82%A2%E3%83%B3%E3%83%89%E3%83%AD%E3%82%A4%E3%83%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E8%AA%AC%E5%BE%97%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AE%E5%8F%8E%E9%9B%86%E3%81%A8%E5%88%86%E6%9E%90/hp4pnema.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E9%81%A0%E9%9A%94%E6%93%8D%E4%BD%9C%E3%82%A2%E3%83%B3%E3%83%89%E3%83%AD%E3%82%A4%E3%83%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E8%AA%AC%E5%BE%97%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AE%E5%8F%8E%E9%9B%86%E3%81%A8%E5%88%86%E6%9E%90/kzssp4zq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E9%81%A0%E9%9A%94%E6%93%8D%E4%BD%9C%E3%82%A2%E3%83%B3%E3%83%89%E3%83%AD%E3%82%A4%E3%83%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E8%AA%AC%E5%BE%97%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AE%E5%8F%8E%E9%9B%86%E3%81%A8%E5%88%86%E6%9E%90/bopuu7bx.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003cp\u003e倫理審査が必要らしい\u003c/p\u003e\n\u003cp\u003e顔とか映すデータを撮る場合は倫理審査のため，被験者に合意を得る必要がありそう\u003c/p\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析","Date":"2023-05-21","Category":"論文","Tags":["dialogue system"],"Authos":"ゆうぼう","Slug":"遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析","Thumbnail":"/images/thumbnails/遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析.png","Description":"遠隔操作アンドロイドを用いたマルチモーダル説得対話コーパスの収集と分析のまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: 知識源との一対多関係を有する対話コーパスによる発話生成\u003c/p\u003e\n\u003cp\u003e研究会: NLP\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: dialogue system, knowledge-base\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-3.pdf\"\u003ehttps://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-3.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eある文脈において利用可能な知識は一意とは限らず，実際に利用された知識意外にも利用可能な知識は存在する可能性がある\u003c/p\u003e\n\u003cp\u003e→　旅行代理店における対話を題材として，基準対話データセットを作成（知識が一意）\u003c/p\u003e\n\u003cp\u003e→　基準対話データセットを元にマルチラベル対話データセットを作成（知識が複数対応）\u003c/p\u003e\n\u003cp\u003eマルチラベル対話データセットを発話生成モデルの生成に用いると，多様で適切な応答が可能になることが示唆\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E7%9F%A5%E8%AD%98%E6%BA%90%E3%81%A8%E3%81%AE%E4%B8%80%E5%AF%BE%E5%A4%9A%E9%96%A2%E4%BF%82%E3%82%92%E6%9C%89%E3%81%99%E3%82%8B%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AB%E3%82%88%E3%82%8B%E7%99%BA%E8%A9%B1%E7%94%9F%E6%88%90/4tpxgaoh.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E7%9F%A5%E8%AD%98%E6%BA%90%E3%81%A8%E3%81%AE%E4%B8%80%E5%AF%BE%E5%A4%9A%E9%96%A2%E4%BF%82%E3%82%92%E6%9C%89%E3%81%99%E3%82%8B%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AB%E3%82%88%E3%82%8B%E7%99%BA%E8%A9%B1%E7%94%9F%E6%88%90/6h5aq4kv.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e基準対話データセットの構築\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eクラウドソーシングを利用して，東京と大阪の観光地441件を対象に，観光地に関する対話おw収集\u003c/p\u003e\n\u003cp\u003e知識情報として，基礎情報はじゃらんからスクレイピング，レビュー情報にはGoogle Map APIを用いて取得\u003c/p\u003e\n\u003cp\u003e店発話は，知識情報をなるべく用いて発話し，使用できる知識情報源は最大で2つとした\u003c/p\u003e\n\u003cp\u003e相槌など知識情報を使用しない発話にはnoneのラベルを付与\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eマルチラベルデータセットの構築\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e利用していない知識は，「利用できない知識」ではなく「利用していない知識」\u003c/p\u003e\n\u003cp\u003e→　基準対話データセットから400件を抽出し，対象の発話において利用可能な知識をアノテーション\u003c/p\u003e\n\u003cp\u003e基準対話データセットの分布とマルチラベル対話データセットの分布を比較すると，多くの知識源が利用可能であるとわかる\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003e一つの発話に対して複数の知識を対応させたマルチラベル対話データセットを作成\u003c/p\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eLaboro製BERTを用いて利用可能な知識情報を選択\u003c/p\u003e\n\u003cp\u003e→　TransformerベースのNTT製大規模対話モデルhobbyistを用いて，知識情報を用いた応答生成\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E7%9F%A5%E8%AD%98%E6%BA%90%E3%81%A8%E3%81%AE%E4%B8%80%E5%AF%BE%E5%A4%9A%E9%96%A2%E4%BF%82%E3%82%92%E6%9C%89%E3%81%99%E3%82%8B%E5%AF%BE%E8%A9%B1%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9%E3%81%AB%E3%82%88%E3%82%8B%E7%99%BA%E8%A9%B1%E7%94%9F%E6%88%90/yy6uxb1q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eBERTを用いた知識選択\u003c/p\u003e\n\u003cp\u003eシングルtestが0.46，マルチtestが0.90\u003c/p\u003e\n\u003cp\u003e適切な知識が選択できていれば正解なので，マルチが高くなるのはそれはそう\u003c/p\u003e\n\u003cp\u003eマルチラベル対話データセットを使用した応答生成は，全て文脈として正しく，知識を反映した多様かつ適切な生成ができていた\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】知識源との一対多関係を有する対話コーパスによる発話生成","Date":"2023-05-21","Category":"論文","Tags":["dialogue system","knowledge-base"],"Authos":"ゆうぼう","Slug":"知識源との一対多関係を有する対話コーパスによる発話生成","Thumbnail":"/images/thumbnails/知識源との一対多関係を有する対話コーパスによる発話生成.png","Description":"知識源との一対多関係を有する対話コーパスによる発話生成のまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: 分類モデルBERTによる不整合生成文の検出について\u003c/p\u003e\n\u003cp\u003e研究会: NLP\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: dialogue system, NLI\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-4.pdf\"\u003ehttps://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-4.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: 日本語SNLI\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eニューラル文章生成において，文章としては自然だが，内容が事実とは異なる**事実不整合（factual inconsistency）**が問題\u003c/p\u003e\n\u003cp\u003e→　BERTを用いて分類タスクをすることで生成文の事実不整合の検出を試みる\u003c/p\u003e\n\u003cp\u003e疑似データセットを作成し学習することで，不整合検出におけるドメイン適応の重要性を明らかにした\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%88%86%E9%A1%9E%E3%83%A2%E3%83%87%E3%83%ABBERT%E3%81%AB%E3%82%88%E3%82%8B%E4%B8%8D%E6%95%B4%E5%90%88%E7%94%9F%E6%88%90%E6%96%87%E3%81%AE%E6%A4%9C%E5%87%BA%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/l81q3d7a.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/%E5%88%86%E9%A1%9E%E3%83%A2%E3%83%87%E3%83%ABBERT%E3%81%AB%E3%82%88%E3%82%8B%E4%B8%8D%E6%95%B4%E5%90%88%E7%94%9F%E6%88%90%E6%96%87%E3%81%AE%E6%A4%9C%E5%87%BA%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/gnt0fefy.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e特に数値データに対してロバストなモデルになるよう学習するため，知識に数値が含まれる「料金情報」「アクセス情報」「営業時間情報」の3カテゴリに絞って学習に用いる\u003c/p\u003e\n\u003cp\u003e疑似例の作成は数値や日付，駅名等を書き換えることで対応\u003c/p\u003e\n\u003cp\u003e料金，アクセス，営業時間情報で書き以下絵対象がお’異なるため，それぞれ小cleanなる改変方法でデータを書き換え\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003e旅行ドメインに対して疑似データセットを作成し，それを用いて学習することで，SNLIデータセットを用いた学習に比べて，事実不整合の生成文の検出精度を向上\u003c/p\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eデータセット\u003c/p\u003e\n\u003cp\u003e事実整合性判定学習データセット\u003c/p\u003e\n\u003cp\u003e料金，アクセス，営業時間情報について作成した疑似生後売れ，不整合例を集めたデータセット\u003c/p\u003e\n\u003cp\u003eニューラル生成文データセット\u003c/p\u003e\n\u003cp\u003eNTT製TransformerのHobbyistを用いて生成した文章を含むデータセット\u003c/p\u003e\n\u003cp\u003eLaboro社製BERTをファインチューニング\u003c/p\u003e\n\u003cp\u003eベースラインデータセットとして，日本語SNLIデータセット\u003c/p\u003e\n\u003cp\u003erecallが最良のエポックの重みを最良モデルとして評価\u003c/p\u003e\n\u003cp\u003erecallが低いモデルは大量の不整合を見逃していることになるため，目的を果たしていないと考えたから\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e提案手法（疑似例を用いたデータセット）は事実不整合検出に有効である\u003c/p\u003e\n\u003cp\u003e正解できなかった不整合例の内訳\u003c/p\u003e\n\u003cp\u003e料金7件／アクセス1件／営業時間16件\u003c/p\u003e\n\u003cp\u003e→　テンプレートの拡充が必要か？\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】分類モデルBERTによる不整合生成文の検出について","Date":"2023-05-21","Category":"論文","Tags":["dialogue system","NLI"],"Authos":"ゆうぼう","Slug":"分類モデルBERTによる不整合生成文の検出について","Thumbnail":"/images/thumbnails/分類モデルBERTによる不整合生成文の検出について.png","Description":"分類モデルBERTによる不整合生成文の検出についてのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Transformerによるhallucination errorの事後修正\u003c/p\u003e\n\u003cp\u003e研究会: NLP\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: dialogue system\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-5.pdf\"\u003ehttps://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-5.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e文生成時に与えた外部知識と異なる内容の発話文を生成してしまうhallucination errorが課題\u003c/p\u003e\n\u003cp\u003e→　hallucination errorを含むデータを疑似的に作成し，BARTやTransformerを用いて事後修正を試みる\u003c/p\u003e\n\u003cp\u003e1.6BのTransformerでは52件中29件の事後修正をした\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Transformer%E3%81%AB%E3%82%88%E3%82%8Bhallucination-error%E3%81%AE%E4%BA%8B%E5%BE%8C%E4%BF%AE%E6%AD%A3/seiwj31o.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e一つの発話文と知識源のペアをテンプレートとして複数のデータを疑似的に作成することで，各知識とカテゴリごとに40000件の発話ぶんと知識源のペアを作成\u003c/p\u003e\n\u003cp\u003e→　「営業時間」「アクセス」「料金」に関するエンティティを書き換えることで疑似的なhallcination errorを含んだ文を作成\u003c/p\u003e\n\u003cp\u003eニューラル生成モデルは事実と無関係な文章を生成する場合がある\u003c/p\u003e\n\u003cp\u003e→　エンティティの書き換えだけではなく，無関係な発話を含んだデータも作成\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003ehallucination errorを含むデータを疑似的に作成することで，ニューラルモデルによる事後修正の試み\u003c/p\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eNTT製japanese-dialog-transormers（1.6B）\u003c/p\u003e\n\u003cp\u003e黒橋研製日本語BART（0.12B）\u003c/p\u003e\n\u003cp\u003ehallucination error修正学習データセットには，無関係な発話を含む（add_unrelated）とそれを含まない（baseline）データセットを二種類用意\u003c/p\u003e\n\u003cp\u003e※知識源とHEを[SEP]でつなげるが，普通はBARTは対応していないのでfairseq上ではFusion-in-DecoderをBARTに実装する必要があるらしい\u003c/p\u003e\n\u003cp\u003e評価指標\u003c/p\u003e\n\u003cp\u003eFaithfulness／BLEU-4 score\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Transformer%E3%81%AB%E3%82%88%E3%82%8Bhallucination-error%E3%81%AE%E4%BA%8B%E5%BE%8C%E4%BF%AE%E6%AD%A3/kl305xy2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Transformer%E3%81%AB%E3%82%88%E3%82%8Bhallucination-error%E3%81%AE%E4%BA%8B%E5%BE%8C%E4%BF%AE%E6%AD%A3/76t0zx7b.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eBLEU-4は「数値が異なる」みたいな単純なhallucination errorは正しく評価できていないのでは？\u003c/p\u003e\n\u003cp\u003eBARTとTransformerの大きな精度差はおそらくパラメータ数と事前学習時のデータセットの差なのでは？\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Transformer%E3%81%AB%E3%82%88%E3%82%8Bhallucination-error%E3%81%AE%E4%BA%8B%E5%BE%8C%E4%BF%AE%E6%AD%A3/p9noavar.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e知識源に出現するエンティティの順序とモデルの主直に出現するエンティティの順序が同じ\u003c/p\u003e\n\u003cp\u003e→　エンティティが出現する順序に注目して書き換えを行っている可能性\u003c/p\u003e\n\u003cp\u003e正しくエンティティの関係を理解できていない？\u003c/p\u003e\n\u003cp\u003eadd_relatedでは発話ぶんにある一文を削除する傾向が見られた\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003cp\u003e今後の展望\u003c/p\u003e\n\u003cp\u003eHE修正学習データセットの基となるデータの収集\u003c/p\u003e\n\u003cp\u003e書き換えルールなど作成方法の拡張の必要性\u003c/p\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】Transformerによるhallucination errorの事後修正","Date":"2023-05-21","Category":"論文","Tags":["dialogue system"],"Authos":"ゆうぼう","Slug":"Transformerによるhallucination-errorの事後修正","Thumbnail":"/images/thumbnails/Transformerによるhallucination-errorの事後修正.png","Description":"Transformerによるhallucination errorの事後修正のまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Prompt-Tuning による個性を持った対話システムの構築\u003c/p\u003e\n\u003cp\u003e研究会: NLP\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: dialogue system, persona, Prompt-Tuning\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-1.pdf\"\u003ehttps://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-1.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: PERSONA-CHAT, DailyDialog\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e与えられたキャラクター設定（ペルソナ）を考慮した応答生成をする雑談対話システムの構築\u003c/p\u003e\n\u003cp\u003e一貫した発話をしない対話システムは魅力的ではない\u003c/p\u003e\n\u003cp\u003e→　一貫性を持たせるためペルソナに着目\u003c/p\u003e\n\u003cp\u003ePrompt-Tuningを行うことで，Fine-Tuningに比べて学習時間と計算資源を削減しつつ，より自然で個性を持ったシステムの構築\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prompt-Tuning-%E3%81%AB%E3%82%88%E3%82%8B%E5%80%8B%E6%80%A7%E3%82%92%E6%8C%81%E3%81%A3%E3%81%9F%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89/gur4p6lh.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eペルソナ情報を埋め込むトークン（Persona Info Token）用のEmbedding層を追加したTransformerモデルを提案\u003c/p\u003e\n\u003cp\u003eこの新たに追加したEmbedding層のパラメータを更新する\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003e事前学習済みモデルのパラメータを更新しないPrompt-Tuningによって学習\u003c/p\u003e\n\u003cp\u003e→　学習に要する時間と計算資源の削減が可能\u003c/p\u003e\n\u003cp\u003e数百個の対話ペアからなる小規模なデータセットであっても，個性を持ったシステムの構築が可能\u003c/p\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eデータセット：Persona-Chat／DailyDialog\u003c/p\u003e\n\u003cp\u003e1往復の2初話ずつに分割→これを対話ペア\u003c/p\u003e\n\u003cp\u003e使用するペルソナ：Persona-chatにおける対話ペア数の多い上位3種類のペルソナのみ\u003c/p\u003e\n\u003cp\u003eペルソナとは無関係な対話ペアとしてDailyDialogを使用\u003c/p\u003e\n\u003cp\u003e→　TopicがRelationshipの対話ペアを使用\u003c/p\u003e\n\u003cp\u003e中でも発話と応答の両方の長さが50文字以下の対話ペアを一定の比率で学習用データセットに混ぜる\u003c/p\u003e\n\u003cp\u003e→　なぜ？：短い発話やペルソナと無関係な一般的な発話おデータセットに取り込む\u003c/p\u003e\n\u003cp\u003eペルソナ文を与える際，長さ\u0026#x3C;200の時は，200になるまでペルソナぶんを繰り返し並べる\u003c/p\u003e\n\u003cp\u003e生成の戦略にはGreedy searchを採用\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prompt-Tuning-%E3%81%AB%E3%82%88%E3%82%8B%E5%80%8B%E6%80%A7%E3%82%92%E6%8C%81%E3%81%A3%E3%81%9F%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89/4nnv2ixn.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prompt-Tuning-%E3%81%AB%E3%82%88%E3%82%8B%E5%80%8B%E6%80%A7%E3%82%92%E6%8C%81%E3%81%A3%E3%81%9F%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89/eoo61yjd.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prompt-Tuning-%E3%81%AB%E3%82%88%E3%82%8B%E5%80%8B%E6%80%A7%E3%82%92%E6%8C%81%E3%81%A3%E3%81%9F%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89/tebl4dqq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prompt-Tuning-%E3%81%AB%E3%82%88%E3%82%8B%E5%80%8B%E6%80%A7%E3%82%92%E6%8C%81%E3%81%A3%E3%81%9F%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89/wqq0hd5g.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e自動評価時：distinct-{1, 2}\u003c/p\u003e\n\u003cp\u003eGPT-J-6BをPrompt-Tuningしたモデルが最も多様性のある生成\u003c/p\u003e\n\u003cp\u003eFine-Tuningの時は入力にペルソナを孵化しない方が良い性能\u003c/p\u003e\n\u003cp\u003e人手評価時\u003c/p\u003e\n\u003cp\u003e全ての項目においてGPT-J-6BをPrompt-Tuningしたモデルの評価が高い\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003cp\u003eLINEとの共同研究\u003c/p\u003e\n\u003cp\u003eAI-Bridging cloudを用いてA100（40GB）を使用した実験\u003c/p\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】Prompt-Tuning による個性を持った対話システムの構築","Date":"2023-05-21","Category":"論文","Tags":["dialogue system","persona","Prompt-Tuning"],"Authos":"ゆうぼう","Slug":"Prompt-Tuning-による個性を持った対話システムの構築","Thumbnail":"/images/thumbnails/Prompt-Tuning-による個性を持った対話システムの構築.png","Description":"Prompt-Tuning による個性を持った対話システムの構築のまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Prediction of Shared Laughter for Human-Robot Dialogue\u003c/p\u003e\n\u003cp\u003e研究会: ICMI\u003c/p\u003e\n\u003cp\u003e年度: 2020\u003c/p\u003e\n\u003cp\u003eキーワード: laughter, shared laughter\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI20.pdf\"\u003ehttp://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI20.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"https://doi.org/10.1145/3395035.3425265\"\u003ehttps://doi.org/10.1145/3395035.3425265\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e会話ロボットがshared laughterを自動生成することを目的にする\u003c/p\u003e\n\u003ch3\u003eShared Laughter Model\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/ktkpx7fr.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eユーザの最初の笑いを検知して，システムが笑うモデル\u003c/p\u003e\n\u003cp\u003e3種類のモジュールが存在する\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eユーザの最初の笑いを検出\u003c/li\u003e\n\u003cli\u003eshared laughterを生成するかどうかを決定\u003c/li\u003e\n\u003cli\u003eどのタイプの笑いをするべきか決定\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eData Collection and Analysis\u003c/h3\u003e\n\u003cp\u003e収集方法：ERICA\u003c/p\u003e\n\u003cp\u003eteleoperateしたのは女性\u003c/p\u003e\n\u003cp\u003e対象：61人の男性\u003c/p\u003e\n\u003cp\u003eシナリオ：speed dating\u003c/p\u003e\n\u003cp\u003e好きな趣味，好きなこと，嫌いなことに関してカジュアルなチャット\u003c/p\u003e\n\u003cp\u003eアノテーション：\u003c/p\u003e\n\u003cp\u003e2種類のタイプに笑いを分けた\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eisolated laugh\n1. 笑い単体で起こる笑い\u003c/li\u003e\n\u003cli\u003espeech laugh\n1. 話しながら起こる笑い\n\u003cimg src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/ztqx5kie.png\" alt=\"\"\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/pncunm6h.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e集まったデータの中で，1206件のinitial laughが確認\u003c/p\u003e\n\u003cp\u003e698件がself laughで508件がshared laugh\u003c/p\u003e\n\u003ch3\u003eModel Creation\u003c/h3\u003e\n\u003cp\u003e特徴量2種類：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAudio-based features\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e40のacoustic メルフィルタバンクの平均と標準偏差\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProsodic features\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e全IPUにまたがるピッチとパワーの値を使った合計で14つの以下の指標\u003c/p\u003e\n\u003cp\u003e平均／中央値／標準偏差／最大値／最小値／範囲\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eモデル\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003eLR／SVM\u003c/p\u003e\n\u003cp\u003eデータサンプルが小さかったからか，deep learningの手法は弱かった\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003eユーザに合わせて毎回笑いを生成するのではなく，適切なタイミングでshared laughterを自動生成することをロボットに持たせたいという目的をもった研究\u003c/p\u003e\n\u003ch2\u003e評価方法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/1mjd5yfi.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eオフラインとオンラインの二つのタイプで評価\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/my7ehxxr.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/5m9f1ed7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e特徴量としてlaughter typeを加えることでrecallが改善する傾向\u003c/p\u003e\n\u003cp\u003e負に歪んだピッチの分布の時は笑いがシェアされがちで笑いが長く続きがち\u003c/p\u003e\n\u003cp\u003elaughter detectionとlaugh type classificationのエラーによってパフォーマンスが落ちることは明らかだが，それでもベースラインをoutperform\u003c/p\u003e\n\u003cp\u003e最もよかったonline modelはacousticとprosodicの特徴量を両方使ったもの\u003c/p\u003e\n\u003ch2\u003e何がすごかった？\u003c/h2\u003e\n\u003cp\u003eprosodicの分析からわかったことが，initial laughのacousiticsはresponse laughを呼び起こすいくつかの特徴があること\u003c/p\u003e\n\u003cp\u003eまだ改善の余地はあるものの，acousticとprosodicの特徴量の両方を使うことはパフォーマンスの改善に役立った\u003c/p\u003e\n\u003cp\u003eshared laughterのタイミングについてはかけた研究である\u003c/p\u003e\n\u003cp\u003e今後の課題である\u003c/p\u003e\n\u003ch2\u003e次に読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003e@inproceedings{10.1145/3395035.3425265,\nauthor = {Lala, Divesh and Inoue, Koji and Kawahara, Tatsuya},\ntitle = {Prediction of Shared Laughter for Human-Robot Dialogue},\nyear = {2020},\nisbn = {9781450380027},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {\u003ca href=\"https://doi.org/10.1145/3395035.3425265%7D\"\u003ehttps://doi.org/10.1145/3395035.3425265}\u003c/a\u003e,\ndoi = {10.1145/3395035.3425265},\nabstract = {Shared laughter is a phenomenon in face-to-face human dialogue which increases engagement and rapport, and so should be considered for conversation robots and agents. Our aim is to create a model of shared laughter generation for conversational robots. As part of this system, we train models which predict if shared laughter will occur, given that the user has laughed. Models trained using combinations of acoustic, prosodic features and laughter type were compared with online versions considered to better quantify their performance in a real system. We find that these models perform better than the random chance, with the multimodal combination of acoustic and prosodic features performing the best.},\nbooktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},\npages = {62–66},\nnumpages = {5},\nkeywords = {shared laughter, machine learning, human-robot dialogue, conversation},\nlocation = {Virtual Event, Netherlands},\nseries = {ICMI '20 Companion}\n}\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】Prediction of Shared Laughter for Human-Robot Dialogue","Date":"2023-05-21","Category":"論文","Tags":["laughter","shared laughter"],"Authos":"ゆうぼう","Slug":"Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue","Thumbnail":"/images/thumbnails/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue.png","Description":"Prediction of Shared Laughter for Human-Robot Dialogueのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms\u003c/p\u003e\n\u003cp\u003e研究会: WACV\u003c/p\u003e\n\u003cp\u003e年度: 2021\u003c/p\u003e\n\u003cp\u003eキーワード: humor detection, multi-modal\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://openaccess.thecvf.com/content/WACV2021/papers/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.pdf\"\u003ehttps://openaccess.thecvf.com/content/WACV2021/papers/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"http://dx.doi.org/10.1109/WACV48630.2021.00062\"\u003ehttp://dx.doi.org/10.1109/WACV48630.2021.00062\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: MHD (Multimodal Humor Dataset)\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eマルチモダールなユーモアデータセット(\u003cstrong\u003eMHD; Multimodal Humor Dataset\u003c/strong\u003e)（The Big Bang Theoryを使用）を構築\u003c/p\u003e\n\u003cp\u003e海外のSitcoms (Situation comedies) では笑い声がドラマ内に含まれている\u003c/p\u003e\n\u003cp\u003e→ sitcomsは定期的に作成されていて，この笑い声を自動で追加するタスクがクリティカルなタスク\u003c/p\u003e\n\u003cp\u003e→ \u003cstrong\u003e笑い声の自動挿入のタスクを自動化することが狙い\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e構築されたデータセットを用いて，マルチモーダルを利用したAttentionベースのモデルを構\u003c/p\u003e\n\u003cp\u003e→SoTA \u0026#x26; データセット分析\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/w0i199qh.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/ak7naea6.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/sl4l2p5h.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eデータセットのこと\u003c/h3\u003e\n\u003cp\u003e対話のチャンクに対してlaughter tracksを使用してラベルを付与\u003c/p\u003e\n\u003cp\u003e笑い声をアノテーションすることがは間接的に人手でのアノテーションと同じになるという過程\u003c/p\u003e\n\u003cp\u003e→ 笑い声の起こる直前の発話の集合をユーモアとしてラベル付け\u003c/p\u003e\n\u003cp\u003eAttributes\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eScene\u003c/li\u003e\n\u003cli\u003eSpeaker\u003c/li\u003e\n\u003cli\u003eRecipients\u003c/li\u003e\n\u003cli\u003eParticipants\u003c/li\u003e\n\u003cli\u003eDialogue Turns\u003c/li\u003e\n\u003cli\u003eDialogue Start/End time\u003c/li\u003e\n\u003cli\u003eHumor Start/End time\n対話のチャンクに複数のlaughter tracksがある場合，最後のみ適用\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eデータ分析の結果はFig 3.を参照のこと\u003c/p\u003e\n\u003ch3\u003eモデルのこと\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/ve5n04t6.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e手動でアノテーションされたマルチモーダルな大規模ユーモアデータセットを構築\u003c/li\u003e\n\u003cli\u003eこれまでのSoTA手法を実験しつつ，multimodal self attention based modelを提案\u003c/li\u003e\n\u003cli\u003e提案手法の汎化性能を検証\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003e5 turns / dialogueとする\u003c/p\u003e\n\u003cp\u003ehumor : non-humor = 1 : 2としてサンプリング\u003c/p\u003e\n\u003cp\u003ehumorのラベルが85%と高く，かなり不均衡のため\u003c/p\u003e\n\u003cp\u003e実験モデル\u003c/p\u003e\n\u003cp\u003e{Attention, Fusion, Sequential} with {only Text, only Video, both of them}\u003c/p\u003e\n\u003cp\u003e評価指標：\u003c/p\u003e\n\u003cp\u003eAccuracy, ROC, F1\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/nlp8wlr7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/flj6yume.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/rizjcdv5.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e提案手法のMSAMが強い\u003c/p\u003e\n\u003cp\u003e表情や動作のようなvisual特徴量がユーモアの合図になっていることがある\u003c/p\u003e\n\u003cp\u003e→ visual特徴量を使うことが有効である\u003c/p\u003e\n\u003cp\u003e@Table 6.より，dialogueのターン数を長くするとよりcontextualにできるが，長くしすぎても精度が落ちている\u003c/p\u003e\n\u003cp\u003e→ dialogue 5, 6がピークになっている→ ゆえにturn数を5として本研究は進められている\u003c/p\u003e\n\u003ch3\u003eDiscussion\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e良いモデルはテキストと視覚的な特徴量の重みづけの仕方を正しく考慮しなければならない\u003c/li\u003e\n\u003cli\u003e失敗例への対策\n\u003cul\u003e\n\u003cli\u003eよりlong tailなユーモアにロバストにならなければいけない\n\u003cul\u003e\n\u003cli\u003e例）Sheldonは滅多にブランケットを羽織らない→羽織った時面白くなる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e知識ベースの弱さへの改善\n\u003cul\u003e\n\u003cli\u003esitcomsは皮肉での笑いが多い（知識がないと伝わらないことがある\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@INPROCEEDINGS{9423266, author={Patro, Badri N. and Lunayach, Mayank and Srivastava, Deepankar and Sarvesh, Sarvesh and Singh, Hunar and Namboodiri, Vinay P.}, booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}, title={Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms}, year={2021}, volume={}, number={}, pages={576-585}, doi={10.1109/WACV48630.2021.00062}}\u003c/p\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms","Date":"2023-05-21","Category":"論文","Tags":["humor detection","multi-modal"],"Authos":"ゆうぼう","Slug":"Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms","Thumbnail":"/images/thumbnails/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms.png","Description":"Multimodal Humor Dataset: Predicting Laughter tracks for Sitcomsのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Multi-Hop Transformer for Document-Level Machine Translation\u003c/p\u003e\n\u003cp\u003e研究会: NAACL\u003c/p\u003e\n\u003cp\u003e年度: 2021\u003c/p\u003e\n\u003cp\u003eキーワード: MT, transformer, Multi-Hop Transformer\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://aclanthology.org/2021.naacl-main.309.pdf\"\u003ehttps://aclanthology.org/2021.naacl-main.309.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"http://dx.doi.org/10.18653/v1/2021.naacl-main.309\"\u003ehttp://dx.doi.org/10.18653/v1/2021.naacl-main.309\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: TED Talk, OpenSubtitles, Europarl7\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eDocument-level neural machine translationにおいて，Multi-Hopなアーキテクチャを導入することにより，従来手法と比べて精度の高い文脈を考慮した機械翻訳を実現\u003c/p\u003e\n\u003cp\u003e翻訳者のように，頭の中に翻訳のドラフトを作り，文脈に合わせて適切に修正する流れ（human-like draft-editing）を明示的にモデリング\u003c/p\u003e\n\u003cp\u003e大きな事前学習済みモデルを使うことなく，使用に足る機械翻訳モデルを実現\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/p7s4t8vi.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eアーキテクチャ周りのこと\u003c/p\u003e\n\u003ch3\u003eSentence Encoder\u003c/h3\u003e\n\u003cp\u003esource-sideとtarget-sideでそれぞれPretrained Encoderがあり，source contextとtarget draftの分散表現をそれぞれ得る\u003c/p\u003e\n\u003ch3\u003eMulti-Hop Encoder\u003c/h3\u003e\n\u003cp\u003esource-contextにおいて文章ごとのreasoningをして，現在の文章の分散表現を得る\u003c/p\u003e\n\u003ch3\u003eMulti-Hop Decoder\u003c/h3\u003e\n\u003cp\u003etarget-side draftから情報を取得して，翻訳の確率分布を得る\u003c/p\u003e\n\u003cp\u003eそのほかアーキテクチャの工夫\u003c/p\u003e\n\u003ch3\u003eContet Gating\u003c/h3\u003e\n\u003cp\u003econtextual informationを過剰にutilizeしすぎないように，context gating machanismを採用\u003c/p\u003e\n\u003cp\u003econtextと現在の文章間の重みを動的にコントロールする\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.796ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"25.003ex\" height=\"3.196ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1060.7 11051.5 1412.5\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-1-TEX-I-1D6FC\" d=\"M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-N-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D70E\" d=\"M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-N-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D434\" d=\"M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-N-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D460\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-N-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D44F\" d=\"M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-N-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-1-TEX-I-1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D6FC\" xlink:href=\"#MJX-1-TEX-I-1D6FC\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(917.8,0)\"\u003e\u003cuse data-c=\"3D\" xlink:href=\"#MJX-1-TEX-N-3D\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(1973.6,0)\"\u003e\u003cuse data-c=\"1D70E\" xlink:href=\"#MJX-1-TEX-I-1D70E\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(2544.6,0)\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"msub\" transform=\"translate(2933.6,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D44A\" xlink:href=\"#MJX-1-TEX-I-1D44A\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D44E\" xlink:href=\"#MJX-1-TEX-I-1D44E\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"msubsup\" transform=\"translate(4334.6,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D434\" xlink:href=\"#MJX-1-TEX-I-1D434\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(783,530.4) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mo\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(389,0)\"\u003e\u003cuse data-c=\"1D45B\" xlink:href=\"#MJX-1-TEX-I-1D45B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(989,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(783,-138.9) scale(0.707)\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-1-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(6364.2,0)\"\u003e\u003cuse data-c=\"2B\" xlink:href=\"#MJX-1-TEX-N-2B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"msub\" transform=\"translate(7364.5,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D44A\" xlink:href=\"#MJX-1-TEX-I-1D44A\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"\u003e\u003cuse data-c=\"1D44F\" xlink:href=\"#MJX-1-TEX-I-1D44F\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"msubsup\" transform=\"translate(8694.8,0)\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D435\" xlink:href=\"#MJX-1-TEX-I-1D435\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(792,530.4) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mo\"\u003e\u003cuse data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(389,0)\"\u003e\u003cuse data-c=\"1D45B\" xlink:href=\"#MJX-1-TEX-I-1D45B\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(989,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"TeXAtom\" transform=\"translate(792,-293.8) scale(0.707)\" data-mjx-texclass=\"ORD\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D460\" xlink:href=\"#MJX-1-TEX-I-1D460\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(469,0)\"\u003e\u003cuse data-c=\"2212\" xlink:href=\"#MJX-1-TEX-N-2212\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mi\" transform=\"translate(1247,0)\"\u003e\u003cuse data-c=\"1D456\" xlink:href=\"#MJX-1-TEX-I-1D456\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(10662.5,0)\"\u003e\u003cuse data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e where sigma is logistic sigmoid function\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003eDocment-level NMTにおける従来手法の問題点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e文章間のreasoningの特徴づけを明示的に行うことなく，単純にcontextの分散表現を導入\u003c/li\u003e\n\u003cli\u003e推論時にはアクセスできないのに，訓練時には追加入力としてのtarget contextにground-truthなデータを入力\n↑　訓練時と推論時において状況が異なる\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eDocument-level NMTにおいてMulti-Hop reasoningをモデリングしたMulti-Hop Transformerの提案と提案モデルによるDocument-level NMTの大きな性能改善\u003c/p\u003e\n\u003cp\u003etarget contextにground-truthで訓練すると推論時にはアクセスできないため，他の翻訳モデルの翻訳結果を使用することで，訓練時と推論時の状況を同じにした\u003c/p\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eBaseline\u003c/p\u003e\n\u003cp\u003eTransformer\u003c/p\u003e\n\u003cp\u003eCA-Transformer\u003c/p\u003e\n\u003cp\u003eCA-HAN\u003c/p\u003e\n\u003cp\u003eCADec\u003c/p\u003e\n\u003cp\u003e計算量のオーバーヘッドを改善するためSentence Encoderはそれぞれのsideでパラメータを共有\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/am19iape.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003elarge-scaleな事前学習済み言語モデルを使用することなく，SoTA翻訳クオリティを達成\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/z4jm21k3.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003econtextを付与するためのAttentionの構造は，ConcatやHierarchicalよりもMulti-HopなAttentionが効果があり\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/tito19hb.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003econtextを考慮する幅のwindow sizeは大きくするほど効果が上がるわけではなく．3が最も良かった\u003c/p\u003e\n\u003cp\u003e4以上にすると悪化傾向らしく，本研究ではwindow size = 3 を採用\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/pc76zuqn.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003econtextにおいてreasoningするときの方向は，一般的な読み順の通りleft-to-rightで順方向にreasoningさせた方が結果は良かった\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/bx5z85gj.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e訓練時と推論時にtarget draftに与える文章が異なる問題への対処に関する実験結果\u003c/p\u003e\n\u003cp\u003eReferenceはground-truthをtarget draftとして与えて訓練，Draftはpre-trained MT systemが生成した翻訳結果をtarget draftとして与えて訓練したモデル\u003c/p\u003e\n\u003cp\u003eDraftの方が結果がよく，pre-trained MT systemの生成結果をtarget draftとする方法によって訓練時と推論時のギャップの橋渡しになることを示唆する結果\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/5955ca444629476ebf23e66629a2413f\"\u003eContext-Aware Self-Attention Networks\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@inproceedings{zhang-etal-2021-multi,\ntitle = \"Multi-Hop Transformer for Document-Level Machine Translation\",\nauthor = \"Zhang, Long and\nZhang, Tong and\nZhang, Haibo and\nYang, Baosong and\nYe, Wei and\nZhang, Shikun\",\nbooktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\nmonth = jun,\nyear = \"2021\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"\u003ca href=\"https://aclanthology.org/2021.naacl-main.309\"\u003ehttps://aclanthology.org/2021.naacl-main.309\u003c/a\u003e\",\ndoi = \"10.18653/v1/2021.naacl-main.309\",\npages = \"3953--3963\",\nabstract = \"Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior {--} human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.\",\n}\u003c/p\u003e\n\u003c/blockquote\u003e\u003cstyle\u003e\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] \u003e g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] \u003e rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e line[data-line], svg[data-table] \u003e g \u003e line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e rect[data-frame], svg[data-table] \u003e g \u003e rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dashed, svg[data-table] \u003e g \u003e .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dotted, svg[data-table] \u003e g \u003e .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e g \u003e svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool \u003e mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool \u003e mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n\u003c/style\u003e","Title":"【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation","Date":"2023-05-21","Category":"論文","Tags":["MT","transformer","Multi-Hop Transformer"],"Authos":"ゆうぼう","Slug":"Multi-Hop-Transformer-for-Document-Level-Machine-Translation","Thumbnail":"/images/thumbnails/Multi-Hop-Transformer-for-Document-Level-Machine-Translation.png","Description":"Multi-Hop Transformer for Document-Level Machine Translationのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Internet-Augmented Dialogue Generation\u003c/p\u003e\n\u003cp\u003e研究会: ACL\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: dialogue system, Internet-Augmented\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://aclanthology.org/2022.acl-long.579.pdf\"\u003ehttps://aclanthology.org/2022.acl-long.579.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"http://dx.doi.org/10.18653/v1/2022.acl-long.579\"\u003ehttp://dx.doi.org/10.18653/v1/2022.acl-long.579\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: Topical-Chat, Wizard of Wikipedia\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e検索クエリを生成し，Bing検索の結果をもとに応答生成を行うことで，大規模言語モデルの抱えるhallucinationの問題を軽減しつつ，up-to-the-minute relavent informationを導入した生成を可能にする．\u003c/p\u003e\n\u003cp\u003eインターネットによるaugmentationを行わないモデルやFAISSベースのモデルよりも，search-queryのモデルは優れた会話能力を達成した．\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e提案手法 (Search Engine-Augmented Generation) の流れ\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eコンテクストから検索クエリを生成\u003c/li\u003e\n\u003cli\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.009ex\" height=\"1.545ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 888 683\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-1-TEX-I-1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D441\" xlink:href=\"#MJX-1-TEX-I-1D441\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e個のドキュメントを取得\u003c/li\u003e\n\u003cli\u003eFiD (Fusion in Decoder)モデルによって，個々のドキュメントをエンコードし，対話コンテクストと結合して，応答を生成\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eインターネットへのアクセスの手法\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFAISS: distributed approximate nearest-neighbor databaseにストアすることでページをキャッシュ\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003eこれがベースライン的な手法\u003c/li\u003e\n\u003cli\u003eCommon CrawlのデータをFAISSストアして検索をかける\u003c/li\u003e\n\u003cli\u003eベースライン手法\n\u003col\u003e\n\u003cli\u003eRAG (Retrieval Augmented Generation)\u003c/li\u003e\n\u003cli\u003eFiD (Fusion in Decoder)\u003c/li\u003e\n\u003cli\u003eFiD-RAG\u003c/li\u003e\n\u003cli\u003eFAISS + Search Query-based Retrieval\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eインターネットに直接アクセスしてページを取得\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003eFAISS-basedの手法の課題を解決するため\n\u003col\u003e\n\u003cli\u003eリアルタイムなウェブ情報に更新するのが難しい\u003c/li\u003e\n\u003cli\u003eローカルのFAISSにストアできるウェブページの数には限界がある\u003c/li\u003e\n\u003cli\u003eインターネット検索エンジンがチューニングしているハイクオリティなページのランキングの利点を活かせない\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eインターネットにアクセスすることで，常に数え切れないほどの最新の情報にアクセスし，それを取り入れた応答生成を可能にする\u003c/li\u003e\n\u003cli\u003eknowledge regulationなどを行うことで，dynamic state of the worldに対応する\n\u003cul\u003e\n\u003cli\u003e大規模言語モデルは，知識をweightsの中で記憶してしまうため，hallucinationが起きやすい→これを正則化によってよりうまく情報をcopyするように学習させる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e長い目で見れば機械学習の手法は実世界とのインタラクションが求められるが，まず自然な第一ステップとしてインターネットへのアクセスをモデル化してみた．\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eWizard of the Internet (WizInt)という新たなタスクで評価\u003c/p\u003e\n\u003cp\u003eT5, BART-large, BlenderBotをファインチューニング\u003c/p\u003e\n\u003cp\u003eRetrieval-augmented methodは5つのドキュメント (\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: -0.186ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"6.157ex\" height=\"1.731ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 2721.6 765\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-2-TEX-I-1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-2-TEX-N-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"\u003e\u003c/path\u003e\u003cpath id=\"MJX-2-TEX-N-35\" d=\"M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"1D441\" xlink:href=\"#MJX-2-TEX-I-1D441\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mo\" transform=\"translate(1165.8,0)\"\u003e\u003cuse data-c=\"3D\" xlink:href=\"#MJX-2-TEX-N-3D\"\u003e\u003c/use\u003e\u003c/g\u003e\u003cg data-mml-node=\"mn\" transform=\"translate(2221.6,0)\"\u003e\u003cuse data-c=\"35\" xlink:href=\"#MJX-2-TEX-N-35\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003e) を使用\u003c/p\u003e\n\u003cp\u003eデコーダ\u003c/p\u003e\n\u003cp\u003eビームサーチ with ビームサイズ = 3\u003c/p\u003e\n\u003cp\u003e最小sequence length = 20\u003c/p\u003e\n\u003cp\u003ebeam blocking ngram = 3\u003c/p\u003e\n\u003cp\u003e評価指標\u003c/p\u003e\n\u003cp\u003ePPL\u003c/p\u003e\n\u003cp\u003eF1\u003c/p\u003e\n\u003cp\u003egold responseとのオーバーラップを評価\u003c/p\u003e\n\u003cp\u003eKnowledge F1 (KF1)\u003c/p\u003e\n\u003cp\u003eモデルの応答と人間がデータ収集時に使った知識のオーバーラップを評価\u003c/p\u003e\n\u003cp\u003e→ F1とKF1はトレードオフ\u003c/p\u003e\n\u003cp\u003eKF1が高く，F1が低いと知識には富んでいるが，会話能力は低\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/l7s7sphq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTable 3からBART-largeを全てのモデルのPLMベースとして採用\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/ixgpnink.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/128zhnye.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e対話生成のプロセスにインターネットの情報を与えると，人との対話においてより事実との不整合の少ない情報を生成することができる\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch3\u003eDatasetの概要\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/c1e96cpy.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/upaxz8vo.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/w6dyemsg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWizard vs apprentice (Figure 3がペルソナ設定のインターフェース)\u003c/p\u003e\n\u003cp\u003eWizardは対話しながらネット検索ができる\u003c/p\u003e\n\u003cp\u003e→検索された結果をアノテーションし，適切な検索結果が得られなければもう一度検索でき，検索結果を無視することも可能\u003c/p\u003e\n\u003cp\u003eApprenticeはペルソナを選び，そのもとでチャット\u003c/p\u003e\n\u003cp\u003eペルソナはPersona-ChatとTopical-Chatデータセットに含まれるペルソナから選択\u003c/p\u003e\n\u003ch3\u003eデータ収集インターフェース\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/z4f9ivph.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003e対話例\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/xaxs9n1p.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/7wk8s9an.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/gktrd3cq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/5onmd3g5.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Internet-Augmented-Dialogue-Generation/el45rvtq.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@inproceedings{komeili-etal-2022-internet,\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003etitle = \"{I}nternet-Augmented Dialogue Generation\",\nauthor = \"Komeili, Mojtaba and\nShuster, Kurt and\nWeston, Jason\",\nbooktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\nmonth = may,\nyear = \"2022\",\naddress = \"Dublin, Ireland\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"\u003ca href=\"https://aclanthology.org/2022.acl-long.579\"\u003ehttps://aclanthology.org/2022.acl-long.579\u003c/a\u003e\",\ndoi = \"10.18653/v1/2022.acl-long.579\",\npages = \"8460--8478\",\nabstract = \"The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).\",\n}\u003c/p\u003e\u003cstyle\u003e\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] \u003e g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] \u003e rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e line[data-line], svg[data-table] \u003e g \u003e line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e rect[data-frame], svg[data-table] \u003e g \u003e rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dashed, svg[data-table] \u003e g \u003e .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dotted, svg[data-table] \u003e g \u003e .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e g \u003e svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool \u003e mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool \u003e mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n\u003c/style\u003e","Title":"【論文まとめ】Internet-Augmented Dialogue Generation","Date":"2023-05-21","Category":"論文","Tags":["dialogue system","Internet-Augmented"],"Authos":"ゆうぼう","Slug":"Internet-Augmented-Dialogue-Generation","Thumbnail":"/images/thumbnails/Internet-Augmented-Dialogue-Generation.png","Description":"Internet-Augmented Dialogue Generationのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\u003c/p\u003e\n\u003cp\u003e研究会: ACL\u003c/p\u003e\n\u003cp\u003e年度: 2020\u003c/p\u003e\n\u003cp\u003eキーワード: transformer, Highway Transformer, Gating Mechanism, Self-Dependency-Units (SDU)\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://aclanthology.org/2020.acl-main.616.pdf\"\u003ehttps://aclanthology.org/2020.acl-main.616.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"http://dx.doi.org/10.18653/v1/2020.acl-main.616\"\u003ehttp://dx.doi.org/10.18653/v1/2020.acl-main.616\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eコード: \u003ca href=\"https://github.com/cyk1337/Highway-Transformer\"\u003ehttps://github.com/cyk1337/Highway-Transformer\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: Penn Tree Bank (PTB), enwik8\u003c/p\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003eLSTM-styleなSDUを提案\u003c/p\u003e\n\u003cp\u003eゲートとしてSDUをTransformer内部に適用することにより，ハイパラをチューニングすることなく，Transformerの浅い層において，内在的な意味の重要性を捉え，より早い収束を可能に\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/qrpajdel.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eSelf-Dependency Units (SDU)\u003c/h3\u003e\n\u003cp\u003esigmoid gatesを導入する\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cmjx-container class=\"MathJax\" jax=\"SVG\"\u003e\u003csvg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.76ex\" height=\"1.545ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 778 683\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003cdefs\u003e\u003cpath id=\"MJX-1-TEX-N-3A8\" d=\"M340 622Q338 623 335 625T331 629T325 631T314 634T298 635T274 636T239 637H212V683H224Q248 680 389 680T554 683H566V637H539Q479 637 464 635T439 622L438 407Q438 192 439 192Q443 193 449 195T474 207T507 232T536 276T557 344Q560 365 562 417T573 493Q587 536 620 544Q627 546 671 546H715L722 540V515Q714 509 708 509Q680 505 671 476T658 392T644 307Q599 177 451 153L438 151V106L439 61Q446 54 451 52T476 48T539 46H566V0H554Q530 3 389 3T224 0H212V46H239Q259 46 273 46T298 47T314 48T325 51T331 54T335 57T340 61V151Q126 178 117 406Q115 503 69 509Q55 509 55 526Q55 541 59 543T86 546H107H120Q150 546 161 543T184 528Q198 514 204 493Q212 472 213 420T226 316T272 230Q287 216 303 207T330 194L339 192Q340 192 340 407V622Z\"\u003e\u003c/path\u003e\u003c/defs\u003e\u003cg stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"\u003e\u003cg data-mml-node=\"math\"\u003e\u003cg data-mml-node=\"mi\"\u003e\u003cuse data-c=\"3A8\" xlink:href=\"#MJX-1-TEX-N-3A8\"\u003e\u003c/use\u003e\u003c/g\u003e\u003c/g\u003e\u003c/g\u003e\u003c/svg\u003e\u003c/mjx-container\u003e\u003c/span\u003eはゲートとして作用し，logistic sigmoidもしくはtanhで実現\u003c/p\u003e\n\u003cp\u003e筆者らの認識\u003c/p\u003e\n\u003cp\u003etanhはupdate gateとして作用し，重要度の幅を-1 to 1に制限\u003c/p\u003e\n\u003cp\u003esigmoidはLSTMのinput gateと似ていて，feature-wise levelでどれくらいの情報を残すか決定\u003c/p\u003e\n\u003ch3\u003ePseudo-highway Connection\u003c/h3\u003e\n\u003cp\u003eresidual connectionされたgating-modified encodingsでMulti Head Dot Product Attention (MHDPA)の分散表現を豊かにするため，新たな計算グラフの枝を追加し，SDUとIdentityとMHDPAをpost LNを使用してresidual connectionする\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cp\u003e本来，人にとって，読み物をよりよく理解するためには，global contextだけではなく，ここの単語の意味も必要\u003c/p\u003e\n\u003cp\u003e→ Self-gatingなアプローチを提案\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTransformerにおける浅い層において，trainingとvalidationでハイパラチューニングすることなく，より高速な収束を達成\u003c/li\u003e\n\u003cli\u003eTransformerでの低レイヤーにおいて，local-range encodingにフォーカスした層を実現\u003c/li\u003e\n\u003cli\u003eSelf-gating mechanismは，R-TransformerやTransformer-XLのコンポーネントとしてRNN-likeなメカニズムを補完\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e実験\u003c/h2\u003e\n\u003cp\u003eSDUを導入し，PTBデータセットにおけるSDUの効果を検証\u003c/p\u003e\n\u003cp\u003esigmoidとtanhを実験\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/op874d4u.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/e99q8luh.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4xk5l3fv.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/gen0ole9.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/bjrucnwe.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4fv3x3v3.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/c6hfqjx9.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/aptzk9jc.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003esigmoidによるSDUが安定しているが，データとタスクによってはtanhの方がoutperformすることがある\u003c/p\u003e\n\u003cp\u003eいずれのactivationを使っても収束は早い\u003c/p\u003e\n\u003cp\u003eenwik8による大規模データでの追実験において，提案手法が浅いレイヤーには寄与することが確かめられた\u003c/p\u003e\n\u003cp\u003eSDUで計算量が増えるが，そこまで差はなかった\u003c/p\u003e\n\u003ch2\u003eその他（なぜ通ったか？等）\u003c/h2\u003e\n\u003ch2\u003e次読みたい論文\u003c/h2\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@inproceedings{chai-etal-2020-highway,\ntitle = \"Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\",\nauthor = \"Chai, Yekun  and\nJin, Shuo  and\nHou, Xinwen\",\nbooktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\nmonth = jul,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"\u003ca href=\"https://aclanthology.org/2020.acl-main.616\"\u003ehttps://aclanthology.org/2020.acl-main.616\u003c/a\u003e\",\ndoi = \"10.18653/v1/2020.acl-main.616\",\npages = \"6887--6900\",\nabstract = \"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.\",\n}\u003c/p\u003e\n\u003c/blockquote\u003e\u003cstyle\u003e\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] \u003e svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] \u003e g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] \u003e rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e line[data-line], svg[data-table] \u003e g \u003e line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e rect[data-frame], svg[data-table] \u003e g \u003e rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dashed, svg[data-table] \u003e g \u003e .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e .mjx-dotted, svg[data-table] \u003e g \u003e .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] \u003e g \u003e svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool \u003e mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool \u003e mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n\u003c/style\u003e","Title":"【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks","Date":"2023-05-21","Category":"論文","Tags":["transformer","Highway Transformer","Gating Mechanism","Self-Dependency-Units (SDU)"],"Authos":"ゆうぼう","Slug":"Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks","Thumbnail":"/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png","Description":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media\u003c/p\u003e\n\u003cp\u003e研究会: Information Processing \u0026#x26; Management\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: COMET, mental health, NLP, mental state knowledge, mentalisation, Contrasive Learning, MentalRoBERTa, KC-Net\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://www.sciencedirect.com/science/article/pii/S0306457322000796\"\u003ehttps://www.sciencedirect.com/science/article/pii/S0306457322000796\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"https://doi.org/10.1016/j.ipm.2022.102961\"\u003ehttps://doi.org/10.1016/j.ipm.2022.102961\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット: Depression_Mixed, Dreaddit, SQuAD\u003c/p\u003e\n\u003ch2\u003e提案手法\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/asccglgh.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e上の流れで学習して，メンタル状態を外部知識のEmbeddingを利用しながら捉える\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eData Preprocessing\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enltk sentence tokenizerを使ってpostを文区切にする\n\u003cul\u003e\n\u003cli\u003e→文ごとのmental stateを捉えるため\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eContext-aware post (CAP) encoder\nRoBERTaをdomain-specificなデータで学習した\u003cstrong\u003eMentalRoBERTa\u003c/strong\u003eなるものがあるのでそれを使って，context-awareなエンコーダとして使用する\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMental satte knowledge infusion\nmental stateの知識を捉えるため，ATOMICで学習されたGPTベースのCOMETを使用する\u003c/p\u003e\n\u003cp\u003e理由：\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e↑mental stateとmental health conditionの関係を捉えるために，ConceptNetではなくATOMICで学習されたものを使った\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConceptNet：一般的な言語の概念を含む\u003c/li\u003e\n\u003cli\u003eATOMIC：human interactionを捉えたcommonsenseを含む\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFeature extraction\n以下の5つのaspectを使用した\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eintent of S\u003c/li\u003e\n\u003cli\u003eeffect on S\u003c/li\u003e\n\u003cli\u003ereaction of S\u003c/li\u003e\n\u003cli\u003eeffect on others\u003c/li\u003e\n\u003cli\u003ereaction of others\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e面白ポイント：COMETのlm_headを削除し，Transformerの内部のみをEncoderとして扱う\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e直接的にpost representationをモデルに統合できて，mental-related variablesを適応することが期待できる\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-markdown\"\u003e\u003cspan class=\"hljs-code\"\u003e\tCAP embeddingsによるtoken-level representationは，max poolingによってsentence-level representationとされる\n\u003c/span\u003e\n\u003cspan class=\"hljs-code\"\u003e\t$\\hat{H}_j^i = max\\_pooling(H[P_{j-1}^i : P_j^i])$\n\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e2.\u003c/span\u003e Knowledge-aware mentalisation\n\u003cspan class=\"hljs-code\"\u003e\t5つの独立したGRUを使用して，mentalのaspect毎に学習するスタイル\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eこれでpost-level representationになる\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-arduino\"\u003e\tその後GRUによるmental aspectごとのpost-level representationとmax poolingされたsentence-level representationをAttentionすることで統合する\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eSupervised contrasive learning\nより文章のsemantic meaningに注意して学習するために，contrasive learningを使用した\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/9nzqkqxg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/u6fwk2ku.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e新規性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003emental state knolwedgeを使うことでスピーカー（実験ではpostした人）のmental stateを明示的にモデル化する\u003c/li\u003e\n\u003cli\u003emodel state knowledgeを理解し，使うモデルの能力を強くするため，knowledge-aware dot-product attentionに基づくmentalisation moduleを導入\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e評価方法\u003c/h2\u003e\n\u003cp\u003ebaseline\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCNN\u003c/li\u003e\n\u003cli\u003eGRU\u003c/li\u003e\n\u003cli\u003eBiLSTM_Attn\u003c/li\u003e\n\u003cli\u003eLR+Features (Logistic Regression)\u003c/li\u003e\n\u003cli\u003eEMO_INF\u003c/li\u003e\n\u003cli\u003eBERT\u003c/li\u003e\n\u003cli\u003eRoBERTa\u003c/li\u003e\n\u003cli\u003eMentalRoBERTa\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePrecision / Recall / F1を比較\u003c/p\u003e\n\u003ch2\u003e何がすごかった？\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/82djhuwa.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/9rsi0ppo.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elabel情報を完全に利用するためのsupervised contrasive learningを使用することでclass-specificな特徴量を捉える必要性を議論\u003c/li\u003e\n\u003cli\u003eSOTAモデル on three stress and depression detection datasets\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e次に読みたい論文\u003c/h2\u003e\n\u003cp\u003eCEM: Commonsense-aware Empathetic Response Generation\u003c/p\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e@article{YANG2022102961,\ntitle = {A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media},\njournal = {Information Processing \u0026#x26; Management},\nvolume = {59},\nnumber = {4},\npages = {102961},\nyear = {2022},\nissn = {0306-4573},\ndoi = {\u003ca href=\"https://doi.org/10.1016/j.ipm.2022.102961\"\u003ehttps://doi.org/10.1016/j.ipm.2022.102961\u003c/a\u003e},\nurl = {\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S0306457322000796\"\u003ehttps://www.sciencedirect.com/science/article/pii/S0306457322000796\u003c/a\u003e},\nauthor = {Kailai Yang and Tianlin Zhang and Sophia Ananiadou},\nkeywords = {Mental health, Natural language processing, Mental state knowledge, Mentalisation, Contrastive learning},\nabstract = {Stress and depression detection on social media aim at the analysis of stress and identification of depression tendency from social media posts, which provide assistance for the early detection of mental health conditions. Existing methods mainly model the mental states of the post speaker implicitly. They also lack the ability to mentalise for complex mental state reasoning. Besides, they are not designed to explicitly capture class-specific features. To resolve the above issues, we propose a mental state Knowledge–aware and Contrastive Network (KC-Net). In detail, we first extract mental state knowledge from a commonsense knowledge base COMET, and infuse the knowledge using Gated Recurrent Units (GRUs) to explicitly model the mental states of the speaker. Then we propose a knowledge–aware mentalisation module based on dot-product attention to accordingly attend to the most relevant knowledge aspects. A supervised contrastive learning module is also utilised to fully leverage label information for capturing class-specific features. We test the proposed methods on a depression detection dataset Depression_Mixed with 3165 Reddit and blog posts, a stress detection dataset Dreaddit with 3553 Reddit posts, and a stress factors recognition dataset SAD with 6850 SMS-like messages. The experimental results show that our method achieves new state-of-the-art results on all datasets: 95.4% of F1 scores on Depression_Mixed, 83.5% on Dreaddit and 77.8% on SAD, with 2.07% average improvement. Factor-specific analysis and ablation study prove the effectiveness of all proposed modules, while UMAP analysis and case study visualise their mechanisms. We believe our work facilitates detection and analysis of depression and stress on social media data, and shows potential for applications on other mental health conditions.}\n}\u003c/p\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media","Date":"2023-05-21","Category":"論文","Tags":["COMET","mental health","NLP","mental state knowledge","mentalisation","Contrasive Learning","MentalRoBERTa","KC-Net"],"Authos":"ゆうぼう","Slug":"A-mental-state-Knowledge–aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media","Thumbnail":"/images/thumbnails/A-mental-state-Knowledge–aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media.png","Description":"A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social mediaのまとめ","Published":true},{"contentHtml":"\u003cp\u003e本記事において使用される図表は，原著論文内の図表を引用しています．\u003c/p\u003e\n\u003cp\u003eまた，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．\u003c/p\u003e\n\u003ch2\u003e論文情報\u003c/h2\u003e\n\u003cp\u003eタイトル: A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models\u003c/p\u003e\n\u003cp\u003e研究会: arxiv\u003c/p\u003e\n\u003cp\u003e年度: 2022\u003c/p\u003e\n\u003cp\u003eキーワード: survey, NLP, knowledge-base, PLMKE, commonsense, encyclopedic, Knowledge-Intensive NLP\u003c/p\u003e\n\u003cp\u003eURL: \u003ca href=\"https://arxiv.org/pdf/2202.08772.pdf\"\u003ehttps://arxiv.org/pdf/2202.08772.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDOI: \u003ca href=\"https://doi.org/10.48550/arXiv.2202.08772\"\u003ehttps://doi.org/10.48550/arXiv.2202.08772\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eデータセット:\u003c/p\u003e\n\u003cp\u003eまとめること\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eKnowledge-Intensive NLPの概要\u003c/li\u003e\n\u003cli\u003eKnowledge Sources\n\u003col\u003e\n\u003cli\u003eEncyclopedic Knowledge\u003c/li\u003e\n\u003cli\u003eCommonsense Knowledge\u003c/li\u003e\n\u003cli\u003e最近のKnowledge Sourcesの特徴\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eKnowledge-Intensive NLP Task\n\u003col\u003e\n\u003cli\u003eKnowledge-Intensive NLP Taskの概要\u003c/li\u003e\n\u003cli\u003eKnowledge-Intensive NLP Taskの特徴\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eKnowledge Fusion Methodsについて\n\u003col\u003e\n\u003cli\u003ePre-Fusion Methods\u003c/li\u003e\n\u003cli\u003ePost-Fusion Methods\u003c/li\u003e\n\u003cli\u003eHybrid-Fusion Methods\u003c/li\u003e\n\u003cli\u003e代表的なモデルの紹介\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eChallengingなことと今後の方向性\n\u003col\u003e\n\u003cli\u003eUnified PLMKEs Across Tasks and Domains\u003c/li\u003e\n\u003cli\u003eReliability of Knowledge Sources\u003c/li\u003e\n\u003cli\u003eReasoning Module Design\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e概要\u003c/h2\u003e\n\u003cp\u003e事前学習済みモデルにより，モデルのcapacityは増加傾向にあるが，encyclopedicやcommonsenseを用いた，knowledgeableなNLPモデルの需要の高まりが生じている\u003c/p\u003e\n\u003cp\u003e**PLMKEs (Pre-trained Language Model-based Knowledge-Enhanced models)**についてまとめたsurvey論文\u003c/p\u003e\n\u003cp\u003elinguistic or factual knowledgeは暗示的にモデルのパラメータに保存される\u003c/p\u003e\n\u003cp\u003e→事前学習済みのNLPモデルがより汎用的な能力を持つことを一部ではあるが説明できる\u003c/p\u003e\n\u003cp\u003e今のpre-trained LMは，明示的なencyclopedicやcommonsenseのレバレッジ能力に欠けている\u003c/p\u003e\n\u003cp\u003ePLMKEsは，関連する外部知識を取り出すモジュールと知識を混ぜるモジュールがある\u003c/p\u003e\n\u003cp\u003ePLMKEsに関連した重要な3つの要素がある\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eKnowledge Sources\u003c/li\u003e\n\u003cli\u003eKnowledge-Intensive NLP Tasks\u003c/li\u003e\n\u003cli\u003eKnowledge Fusion Methods\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eKnowledge Sources\u003c/h2\u003e\n\u003ch3\u003eEncyclopedic knowledge\u003c/h3\u003e\n\u003cp\u003eエンティティに関する属性とエンティティ間の関係性をもった知識\u003c/p\u003e\n\u003cp\u003eEntity: person → Attributes: age → Relations: educated at\u003c/p\u003e\n\u003cp\u003eWikipediaは大量のencyclopedicな知識を持っている\u003c/p\u003e\n\u003cp\u003e人物の経歴やイベントの背景などを含んでいる\u003c/p\u003e\n\u003cp\u003e一般的にはtripletsで構成されていることが多い\u003c/p\u003e\n\u003cp\u003ee.g. \u0026#x3C;Tom Hanks, occupation, actor\u003e\u003c/p\u003e\n\u003cp\u003eWikidataのような知識データがPLMKEsに広く使用されている\u003c/p\u003e\n\u003ch3\u003eCommonsense Knowledge\u003c/h3\u003e\n\u003cp\u003e日常生活のなかでの状況に関する知識\u003c/p\u003e\n\u003cp\u003eイベントとその影響を記す\u003c/p\u003e\n\u003cp\u003ee.g. mop up the floor if we split food over it / study hard to win scholarship / goat has four legs\u003c/p\u003e\n\u003cp\u003ecommonsenseの特徴\u003c/p\u003e\n\u003cp\u003e多くの人の間で共有されている知識であり，コミュニケーションの中で暗示的に想定されている知識である\u003c/p\u003e\n\u003cp\u003ecommonsenseもtripletsで表現される\u003c/p\u003e\n\u003cp\u003e最近のPLMKEsでは，ConceptNetとATOMICが外部知識として使用されることが多い\u003c/p\u003e\n\u003ch3\u003eKnowledge Sourcesの特徴\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/gcgqsmdk.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003elarge-scaleでdiverse\u003c/p\u003e\n\u003cp\u003e現在のソースはより正確で安定的に作られている\u003c/p\u003e\n\u003cp\u003eアノテーションのプロセスは部分的に自動化されていて，非エキスパートにもaccessibleになっている\u003c/p\u003e\n\u003cp\u003e知識データがカバーするドメインは多様\u003c/p\u003e\n\u003cp\u003eオープンドメインのものもあれば，specificなドメインのものも\u003c/p\u003e\n\u003cp\u003eWikipedia, DBPedia, Freebaseなどはオープンドメイン\u003c/p\u003e\n\u003cp\u003eUMLSやAMinerなどはbiomedicineやscienceの特定ドメイン\u003c/p\u003e\n\u003cp\u003edomain-specificなアプリケーションをブーストできる知識\u003c/p\u003e\n\u003cp\u003ecommonsenseに関しては\u003c/p\u003e\n\u003cp\u003eConceptNetやTransOMCSは複数のドメインのcommonsenseをカバー\u003c/p\u003e\n\u003cp\u003eATOMICやASERはある特定のタイプのcommonsenseにフォーカスした知識ソース\u003c/p\u003e\n\u003ch2\u003eKnowledge-Intensive NLP Task\u003c/h2\u003e\n\u003ch3\u003e概要\u003c/h3\u003e\n\u003cp\u003eKnowledge-intensive NLP taskは必要とする知識ソースの種類で2つに分けられ，さらに詳細に分けることができる\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/ju1lqjam.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eencyclopedic knowledge-intensive NLP task\nencyclopedicの知識ソースを利用する\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eopen-domain QA\u003c/li\u003e\n\u003cli\u003efact verification\u003c/li\u003e\n\u003cli\u003eentity linking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/kr89pm58.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ecommonsense knowledge-intensive NLP task\ncommonsenseの知識ソースを利用する\u003c/p\u003e\n\u003cp\u003ecommonsenseの多様性のために，タスクのタイプ自体も多様化している\u003c/p\u003e\n\u003cp\u003eモデルが正確に日常のシナリオを理解し，応答するか否かのテストにフォーカスしたタスク\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGeneral Commonsense\u003c/li\u003e\n\u003cli\u003eSocial Commonsense\u003c/li\u003e\n\u003cli\u003ePhysical Commonsense\u003c/li\u003e\n\u003cli\u003eTemporal Commonsense\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eKnowledge-Intensive Taskの特徴\u003c/h3\u003e\n\u003cp\u003e実際は，モデルにとってだけではなく，人間にとってもいかなる知識の参照なしに問題に答えるのは難しい．（バラクオバマの誕生日はいつ？など\u003c/p\u003e\n\u003cp\u003eしかも，外部知識が必要なのにinputとして必要な外部知識が渡されないため，とてもチャレンジングなタスクになっている\u003c/p\u003e\n\u003cp\u003eそもそも必要な外部知識にグラウンディングするモジュールをPLMKEsの設計に加えることを考慮するようになっている\u003c/p\u003e\n\u003ch2\u003eKnowledge Fusion Methods\u003c/h2\u003e\n\u003cp\u003eモデルが知識を統合するステージは二箇所あり，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePre-fusion; pre-training\u003c/li\u003e\n\u003cli\u003ePost-fusion; fine-tuning\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eの二通りが考えられる（もしくはその両方のステージ\u003c/p\u003e\n\u003ch3\u003ePre-Fusion Methods\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/z4lvh39q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003epre-trainingのステージで知識を統合する手法\u003c/p\u003e\n\u003cp\u003eモデルに知識を入力するため，構造化された知識データを非構造化データのテキストコーパスへと処理→モデルに入力\u003c/p\u003e\n\u003cp\u003eテキストデータとして知識を入力するため，大きくモデルのアーキテクチャを変更する必要はない\u003c/p\u003e\n\u003cp\u003eただし，知識グラフのような構造化データを非構造化データへ変えることは難しいこともある\u003c/p\u003e\n\u003cp\u003e簡単な対処法はエンティティと関係性を結合するか，流暢な文章をconditional text generation modelに生成させるか\u003c/p\u003e\n\u003cp\u003eZhang et al. 2019 | Agarwal et al. 2021 を参照（必要になれば読む\u003c/p\u003e\n\u003ch3\u003ePost-Fusion Methods\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/bshr899d.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eまず，関連知識をキャプチャする\u003c/p\u003e\n\u003cp\u003e次に取得した関連知識をGNNなどのエンコーダでembeddingを得る\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eそれを追加特徴量としてpre-trained LMに与える（図でいうA）\u003c/li\u003e\n\u003cli\u003e直接pre-trained LMに入力する（図でいうB）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eHybrid-Fusion Methods\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/1yg24grj.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003epre-trainingとfine-tuningの両方のステージで知識を統合する\u003c/p\u003e\n\u003cp\u003e追加の学習されるretrieverによりaugmentされたpre-trained modelは，fine-tuningのステージでより効果的にretrieverからの知識を活用できる\u003c/p\u003e\n\u003cp\u003eretrieval-augmented pre-trainingでhybrid-fusionが広く使われている\u003c/p\u003e\n\u003ch3\u003e代表的なモデル\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/xh93uokd.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/cl36oka6.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTable4/5はSOTAモデルを示す\u003c/p\u003e\n\u003cp\u003eencyclopedic knowledge-intensive taskにおいては，BOOLQをのぞき，全てpost-fusionを採用\u003c/p\u003e\n\u003cp\u003ecommonsense knowledge-intensive taskにおいては，CommonsenseQAをのぞき，全てpre-fusionを採用\u003c/p\u003e\n\u003cp\u003epre-fusionとpost-fusionの違いは何？\u003c/p\u003e\n\u003cp\u003epre-fusionは，知識を事前学習のパラメータに暗示的に保存数る\u003c/p\u003e\n\u003cp\u003e最終的にどの知識がパラメータに保存するのかを決定するのは難しい\u003c/p\u003e\n\u003cp\u003e知識の引き出しや利用の難しさが増す\u003c/p\u003e\n\u003cp\u003epost-fusionは，明示的で具体的なテキストの知識を推論できる\u003c/p\u003e\n\u003cp\u003epost-fusionの利点は，commonsense knolwedge-intensive taskでは欠点になりうる\u003c/p\u003e\n\u003cp\u003ecommonsenseはたいていテキストの中に暗示的に置かれていて，commonsenseの知識ソースのカバー範囲はencyclopedicの知識ソースのカバー範囲に比べればとても小さい\u003c/p\u003e\n\u003cp\u003elarge-scaleなcommonsenseのソースの利用がたとえ有用だとしても，日常生活で使われる大半のcommonsenseを見落としがちなまま\u003c/p\u003e\n\u003cp\u003e→commonsenseにおいて，post-fusionがあまり効かないのはそのためなのでは？\u003c/p\u003e\n\u003ch2\u003eChallenges and Future Directions\u003c/h2\u003e\n\u003ch3\u003eUnified PLMKEs Across Tasks and Domains\u003c/h3\u003e\n\u003cp\u003etask-specificなモデリングでは進展がある\u003c/p\u003e\n\u003cp\u003epost-fusionとhybrid-fusionはencyclopedicで適用されているが，commonsenseでは採用できておらず恩恵が得られていない\u003c/p\u003e\n\u003cp\u003e異なるタスク間でのPLMKEsはユニークであるため，各タスク間で互換性がない\u003c/p\u003e\n\u003cp\u003ebiomedicalやlegalの知識に関するknowledge-intensive NLP taskまで拡張されている\u003c/p\u003e\n\u003cp\u003e最近では，異なる時間や地域に存在する知識の多様性に対しても重要度を割り当てている\u003c/p\u003e\n\u003cp\u003eタスク間やdomain間でのunified PLMKEsの必要性がましている\u003c/p\u003e\n\u003ch3\u003eReliability of Kowledge Sources\u003c/h3\u003e\n\u003cp\u003e知識ソースの信頼性に関して\u003c/p\u003e\n\u003cp\u003e多くのlarge-scaleな知識ソースは自動的な知識獲得アルゴリズムで構築されている\u003c/p\u003e\n\u003cp\u003e→スケールと正確性はトレードオフになってしまう\u003c/p\u003e\n\u003cp\u003ePLMKEsにおけるバイアスの増幅はバイアスのある知識ソースによって構築されてしまう\u003c/p\u003e\n\u003cp\u003e知識獲得アルゴリズムの見直しや使う前の知識ソースの注意深い精査が必要である\u003c/p\u003e\n\u003ch3\u003eReasoning Module Design\u003c/h3\u003e\n\u003cp\u003eReasoningはknowledge-intensive NLP taskを解く上で重要なステップである\u003c/p\u003e\n\u003cp\u003ecommonsenseを考えるときは手順を踏んで，複雑な状況を把握する\u003c/p\u003e\n\u003cp\u003ee.g. \u003c/p\u003e\n\u003cp\u003eまず，床が綺麗でないことを把握\u003c/p\u003e\n\u003cp\u003eこぼした食べ物を踏んで他の人の靴が汚くなったのだろうと考える\u003c/p\u003e\n\u003cp\u003e↑上記状況を踏まえて，モップをかける意図が生まれる\u003c/p\u003e\n\u003cp\u003e人間のような日々の状況を認識する能力を獲得するには，multi-hopなreasoning moduleが必要になる（上の例みたいな形\u003c/p\u003e\n\u003ch2\u003e引用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003c/blockquote\u003e","Title":"【論文まとめ】A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models","Date":"2023-05-21","Category":"論文","Tags":["survey","NLP","knowledge-base","PLMKE","commonsense","encyclopedic","Knowledge-Intensive NLP"],"Authos":"ゆうぼう","Slug":"A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models","Thumbnail":"/images/thumbnails/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models.png","Description":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Modelsのまとめ","Published":true}],"category":"論文","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Dialogue Structure Learning","dialogue system","DST","empathetic dialogue system","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"],"page":1},"__N_SSG":true},"page":"/category/[category]/[page]","query":{"category":"論文","page":"1"},"buildId":"qAoBOyZoxCIoSGdt5co7N","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>