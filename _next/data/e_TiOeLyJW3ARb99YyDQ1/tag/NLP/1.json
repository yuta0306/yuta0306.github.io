{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media</p>\n<p>研究会: Information Processing &#x26; Management</p>\n<p>年度: 2022</p>\n<p>キーワード: COMET, mental health, NLP, mental state knowledge, mentalisation, Contrasive Learning, MentalRoBERTa, KC-Net</p>\n<p>URL: <a href=\"https://www.sciencedirect.com/science/article/pii/S0306457322000796\">https://www.sciencedirect.com/science/article/pii/S0306457322000796</a></p>\n<p>DOI: <a href=\"https://doi.org/10.1016/j.ipm.2022.102961\">https://doi.org/10.1016/j.ipm.2022.102961</a></p>\n<p>データセット: Depression_Mixed, Dreaddit, SQuAD</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/asccglgh.png\" alt=\"\"></p>\n<p>上の流れで学習して，メンタル状態を外部知識のEmbeddingを利用しながら捉える</p>\n<ol>\n<li>\n<p>Data Preprocessing</p>\n<ul>\n<li>nltk sentence tokenizerを使ってpostを文区切にする\n<ul>\n<li>→文ごとのmental stateを捉えるため</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Context-aware post (CAP) encoder\nRoBERTaをdomain-specificなデータで学習した<strong>MentalRoBERTa</strong>なるものがあるのでそれを使って，context-awareなエンコーダとして使用する</p>\n</li>\n<li>\n<p>Mental satte knowledge infusion\nmental stateの知識を捉えるため，ATOMICで学習されたGPTベースのCOMETを使用する</p>\n<p>理由：</p>\n</li>\n</ol>\n<p>↑mental stateとmental health conditionの関係を捉えるために，ConceptNetではなくATOMICで学習されたものを使った</p>\n<ul>\n<li>ConceptNet：一般的な言語の概念を含む</li>\n<li>ATOMIC：human interactionを捉えたcommonsenseを含む\n<ol>\n<li>\n<p>Feature extraction\n以下の5つのaspectを使用した</p>\n<ul>\n<li>intent of S</li>\n<li>effect on S</li>\n<li>reaction of S</li>\n<li>effect on others</li>\n<li>reaction of others</li>\n</ul>\n<p><strong>面白ポイント：COMETのlm_headを削除し，Transformerの内部のみをEncoderとして扱う</strong></p>\n</li>\n</ol>\n</li>\n</ul>\n<p>直接的にpost representationをモデルに統合できて，mental-related variablesを適応することが期待できる</p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-code\">\tCAP embeddingsによるtoken-level representationは，max poolingによってsentence-level representationとされる\n</span>\n<span class=\"hljs-code\">\t$\\hat{H}_j^i = max\\_pooling(H[P_{j-1}^i : P_j^i])$\n</span>\n<span class=\"hljs-bullet\">2.</span> Knowledge-aware mentalisation\n<span class=\"hljs-code\">\t5つの独立したGRUを使用して，mentalのaspect毎に学習するスタイル\n</span></code></pre>\n<p>これでpost-level representationになる</p>\n<pre><code class=\"hljs language-arduino\">\tその後GRUによるmental aspectごとのpost-level representationとmax poolingされたsentence-level representationをAttentionすることで統合する\n</code></pre>\n<ol start=\"4\">\n<li>Supervised contrasive learning\nより文章のsemantic meaningに注意して学習するために，contrasive learningを使用した</li>\n</ol>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/9nzqkqxg.png\" alt=\"\"></p>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/u6fwk2ku.png\" alt=\"\"></p>\n<h2>新規性</h2>\n<ul>\n<li>mental state knolwedgeを使うことでスピーカー（実験ではpostした人）のmental stateを明示的にモデル化する</li>\n<li>model state knowledgeを理解し，使うモデルの能力を強くするため，knowledge-aware dot-product attentionに基づくmentalisation moduleを導入</li>\n</ul>\n<h2>評価方法</h2>\n<p>baseline</p>\n<ul>\n<li>CNN</li>\n<li>GRU</li>\n<li>BiLSTM_Attn</li>\n<li>LR+Features (Logistic Regression)</li>\n<li>EMO_INF</li>\n<li>BERT</li>\n<li>RoBERTa</li>\n<li>MentalRoBERTa</li>\n</ul>\n<p>Precision / Recall / F1を比較</p>\n<h2>何がすごかった？</h2>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/82djhuwa.png\" alt=\"\"></p>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/9rsi0ppo.png\" alt=\"\"></p>\n<ul>\n<li>label情報を完全に利用するためのsupervised contrasive learningを使用することでclass-specificな特徴量を捉える必要性を議論</li>\n<li>SOTAモデル on three stress and depression detection datasets</li>\n</ul>\n<h2>次に読みたい論文</h2>\n<p>CEM: Commonsense-aware Empathetic Response Generation</p>\n<h2>引用</h2>\n<blockquote>\n<p>@article{YANG2022102961,\ntitle = {A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media},\njournal = {Information Processing &#x26; Management},\nvolume = {59},\nnumber = {4},\npages = {102961},\nyear = {2022},\nissn = {0306-4573},\ndoi = {<a href=\"https://doi.org/10.1016/j.ipm.2022.102961\">https://doi.org/10.1016/j.ipm.2022.102961</a>},\nurl = {<a href=\"https://www.sciencedirect.com/science/article/pii/S0306457322000796\">https://www.sciencedirect.com/science/article/pii/S0306457322000796</a>},\nauthor = {Kailai Yang and Tianlin Zhang and Sophia Ananiadou},\nkeywords = {Mental health, Natural language processing, Mental state knowledge, Mentalisation, Contrastive learning},\nabstract = {Stress and depression detection on social media aim at the analysis of stress and identification of depression tendency from social media posts, which provide assistance for the early detection of mental health conditions. Existing methods mainly model the mental states of the post speaker implicitly. They also lack the ability to mentalise for complex mental state reasoning. Besides, they are not designed to explicitly capture class-specific features. To resolve the above issues, we propose a mental state Knowledge–aware and Contrastive Network (KC-Net). In detail, we first extract mental state knowledge from a commonsense knowledge base COMET, and infuse the knowledge using Gated Recurrent Units (GRUs) to explicitly model the mental states of the speaker. Then we propose a knowledge–aware mentalisation module based on dot-product attention to accordingly attend to the most relevant knowledge aspects. A supervised contrastive learning module is also utilised to fully leverage label information for capturing class-specific features. We test the proposed methods on a depression detection dataset Depression_Mixed with 3165 Reddit and blog posts, a stress detection dataset Dreaddit with 3553 Reddit posts, and a stress factors recognition dataset SAD with 6850 SMS-like messages. The experimental results show that our method achieves new state-of-the-art results on all datasets: 95.4% of F1 scores on Depression_Mixed, 83.5% on Dreaddit and 77.8% on SAD, with 2.07% average improvement. Factor-specific analysis and ablation study prove the effectiveness of all proposed modules, while UMAP analysis and case study visualise their mechanisms. We believe our work facilitates detection and analysis of depression and stress on social media data, and shows potential for applications on other mental health conditions.}\n}</p>\n</blockquote>","Title":"【論文まとめ】A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media","Date":"2023-05-21","Category":"論文","Tags":["COMET","mental health","NLP","mental state knowledge","mentalisation","Contrasive Learning","MentalRoBERTa","KC-Net"],"Authos":"ゆうぼう","Slug":"A-mental-state-Knowledge–aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media","Thumbnail":"/images/thumbnails/A-mental-state-Knowledge–aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media.png","Description":"A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social mediaのまとめ","Published":true},{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models</p>\n<p>研究会: arxiv</p>\n<p>年度: 2022</p>\n<p>キーワード: survey, NLP, knowledge-base, PLMKE, commonsense, encyclopedic, Knowledge-Intensive NLP</p>\n<p>URL: <a href=\"https://arxiv.org/pdf/2202.08772.pdf\">https://arxiv.org/pdf/2202.08772.pdf</a></p>\n<p>DOI: <a href=\"https://doi.org/10.48550/arXiv.2202.08772\">https://doi.org/10.48550/arXiv.2202.08772</a></p>\n<p>データセット:</p>\n<p>まとめること</p>\n<ol>\n<li>Knowledge-Intensive NLPの概要</li>\n<li>Knowledge Sources\n<ol>\n<li>Encyclopedic Knowledge</li>\n<li>Commonsense Knowledge</li>\n<li>最近のKnowledge Sourcesの特徴</li>\n</ol>\n</li>\n<li>Knowledge-Intensive NLP Task\n<ol>\n<li>Knowledge-Intensive NLP Taskの概要</li>\n<li>Knowledge-Intensive NLP Taskの特徴</li>\n</ol>\n</li>\n<li>Knowledge Fusion Methodsについて\n<ol>\n<li>Pre-Fusion Methods</li>\n<li>Post-Fusion Methods</li>\n<li>Hybrid-Fusion Methods</li>\n<li>代表的なモデルの紹介</li>\n</ol>\n</li>\n<li>Challengingなことと今後の方向性\n<ol>\n<li>Unified PLMKEs Across Tasks and Domains</li>\n<li>Reliability of Knowledge Sources</li>\n<li>Reasoning Module Design</li>\n</ol>\n</li>\n</ol>\n<h2>概要</h2>\n<p>事前学習済みモデルにより，モデルのcapacityは増加傾向にあるが，encyclopedicやcommonsenseを用いた，knowledgeableなNLPモデルの需要の高まりが生じている</p>\n<p>**PLMKEs (Pre-trained Language Model-based Knowledge-Enhanced models)**についてまとめたsurvey論文</p>\n<p>linguistic or factual knowledgeは暗示的にモデルのパラメータに保存される</p>\n<p>→事前学習済みのNLPモデルがより汎用的な能力を持つことを一部ではあるが説明できる</p>\n<p>今のpre-trained LMは，明示的なencyclopedicやcommonsenseのレバレッジ能力に欠けている</p>\n<p>PLMKEsは，関連する外部知識を取り出すモジュールと知識を混ぜるモジュールがある</p>\n<p>PLMKEsに関連した重要な3つの要素がある</p>\n<ol>\n<li>Knowledge Sources</li>\n<li>Knowledge-Intensive NLP Tasks</li>\n<li>Knowledge Fusion Methods</li>\n</ol>\n<h2>Knowledge Sources</h2>\n<h3>Encyclopedic knowledge</h3>\n<p>エンティティに関する属性とエンティティ間の関係性をもった知識</p>\n<p>Entity: person → Attributes: age → Relations: educated at</p>\n<p>Wikipediaは大量のencyclopedicな知識を持っている</p>\n<p>人物の経歴やイベントの背景などを含んでいる</p>\n<p>一般的にはtripletsで構成されていることが多い</p>\n<p>e.g. &#x3C;Tom Hanks, occupation, actor></p>\n<p>Wikidataのような知識データがPLMKEsに広く使用されている</p>\n<h3>Commonsense Knowledge</h3>\n<p>日常生活のなかでの状況に関する知識</p>\n<p>イベントとその影響を記す</p>\n<p>e.g. mop up the floor if we split food over it / study hard to win scholarship / goat has four legs</p>\n<p>commonsenseの特徴</p>\n<p>多くの人の間で共有されている知識であり，コミュニケーションの中で暗示的に想定されている知識である</p>\n<p>commonsenseもtripletsで表現される</p>\n<p>最近のPLMKEsでは，ConceptNetとATOMICが外部知識として使用されることが多い</p>\n<h3>Knowledge Sourcesの特徴</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/gcgqsmdk.png\" alt=\"\"></p>\n<p>large-scaleでdiverse</p>\n<p>現在のソースはより正確で安定的に作られている</p>\n<p>アノテーションのプロセスは部分的に自動化されていて，非エキスパートにもaccessibleになっている</p>\n<p>知識データがカバーするドメインは多様</p>\n<p>オープンドメインのものもあれば，specificなドメインのものも</p>\n<p>Wikipedia, DBPedia, Freebaseなどはオープンドメイン</p>\n<p>UMLSやAMinerなどはbiomedicineやscienceの特定ドメイン</p>\n<p>domain-specificなアプリケーションをブーストできる知識</p>\n<p>commonsenseに関しては</p>\n<p>ConceptNetやTransOMCSは複数のドメインのcommonsenseをカバー</p>\n<p>ATOMICやASERはある特定のタイプのcommonsenseにフォーカスした知識ソース</p>\n<h2>Knowledge-Intensive NLP Task</h2>\n<h3>概要</h3>\n<p>Knowledge-intensive NLP taskは必要とする知識ソースの種類で2つに分けられ，さらに詳細に分けることができる</p>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/ju1lqjam.png\" alt=\"\"></p>\n<ul>\n<li>\n<p>encyclopedic knowledge-intensive NLP task\nencyclopedicの知識ソースを利用する</p>\n<ul>\n<li>open-domain QA</li>\n<li>fact verification</li>\n<li>entity linking</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/kr89pm58.png\" alt=\"\"></p>\n<ul>\n<li>\n<p>commonsense knowledge-intensive NLP task\ncommonsenseの知識ソースを利用する</p>\n<p>commonsenseの多様性のために，タスクのタイプ自体も多様化している</p>\n<p>モデルが正確に日常のシナリオを理解し，応答するか否かのテストにフォーカスしたタスク</p>\n<ul>\n<li>General Commonsense</li>\n<li>Social Commonsense</li>\n<li>Physical Commonsense</li>\n<li>Temporal Commonsense</li>\n</ul>\n</li>\n</ul>\n<h3>Knowledge-Intensive Taskの特徴</h3>\n<p>実際は，モデルにとってだけではなく，人間にとってもいかなる知識の参照なしに問題に答えるのは難しい．（バラクオバマの誕生日はいつ？など</p>\n<p>しかも，外部知識が必要なのにinputとして必要な外部知識が渡されないため，とてもチャレンジングなタスクになっている</p>\n<p>そもそも必要な外部知識にグラウンディングするモジュールをPLMKEsの設計に加えることを考慮するようになっている</p>\n<h2>Knowledge Fusion Methods</h2>\n<p>モデルが知識を統合するステージは二箇所あり，</p>\n<ul>\n<li>Pre-fusion; pre-training</li>\n<li>Post-fusion; fine-tuning</li>\n</ul>\n<p>の二通りが考えられる（もしくはその両方のステージ</p>\n<h3>Pre-Fusion Methods</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/z4lvh39q.png\" alt=\"\"></p>\n<p>pre-trainingのステージで知識を統合する手法</p>\n<p>モデルに知識を入力するため，構造化された知識データを非構造化データのテキストコーパスへと処理→モデルに入力</p>\n<p>テキストデータとして知識を入力するため，大きくモデルのアーキテクチャを変更する必要はない</p>\n<p>ただし，知識グラフのような構造化データを非構造化データへ変えることは難しいこともある</p>\n<p>簡単な対処法はエンティティと関係性を結合するか，流暢な文章をconditional text generation modelに生成させるか</p>\n<p>Zhang et al. 2019 | Agarwal et al. 2021 を参照（必要になれば読む</p>\n<h3>Post-Fusion Methods</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/bshr899d.png\" alt=\"\"></p>\n<p>まず，関連知識をキャプチャする</p>\n<p>次に取得した関連知識をGNNなどのエンコーダでembeddingを得る</p>\n<ul>\n<li>それを追加特徴量としてpre-trained LMに与える（図でいうA）</li>\n<li>直接pre-trained LMに入力する（図でいうB）</li>\n</ul>\n<h3>Hybrid-Fusion Methods</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/1yg24grj.png\" alt=\"\"></p>\n<p>pre-trainingとfine-tuningの両方のステージで知識を統合する</p>\n<p>追加の学習されるretrieverによりaugmentされたpre-trained modelは，fine-tuningのステージでより効果的にretrieverからの知識を活用できる</p>\n<p>retrieval-augmented pre-trainingでhybrid-fusionが広く使われている</p>\n<h3>代表的なモデル</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/xh93uokd.png\" alt=\"\"></p>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/cl36oka6.png\" alt=\"\"></p>\n<p>Table4/5はSOTAモデルを示す</p>\n<p>encyclopedic knowledge-intensive taskにおいては，BOOLQをのぞき，全てpost-fusionを採用</p>\n<p>commonsense knowledge-intensive taskにおいては，CommonsenseQAをのぞき，全てpre-fusionを採用</p>\n<p>pre-fusionとpost-fusionの違いは何？</p>\n<p>pre-fusionは，知識を事前学習のパラメータに暗示的に保存数る</p>\n<p>最終的にどの知識がパラメータに保存するのかを決定するのは難しい</p>\n<p>知識の引き出しや利用の難しさが増す</p>\n<p>post-fusionは，明示的で具体的なテキストの知識を推論できる</p>\n<p>post-fusionの利点は，commonsense knolwedge-intensive taskでは欠点になりうる</p>\n<p>commonsenseはたいていテキストの中に暗示的に置かれていて，commonsenseの知識ソースのカバー範囲はencyclopedicの知識ソースのカバー範囲に比べればとても小さい</p>\n<p>large-scaleなcommonsenseのソースの利用がたとえ有用だとしても，日常生活で使われる大半のcommonsenseを見落としがちなまま</p>\n<p>→commonsenseにおいて，post-fusionがあまり効かないのはそのためなのでは？</p>\n<h2>Challenges and Future Directions</h2>\n<h3>Unified PLMKEs Across Tasks and Domains</h3>\n<p>task-specificなモデリングでは進展がある</p>\n<p>post-fusionとhybrid-fusionはencyclopedicで適用されているが，commonsenseでは採用できておらず恩恵が得られていない</p>\n<p>異なるタスク間でのPLMKEsはユニークであるため，各タスク間で互換性がない</p>\n<p>biomedicalやlegalの知識に関するknowledge-intensive NLP taskまで拡張されている</p>\n<p>最近では，異なる時間や地域に存在する知識の多様性に対しても重要度を割り当てている</p>\n<p>タスク間やdomain間でのunified PLMKEsの必要性がましている</p>\n<h3>Reliability of Kowledge Sources</h3>\n<p>知識ソースの信頼性に関して</p>\n<p>多くのlarge-scaleな知識ソースは自動的な知識獲得アルゴリズムで構築されている</p>\n<p>→スケールと正確性はトレードオフになってしまう</p>\n<p>PLMKEsにおけるバイアスの増幅はバイアスのある知識ソースによって構築されてしまう</p>\n<p>知識獲得アルゴリズムの見直しや使う前の知識ソースの注意深い精査が必要である</p>\n<h3>Reasoning Module Design</h3>\n<p>Reasoningはknowledge-intensive NLP taskを解く上で重要なステップである</p>\n<p>commonsenseを考えるときは手順を踏んで，複雑な状況を把握する</p>\n<p>e.g. </p>\n<p>まず，床が綺麗でないことを把握</p>\n<p>こぼした食べ物を踏んで他の人の靴が汚くなったのだろうと考える</p>\n<p>↑上記状況を踏まえて，モップをかける意図が生まれる</p>\n<p>人間のような日々の状況を認識する能力を獲得するには，multi-hopなreasoning moduleが必要になる（上の例みたいな形</p>\n<h2>引用</h2>\n<blockquote>\n</blockquote>","Title":"【論文まとめ】A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models","Date":"2023-05-21","Category":"論文","Tags":["survey","NLP","knowledge-base","PLMKE","commonsense","encyclopedic","Knowledge-Intensive NLP"],"Authos":"ゆうぼう","Slug":"A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models","Thumbnail":"/images/thumbnails/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models.png","Description":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Modelsのまとめ","Published":true},{"contentHtml":"<h2>コーパスとは</h2>\n<p>**コーパス(corpus)**とは、集めた文書のことをいいます。</p>\n<p>もともとの原義としては、ある主題とかある作者に関する文書を集めたものがコーパスと呼ばれていたそうです。</p>\n<p>現在はもう少し広義で捉えられ、<strong>文書や音声を集めたデータそのもの、あるいはデータに情報を付与して加工したもの</strong>を総称してコーパスというそうです。</p>\n<p>最近の自然言語処理のタスクの進展は、このコーパスに活用による部分が多いです。</p>\n<h2>生コーパス(raw corpus)</h2>\n<p>前のセクションで話したように、コーパスには加工を加えたものと、そのまま生データのものと二通り考えられました。</p>\n<p>そこで、生データのままの文章や音声を「<strong>生コーパス(raw corpus)</strong>」と呼ぶことができます。</p>\n<h2>翻訳に関するコーパスの分類</h2>\n<p>生コーパスの中でも、その種はいくつか存在します。\nこのトピックでは機械翻訳で扱われるようなコーパスの分類についてお話します。</p>\n<h2>#対訳コーパス／パラレルコーパス</h2>\n<p>まずは、「<strong>対訳コーパス(bilingual corpus)</strong>」または「<strong>パラレルコーパス(parallel corpus)</strong>」です。</p>\n<p>この対訳コーパスとは、翻訳関係にある2言語の文書対を収集した生コーパスになります。このコーパスは、非常に貴重ではありますが、なかなか入手しにくい希少なデータです。しかし、この対訳コーパスは機械翻訳においてとても重要な知識源となっていることは確かです。</p>\n<h2>#コンパラブルコーパス</h2>\n<p>対訳コーパスでは、希少なコーパスであったのに対して、  「<strong>コンパラブルコーパス(comparable corpus)</strong>」は、しっかりとした対訳関係にないにしても、同じトピックに関して2言語の文書対のコーパスです。</p>\n<p>コンパラブルコーパスは、対訳コーパスほどきっちりとした対訳が制約されないので、このような文書は大量に存在します。これらの文書を収集したものがコンパラブルコーパスです。</p>\n<p>例としてわかりやすいのは、Wikipedeaでしょう。  Wikipediaでは言語リンクでつながった複数の言語でのページが存在します。これらの文書対ではきっちりとした対訳は保証されませんが、大量のデータを入手することができ、これも極めて重要な知識源となります。</p>\n<h2>まとめ</h2>\n<p>以上がコーパスに関する簡単な説明でした。他にも均衡コーパス(balanced corpus)や注釈コーパス(annotated corpus)といった分類もあります。</p>\n<p>ここで抑えるべき重要なことは、コーパスとは広義で<strong>文書や音声を集めたデータそのもの、あるいはデータに情報を付与して加工したもの</strong>ということでしょう。</p>\n<p>今回は主にコーパスの説明とともに生コーパスについての説明をしていきました。実際に加工を加えたコーパスに関しては、また別の記事にしたいと思います!</p>","Title":"自然言語処理(NLP)のコーパスって何なん？","Date":"2020-07-11","Category":"ML","Tags":["ML","NLP"],"Authors":"ゆうぼう","Slug":"corpus","Thumbnail":"/images/thumbnails/network.jpg","Description":"自然言語処理という機械学習のタスクにおいて「コーパス」という言葉が出てきます。そのコーパスについてお話をしていきます。","Published":true}],"tag":"NLP","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","CSS","dialogue system","DST","empathetic dialogue system","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true}