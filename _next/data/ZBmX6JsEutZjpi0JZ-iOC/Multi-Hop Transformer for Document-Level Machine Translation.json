{"pageProps":{"postData":{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<h2>概要</h2>\n<p>Document-level neural machine translationにおいて，Multi-Hopなアーキテクチャを導入することにより，従来手法と比べて精度の高い文脈を考慮した機械翻訳を実現</p>\n<p>翻訳者のように，頭の中に翻訳のドラフトを作り，文脈に合わせて適切に修正する流れ（human-like draft-editing）を明示的にモデリング</p>\n<p>大きな事前学習済みモデルを使うことなく，使用に足る機械翻訳モデルを実現</p>\n<h2>提案手法</h2>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c2619e4c-99b6-4874-8d32-2f7a07bb54a3/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T122333Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=5c9267ed30e96772da34fc0006b6bccc8575aed66b0b80841a9a632fa0397598&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>アーキテクチャ周りのこと</p>\n<h3>Sentence Encoder</h3>\n<p>source-sideとtarget-sideでそれぞれPretrained Encoderがあり，source contextとtarget draftの分散表現をそれぞれ得る</p>\n<h3>Multi-Hop Encoder</h3>\n<p>source-contextにおいて文章ごとのreasoningをして，現在の文章の分散表現を得る</p>\n<h3>Multi-Hop Decoder</h3>\n<p>target-side draftから情報を取得して，翻訳の確率分布を得る</p>\n<p>そのほかアーキテクチャの工夫</p>\n<h3>Contet Gating</h3>\n<p>contextual informationを過剰にutilizeしすぎないように，context gating machanismを採用</p>\n<h2>新規性</h2>\n<p>Docment-level NMTにおける従来手法の問題点</p>\n<p>Document-level NMTにおいてMulti-Hop reasoningをモデリングしたMulti-Hop Transformerの提案と提案モデルによるDocument-level NMTの大きな性能改善</p>\n<p>target contextにground-truthで訓練すると推論時にはアクセスできないため，他の翻訳モデルの翻訳結果を使用することで，訓練時と推論時の状況を同じにした</p>\n<h2>実験</h2>\n<p>Baseline</p>\n<p>計算量のオーバーヘッドを改善するためSentence Encoderはそれぞれのsideでパラメータを共有</p>\n<h2>まとめ</h2>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e9aa2659-c723-4d78-b619-a2fca604a864/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.26.30.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T122343Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=e622e907b6dec0e51b93e156491ae8aa101d2537fce522b248107d0c6a0ccf8e&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>large-scaleな事前学習済み言語モデルを使用することなく，SoTA翻訳クオリティを達成</p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/648e061a-037a-47b0-9ccd-cb5e34f0584a/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.28.39.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T122344Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=c82ac65b2feebf537235d8a9fd5f55ef487bd7c09206b4cd6d785edd5ca9d6d1&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>contextを付与するためのAttentionの構造は，ConcatやHierarchicalよりもMulti-HopなAttentionが効果があり</p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/7b8e8b86-406c-4fca-ac8d-f024c3d62def/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.30.06.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T122345Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=f433db1343231a7f7a7b89abdf399c2ac81b6d04d017ac97da791ce3398d7502&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>contextを考慮する幅のwindow sizeは大きくするほど効果が上がるわけではなく．3が最も良かった</p>\n<p>4以上にすると悪化傾向らしく，本研究ではwindow size = 3 を採用</p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/38aeab8f-9c30-446e-b0a5-c01920ef5946/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.36.58.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T122348Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=da1fd42102c2946bb64f03e98c69ca4b5d9f669904822636962b1f9a1a3d13d2&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>contextにおいてreasoningするときの方向は，一般的な読み順の通りleft-to-rightで順方向にreasoningさせた方が結果は良かった</p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4f9ed4db-e0ac-48b0-9f5d-44af8ae2d2c5/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_14.38.40.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T122348Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=2c1022fab370762e293df5855cf3cfcc4f3b9f2f338600106afd7de4931804ac&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>訓練時と推論時にtarget draftに与える文章が異なる問題への対処に関する実験結果</p>\n<p>Referenceはground-truthをtarget draftとして与えて訓練，Draftはpre-trained MT systemが生成した翻訳結果をtarget draftとして与えて訓練したモデル</p>\n<p>Draftの方が結果がよく，pre-trained MT systemの生成結果をtarget draftとする方法によって訓練時と推論時のギャップの橋渡しになることを示唆する結果</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<p><a href=\"/5955ca444629476ebf23e66629a2413f\">Context-Aware Self-Attention Networks</a></p>\n</body>\n</html>\n","Title":"Multi-Hop Transformer for Document-Level Machine Translation","Date":"2023-05-21","Category":"論文","Tags":"MT,transformer,Multi-Hop Transformer","Authos":"ゆうぼう","Slug":"Multi-Hop-Transformer for Document-Level Machine Translation","Thumbnail":"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/31894441-2dc1-4741-aa95-3d3a1d9b6411/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-08-23_13.58.23.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230521T122329Z&X-Amz-Expires=3600&X-Amz-Signature=004c5b680a1a04cda7e6e1ae7f953ce5a65b1186f9daf786cc0aefecb246663e&X-Amz-SignedHeaders=host&x-id=GetObject","Description":"Multi-Hop Transformer for Document-Level Machine Translationのまとめ","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","conda","CSS","ESPNet","ffmpeg","Flask","Go","Google Colaboratory","Heroku","HTML","JavaScript","JSON","Kaggle","Linux","Mac","make","map","MeCab","ML","MT,transformer,Multi-Hop Transformer","MySQL","NLP","Node","node.js","npm","Pandas","Poetry","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","SISR","subprocess","Super-Resolution","tensorflow","Tkinter","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"]},"__N_SSG":true}