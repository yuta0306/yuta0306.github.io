{"pageProps":{"postData":{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: 対話システムはどのように話すべきか</p>\n<p>研究会: 日本音響学会誌</p>\n<p>年度: 2022</p>\n<p>キーワード: 聞き手反応, dialogue system</p>\n<p>URL: <a href=\"https://www.jstage.jst.go.jp/article/jasj/78/5/78_283/_pdf\">https://www.jstage.jst.go.jp/article/jasj/78/5/78_283/_pdf</a></p>\n<p>DOI: <a href=\"https://doi.org/10.20697/jasj.78.5_283\">https://doi.org/10.20697/jasj.78.5_283</a></p>\n<p>データセット:</p>\n<h2>概要</h2>\n<p>「<strong>聞き手反応</strong>」をキーワードに，対話システムが発する音声をどのように生成するべきかという観点から音声対話システムとの自然な対話の実現に迫った研究の紹介．</p>\n<h2>聞き手反応</h2>\n<p>会話は話し手と聞き手の相互行為により産出されるもの</p>\n<p>聞き手の反応を「相槌表現」として次の分類（Den et al., 2011）</p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/8io2tuxq.png\" alt=\"\"></p>\n<p><strong>聞き手の行動の観察は分析上不可欠</strong>であるが，音声対話システム研究ではほとんど考慮されない</p>\n<p><strong>人はシステムに対して，（人同士の対話と異なり）ほとんど聞き手反応をしない</strong></p>\n<p>人は，どのようにして声でシステムを操作するかを学ぶ</p>\n<p>→ 機械は機械であり，人と同じようにコミュニケーションができないことを知っている</p>\n<h2>聞き手反応を誘発する話し方</h2>\n<p>音声対話システム vs 人における違い</p>\n<ul>\n<li>発話タイミングを含むターンテイキング</li>\n<li><strong>話し方</strong>\n<ul>\n<li><strong>人同士の対話における発話は，必ずしも明瞭かつ流暢ではない．</strong></li>\n</ul>\n</li>\n</ul>\n<p>高津ら（2018）の研究では，人同士の対話にのみ存在する話し方の特徴を要因に分解し，次のコードで表す．</p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/bjl0al4r.png\" alt=\"\"></p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/omr9dl0c.png\" alt=\"\"></p>\n<p>そして，以下の3条件でWoZ対話的なことを行う</p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/fhyb4a69.png\" alt=\"\"></p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/4plgxweb.png\" alt=\"\"></p>\n<p>発話計画例と実験の結果は次のとおり</p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/0k7tzm7m.png\" alt=\"\"></p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/vryomqfg.png\" alt=\"\"></p>\n<p><strong>機械的な発話をすると，聞き手反応が減る</strong></p>\n<h2>自発音声コーパスを元にした音声で話すエージェント（飯塚ら, 2019）</h2>\n<p>読み上げ音声コーパスではなく，自然な会話コーパスを用いて音声合成を訓練することで，読み上げと会話の音声のギャップを埋めうる</p>\n<p>しかし，そのような試みは少ない</p>\n<p>以下が考えうる要因</p>\n<ol>\n<li>声のプロと違い，普通の人々の音声は正確性，明瞭性に欠けるから，音声合成に相応しくないという思い込み</li>\n<li>会話音声の多様性．テキストだけでは説明できない音声のパラ言語的な特徴の変動が読み上げ音声に比べて大きく，モデル化が相対的に困難なこと</li>\n<li><strong>会話コーパスの小ささ．音声合成ではデータりょうが品質に直結するが，会話コーパスの場合，話者一人当たりのデータ量が少なく，音声のモデル化が困難なこと</strong>\n<ol>\n<li>筆者ら研究チームでは，とりわけこれがネックだった</li>\n<li>→ ニューラルボコーダ（高木, 2019）の登場によりインパクトに</li>\n</ol>\n</li>\n</ol>\n<p>実験として，読み上げ独話音声コーパスJSUT vs 自発対話音声コーパスUUDB（宇都宮大学パラ言語情報研究向け音声対話データベース）</p>\n<p>音声合成にはTacotron 2，スペクトルからの波形生成にはMelGAN</p>\n<p>UUDBでは女性話者1名のデータでfine-tuning</p>\n<p>MMDAgentを利用して音声対話システムを構築（研究仮説を検証するため）</p>\n<p>国当てクイズの対話シナリオ</p>\n<p>実験結果とその対話動画視聴から得たアンケート結果</p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/9m8j5vxi.png\" alt=\"\"></p>\n<p><img src=\"/images/article/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AF%E3%81%A9%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E8%A9%B1%E3%81%99%E3%81%B9%E3%81%8D%E3%81%8B/qfx0mds4.png\" alt=\"\"></p>\n<p><strong>自然な会話コーパス（ここではUUDB）を元にした合成音声で話すシステムとの対話の方が，人同士の対話に近い振る舞いをしている．</strong></p>\n<p>感情表出系感動詞及び笑いの数に有意な差は認められず</p>\n<p>「不気味の谷」？？？</p>\n<h2>まとめ</h2>\n<p>自然な音声対話の実現を望むならば，もっと本物の音声コミュニケーションと真剣に向き合わなければならないのではないか？</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n<p>@article{2022,\nauthor = {森 大毅},\ndoi = {10.20697/jasj.78.5_283},\njournal = {日本音響学会誌},\nnumber = {5},\npages = {283-288},\ntitle = {対話システムはどのように話すべきか},\nvolume = {78},\nyear = {2022},\nbdsk-url-1 = {<a href=\"https://doi.org/10.20697/jasj.78.5_283\">https://doi.org/10.20697/jasj.78.5_283</a>}}</p>\n</blockquote>","Title":"【論文まとめ】対話システムはどのように話すべきか","Date":"2023-07-22","Category":"論文","Tags":["聞き手反応","dialogue system"],"Authos":"ゆうぼう","Slug":"対話システムはどのように話すべきか","Description":"対話システムはどのように話すべきかのまとめ","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"]},"__N_SSG":true}