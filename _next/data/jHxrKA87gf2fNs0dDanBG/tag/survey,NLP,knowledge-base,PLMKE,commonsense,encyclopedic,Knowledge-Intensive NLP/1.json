{"pageProps":{"TaggedPostData":[{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p>まとめること</p>\n<ol>\n<li>Knowledge-Intensive NLPの概要</li>\n<li>Knowledge Sources</li>\n<li>Encyclopedic Knowledge</li>\n<li>Commonsense Knowledge</li>\n<li>最近のKnowledge Sourcesの特徴</li>\n<li>Knowledge-Intensive NLP Task</li>\n<li>Knowledge-Intensive NLP Taskの概要</li>\n<li>Knowledge-Intensive NLP Taskの特徴</li>\n<li>Knowledge Fusion Methodsについて</li>\n<li>Pre-Fusion Methods</li>\n<li>Post-Fusion Methods</li>\n<li>Hybrid-Fusion Methods</li>\n<li>代表的なモデルの紹介</li>\n<li>Challengingなことと今後の方向性</li>\n<li>Unified PLMKEs Across Tasks and Domains</li>\n<li>Reliability of Knowledge Sources</li>\n<li>Reasoning Module Design</li>\n</ol>\n<h2>概要</h2>\n<p>事前学習済みモデルにより，モデルのcapacityは増加傾向にあるが，encyclopedicやcommonsenseを用いた，knowledgeableなNLPモデルの需要の高まりが生じている</p>\n<p>**PLMKEs (Pre-trained Language Model-based Knowledge-Enhanced models)**についてまとめたsurvey論文</p>\n<p>linguistic or factual knowledgeは暗示的にモデルのパラメータに保存される</p>\n<p>→事前学習済みのNLPモデルがより汎用的な能力を持つことを一部ではあるが説明できる</p>\n<p>今のpre-trained LMは，明示的なencyclopedicやcommonsenseのレバレッジ能力に欠けている</p>\n<p>PLMKEsは，関連する外部知識を取り出すモジュールと知識を混ぜるモジュールがある</p>\n<p>PLMKEsに関連した重要な3つの要素がある</p>\n<ol>\n<li>Knowledge Sources</li>\n<li>Knowledge-Intensive NLP Tasks</li>\n<li>Knowledge Fusion Methods</li>\n</ol>\n<h2>Knowledge Sources</h2>\n<h3>Encyclopedic knowledge</h3>\n<p>エンティティに関する属性とエンティティ間の関係性をもった知識</p>\n<p>Entity: person → Attributes: age → Relations: educated at</p>\n<p>Wikipediaは大量のencyclopedicな知識を持っている</p>\n<p>人物の経歴やイベントの背景などを含んでいる</p>\n<p>一般的にはtripletsで構成されていることが多い</p>\n<p>e.g. &#x3C;Tom Hanks, occupation, actor></p>\n<p>Wikidataのような知識データがPLMKEsに広く使用されている</p>\n<h3>Commonsense Knowledge</h3>\n<p>日常生活のなかでの状況に関する知識</p>\n<p>イベントとその影響を記す</p>\n<p>e.g. mop up the floor if we split food over it / study hard to win scholarship / goat has four legs</p>\n<p>commonsenseの特徴</p>\n<p>多くの人の間で共有されている知識であり，コミュニケーションの中で暗示的に想定されている知識である</p>\n<p>commonsenseもtripletsで表現される</p>\n<p>最近のPLMKEsでは，ConceptNetとATOMICが外部知識として使用されることが多い</p>\n<h3>Knowledge Sourcesの特徴</h3>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/30b21e96-66e9-4c93-b612-2a5ec35b43c9/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-26_16.42.16.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T181930Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=a08e0cc46127ef5b006be5eca7349c3b4f203325cb4dd1c39cb9757131c24c8e&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>large-scaleでdiverse</p>\n<p>現在のソースはより正確で安定的に作られている</p>\n<p>アノテーションのプロセスは部分的に自動化されていて，非エキスパートにもaccessibleになっている</p>\n<p>知識データがカバーするドメインは多様</p>\n<p>オープンドメインのものもあれば，specificなドメインのものも</p>\n<p>Wikipedia, DBPedia, Freebaseなどはオープンドメイン</p>\n<p>UMLSやAMinerなどはbiomedicineやscienceの特定ドメイン</p>\n<p>domain-specificなアプリケーションをブーストできる知識</p>\n<p>commonsenseに関しては</p>\n<p>ConceptNetやTransOMCSは複数のドメインのcommonsenseをカバー</p>\n<p>ATOMICやASERはある特定のタイプのcommonsenseにフォーカスした知識ソース</p>\n<h2>Knowledge-Intensive NLP Task</h2>\n<h3>概要</h3>\n<p>Knowledge-intensive NLP taskは必要とする知識ソースの種類で2つに分けられ，さらに詳細に分けることができる</p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ab23c756-9f8e-4f3c-a76c-cdd674276022/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-26_17.02.12.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T181947Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=b76560af21a3c08235665cc245c16fb63334660f36a2530535b06fa485395991&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<ul>\n<li>encyclopedic knowledge-intensive NLP task</li>\n</ul>\n<p>encyclopedicの知識ソースを利用する</p>\n<ul>\n<li>open-domain QA</li>\n<li>fact verification</li>\n<li>entity linking</li>\n</ul>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/28ba511c-8fa2-4031-9624-7a7460f25913/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-26_17.02.31.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T181952Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=b0c31adc0b4a7f7ea85b0fe08456ba8559910988866d93f039288cbd4421f3a6&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<ul>\n<li>commonsense knowledge-intensive NLP task</li>\n</ul>\n<p>commonsenseの知識ソースを利用する</p>\n<p>commonsenseの多様性のために，タスクのタイプ自体も多様化している</p>\n<p>モデルが正確に日常のシナリオを理解し，応答するか否かのテストにフォーカスしたタスク</p>\n<ul>\n<li>General Commonsense</li>\n<li>Social Commonsense</li>\n<li>Physical Commonsense</li>\n<li>Temporal Commonsense</li>\n</ul>\n<h3>Knowledge-Intensive Taskの特徴</h3>\n<p>実際は，モデルにとってだけではなく，人間にとってもいかなる知識の参照なしに問題に答えるのは難しい．（バラクオバマの誕生日はいつ？など</p>\n<p>しかも，外部知識が必要なのにinputとして必要な外部知識が渡されないため，とてもチャレンジングなタスクになっている</p>\n<p>そもそも必要な外部知識にグラウンディングするモジュールをPLMKEsの設計に加えることを考慮するようになっている</p>\n<h2>Knowledge Fusion Methods</h2>\n<p>モデルが知識を統合するステージは二箇所あり，</p>\n<ul>\n<li>Pre-fusion; pre-training</li>\n<li>Post-fusion; fine-tuning</li>\n</ul>\n<p>の二通りが考えられる（もしくはその両方のステージ</p>\n<h3>Pre-Fusion Methods</h3>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ddeea37a-3062-47f7-b3ac-15057b8a1349/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-26_17.11.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T182017Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=914d261462ed9d88bc35c4edfa5d080b2ffe29c48a555d3ffe8d9407c9fc70a8&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>pre-trainingのステージで知識を統合する手法</p>\n<p>モデルに知識を入力するため，構造化された知識データを非構造化データのテキストコーパスへと処理→モデルに入力</p>\n<p>テキストデータとして知識を入力するため，大きくモデルのアーキテクチャを変更する必要はない</p>\n<p>ただし，知識グラフのような構造化データを非構造化データへ変えることは難しいこともある</p>\n<p>簡単な対処法はエンティティと関係性を結合するか，流暢な文章をconditional text generation modelに生成させるか</p>\n<p>Zhang et al. 2019 | Agarwal et al. 2021 を参照（必要になれば読む</p>\n<h3>Post-Fusion Methods</h3>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a978299c-4851-41c8-a702-a3aafb0f9323/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-26_17.37.53.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T182027Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=e4e8f6a6703612a8f1118eb67304d93279f45d48fff78fd6f0b2c2d8b2b5a063&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>まず，関連知識をキャプチャする</p>\n<p>次に取得した関連知識をGNNなどのエンコーダでembeddingを得る</p>\n<ul>\n<li>それを追加特徴量としてpre-trained LMに与える（図でいうA）</li>\n<li>直接pre-trained LMに入力する（図でいうB）</li>\n</ul>\n<h3>Hybrid-Fusion Methods</h3>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b696904a-be2b-4bd8-9d93-ca87a47ecc5e/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-26_19.38.22.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T182031Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=8c309aba75c92d449b843fead794f87d2bd0340923faeef77c381a33ab5eb82e&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>pre-trainingとfine-tuningの両方のステージで知識を統合する</p>\n<p>追加の学習されるretrieverによりaugmentされたpre-trained modelは，fine-tuningのステージでより効果的にretrieverからの知識を活用できる</p>\n<p>retrieval-augmented pre-trainingでhybrid-fusionが広く使われている</p>\n<h3>代表的なモデル</h3>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ce1f044a-ba15-4c30-8d61-eab525800d9d/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-26_19.39.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T182039Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=2bfc9853f1a4e6bff019baa7c3effe98a1c01a806e603f05131576021c53bf3e&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/81d13c12-2379-4709-bc00-97fd4185fd4b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-31_12.00.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T182039Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=3f15cc31ed578332b99f8185529710bceec9e12123b9b990b31a909b1da89850&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>Table4/5はSOTAモデルを示す</p>\n<p>encyclopedic knowledge-intensive taskにおいては，BOOLQをのぞき，全てpost-fusionを採用</p>\n<p>commonsense knowledge-intensive taskにおいては，CommonsenseQAをのぞき，全てpre-fusionを採用</p>\n<p>pre-fusionとpost-fusionの違いは何？</p>\n<p>pre-fusionは，知識を事前学習のパラメータに暗示的に保存数る</p>\n<p>最終的にどの知識がパラメータに保存するのかを決定するのは難しい</p>\n<p>知識の引き出しや利用の難しさが増す</p>\n</body>\n</html>\n","Title":"【論文まとめ】A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models","Date":"2023-05-21","Category":"論文","Tags":"survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP","Authos":"ゆうぼう","Slug":"A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models","Thumbnail":"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/72a3a885-7a2d-4363-ba6b-3371efd274e7/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-03-22_17.49.18.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230521T181845Z&X-Amz-Expires=3600&X-Amz-Signature=4f2395f9c9ae855d53990b80015353bf52b3381b31b93d31fe5fd92cdd60b7a5&X-Amz-SignedHeaders=host&x-id=GetObject","Description":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Modelsのまとめ","Published":true}],"tag":"survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET,mental health,NLP,mental state knowledge,mentalisation,Contrasive Learning,MentalRoBERTa,KC-Net","conda","CSS","dialogue system","dialogue system,Internet-Augmented","dialogue system,knowledge-base","dialogue system,NLI","dialogue system,persona,Prompt-Tuning","dialogue system,survey,DST","ESPNet","ffmpeg","Flask","Go","Google Colaboratory","Heroku","HTML","humor detection,multi-modal","JavaScript","JSON","Kaggle","laughter,shared laughter","Linux","Mac","make","map","MeCab","ML","MT,transformer,Multi-Hop Transformer","MySQL","NLP","Node","node.js","npm","Pandas","Poetry","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","SISR","subprocess","Super-Resolution","survey,dialogue system","survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP","tensorflow","Tkinter","transformer,Highway Transformer,Gating Mechanism,Self-Dependency-Units (SDU)","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true}