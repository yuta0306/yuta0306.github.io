{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</p>\n<p>研究会: EMNLP</p>\n<p>年度: 2023</p>\n<p>キーワード: LLM, multi-modal, Demo</p>\n<p>URL: <a href=\"https://arxiv.org/pdf/2306.02858.pdf\">https://arxiv.org/pdf/2306.02858.pdf</a></p>\n<p>DOI: <a href=\"https://doi.org/10.48550/arXiv.2306.02858\">https://doi.org/10.48550/arXiv.2306.02858</a></p>\n<p>コード: <a href=\"https://github.com/damo-nlp-sg/video-llama\">https://github.com/damo-nlp-sg/video-llama</a></p>\n<p>データセット: MSR-VTT, MSVD, VideoInstruct, ActivityNet-QA</p>\n<h2>概要</h2>\n<p>ビデオ中の動画と音声を理解できるVideo-LLaMAを提案</p>\n<p>Video Q-Former (BLIP2)とAudio Q-Former (Imagebind)を用いて，動画のシーン間の変化を捉えたり，audio-visualな情報を統合したりする</p>\n<p>Video-LLaMAは動画を理解して，ビデオ中の動画や音声に基づいた意味のある応答を生成できる</p>\n<p><a href=\"https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA\">デモ on huggingface</a></p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/sxpfrxf6.png\" alt=\"\"></p>\n<h3>Architecture</h3>\n<p>図の通り，Vision-Language BranchとAudio-Language Branchに分岐</p>\n<p><strong>Vision-Language Branch</strong></p>\n<ol>\n<li>各フレームをフリーズしたBLIP2に入力（EVA-CLIPのViT-G/14とpre-trained Q-Former）</li>\n<li>positional embeddingを適用</li>\n<li>Video Q-Former</li>\n<li>線形層</li>\n<li>LLMへ</li>\n</ol>\n<p><strong>Audio-Language Branch</strong></p>\n<ol>\n<li>2秒ごとに音声をクリップ</li>\n<li>各クリップ音声を128 binsのメルスペクトログラムに変換</li>\n<li>Imagebind (as Audio Encoder)</li>\n<li>Imagebindの出力に対して，learnableなpositional embeddingを加算</li>\n<li>Audio Q-former</li>\n<li>線形層</li>\n<li>LLMへ</li>\n</ol>\n<h3>Multi-branch Cross-modal Training</h3>\n<p><strong>Vision-Language Branchの学習</strong></p>\n<p>事前学習→インストラクションチューニング</p>\n<p>事前学習データセット：Webvid-2M，CC595k（CC3Mからフィルタされたもの）</p>\n<p>インストラクションデータセット：MIniGPT4，LLaVA，Video-Chat</p>\n<p>インストラクションチューニングすると，Video-LLaMAは良い能力を発揮</p>\n<p><strong>Audio-Language Branchの学習</strong></p>\n<p>audio-textなデータが少ないことが課題</p>\n<p>→<strong>異なるモダリティを同じ埋め込み空間にalignmentするImagebindをAudio Encoderとして用い，visual-textデータを使って学習</strong></p>\n<p>音声データで学習しないが，推論時は音声を理解することができる</p>\n<h2>新規性</h2>\n<ul>\n<li>与えられたビデオの動画と音声を同時に処理して，会話ができるVideo-LLaMAを提案</li>\n<li>vision-language alignmentとaudio-language alignmentの両方を達成するmulti-branch cross-modal pre-training frameworkを提案</li>\n</ul>\n<h2>Examples</h2>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/4pkwtgeu.png\" alt=\"\"></p>\n<p>動画と音声の両方を理解できている例</p>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/ke9lg4al.png\" alt=\"\"></p>\n<p>Temporal dynamicsを理解できている例</p>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/dkti6vzg.png\" alt=\"\"></p>\n<p>staticな画像を理解できている例</p>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/55eess6q.png\" alt=\"\"></p>\n<p>一般的な知識を示せている例</p>\n<p>chat形式の例は論文のappendixを参照</p>\n<h2>まとめ</h2>\n<p>動画と音声を理解できるVideo-LLaMAを提案</p>\n<p>Vision-Language BranchとAudio-Language Branchで分岐して，動画と音声を理解するアーキテクチャを提案し，Imagebindをvisual-textデータで学習することでaudio-textデータの少なさをカバー</p>\n<p>Hallcinationがあることや，映画やテレビのような長い動画を処理できないことがlimitaitions</p>\n<h2>その他</h2>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/tq9elvwf.png\" alt=\"\"></p>\n<p>ポピュラーなマルチモーダルLLM</p>\n<p><strong>所感</strong></p>\n<p>Imagebindで同じ埋め込み空間に異なるモダリティの埋め込みを押し込んでいるのを利用して，audio-textデータの少なさをカバーしているのが上手いのだろうが，それでうまくいくことにちょっと気持ち悪さが残った（個人的に，大規模なaudio-textデータ構築へのモチベがより大きくなるなど）</p>\n<p>素人感想だと，Audio-Language BranchにImagebindを使うのなら，Vision-Lannguage BranchもImagebindで良いのでは？と思った</p>\n<p>とはいえ，temporalな情報をLLMで扱う手法はかなり参考になる</p>\n<h2>次読みたい論文</h2>\n<p><strong><a href=\"https://arxiv.org/abs/2305.05665\">ImageBind: One Embedding Space To Bind Them All</a></strong></p>\n<p><strong><a href=\"https://arxiv.org/abs/2304.12995\">AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</a></strong></p>\n<h2>引用</h2>\n<blockquote>\n<p>@article{zhang2023video,\ntitle={Video-llama: An instruction-tuned audio-visual language model for video understanding},\nauthor={Zhang, Hang and Li, Xin and Bing, Lidong},\njournal={arXiv preprint arXiv:2306.02858},\nyear={2023}\n}</p>\n</blockquote>","Title":"【論文まとめ】Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","Date":"2023-11-02","Category":"論文","Tags":["LLM","multi-modal","Demo"],"Authos":"ゆうぼう","Slug":"Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding","Thumbnail":"/images/thumbnails/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding.png","Description":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understandingのまとめ","Published":true}],"tag":"LLM","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"],"pages":1,"page":1},"__N_SSG":true}