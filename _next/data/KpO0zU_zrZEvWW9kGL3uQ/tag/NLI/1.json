{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: 分類モデルBERTによる不整合生成文の検出について</p>\n<p>研究会: NLP</p>\n<p>年度: 2022</p>\n<p>キーワード: dialogue system, NLI</p>\n<p>URL: <a href=\"https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-4.pdf\">https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B2-4.pdf</a></p>\n<p>データセット: 日本語SNLI</p>\n<h2>概要</h2>\n<p>ニューラル文章生成において，文章としては自然だが，内容が事実とは異なる**事実不整合（factual inconsistency）**が問題</p>\n<p>→　BERTを用いて分類タスクをすることで生成文の事実不整合の検出を試みる</p>\n<p>疑似データセットを作成し学習することで，不整合検出におけるドメイン適応の重要性を明らかにした</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/%E5%88%86%E9%A1%9E%E3%83%A2%E3%83%87%E3%83%ABBERT%E3%81%AB%E3%82%88%E3%82%8B%E4%B8%8D%E6%95%B4%E5%90%88%E7%94%9F%E6%88%90%E6%96%87%E3%81%AE%E6%A4%9C%E5%87%BA%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/l81q3d7a.png\" alt=\"\"></p>\n<p><img src=\"/images/article/%E5%88%86%E9%A1%9E%E3%83%A2%E3%83%87%E3%83%ABBERT%E3%81%AB%E3%82%88%E3%82%8B%E4%B8%8D%E6%95%B4%E5%90%88%E7%94%9F%E6%88%90%E6%96%87%E3%81%AE%E6%A4%9C%E5%87%BA%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/gnt0fefy.png\" alt=\"\"></p>\n<p>特に数値データに対してロバストなモデルになるよう学習するため，知識に数値が含まれる「料金情報」「アクセス情報」「営業時間情報」の3カテゴリに絞って学習に用いる</p>\n<p>疑似例の作成は数値や日付，駅名等を書き換えることで対応</p>\n<p>料金，アクセス，営業時間情報で書き以下絵対象がお’異なるため，それぞれ小cleanなる改変方法でデータを書き換え</p>\n<h2>新規性</h2>\n<p>旅行ドメインに対して疑似データセットを作成し，それを用いて学習することで，SNLIデータセットを用いた学習に比べて，事実不整合の生成文の検出精度を向上</p>\n<h2>実験</h2>\n<p>データセット</p>\n<p>事実整合性判定学習データセット</p>\n<p>料金，アクセス，営業時間情報について作成した疑似生後売れ，不整合例を集めたデータセット</p>\n<p>ニューラル生成文データセット</p>\n<p>NTT製TransformerのHobbyistを用いて生成した文章を含むデータセット</p>\n<p>Laboro社製BERTをファインチューニング</p>\n<p>ベースラインデータセットとして，日本語SNLIデータセット</p>\n<p>recallが最良のエポックの重みを最良モデルとして評価</p>\n<p>recallが低いモデルは大量の不整合を見逃していることになるため，目的を果たしていないと考えたから</p>\n<h2>まとめ</h2>\n<p>提案手法（疑似例を用いたデータセット）は事実不整合検出に有効である</p>\n<p>正解できなかった不整合例の内訳</p>\n<p>料金7件／アクセス1件／営業時間16件</p>\n<p>→　テンプレートの拡充が必要か？</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n</blockquote>","Title":"【論文まとめ】分類モデルBERTによる不整合生成文の検出について","Date":"2023-05-21","Category":"論文","Tags":["dialogue system","NLI"],"Authos":"ゆうぼう","Slug":"分類モデルBERTによる不整合生成文の検出について","Thumbnail":"/images/thumbnails/分類モデルBERTによる不整合生成文の検出について.png","Description":"分類モデルBERTによる不整合生成文の検出についてのまとめ","Published":true}],"tag":"NLI","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"],"pages":1,"page":1},"__N_SSG":true}