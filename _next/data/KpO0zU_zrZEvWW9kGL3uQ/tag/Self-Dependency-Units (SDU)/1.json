{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</p>\n<p>研究会: ACL</p>\n<p>年度: 2020</p>\n<p>キーワード: transformer, Highway Transformer, Gating Mechanism, Self-Dependency-Units (SDU)</p>\n<p>URL: <a href=\"https://aclanthology.org/2020.acl-main.616.pdf\">https://aclanthology.org/2020.acl-main.616.pdf</a></p>\n<p>DOI: <a href=\"http://dx.doi.org/10.18653/v1/2020.acl-main.616\">http://dx.doi.org/10.18653/v1/2020.acl-main.616</a></p>\n<p>コード: <a href=\"https://github.com/cyk1337/Highway-Transformer\">https://github.com/cyk1337/Highway-Transformer</a></p>\n<p>データセット: Penn Tree Bank (PTB), enwik8</p>\n<h2>概要</h2>\n<p>LSTM-styleなSDUを提案</p>\n<p>ゲートとしてSDUをTransformer内部に適用することにより，ハイパラをチューニングすることなく，Transformerの浅い層において，内在的な意味の重要性を捉え，より早い収束を可能に</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/qrpajdel.png\" alt=\"\"></p>\n<h3>Self-Dependency Units (SDU)</h3>\n<p>sigmoid gatesを導入する</p>\n<p><span class=\"math math-inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.76ex\" height=\"1.545ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 778 683\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><path id=\"MJX-1-TEX-N-3A8\" d=\"M340 622Q338 623 335 625T331 629T325 631T314 634T298 635T274 636T239 637H212V683H224Q248 680 389 680T554 683H566V637H539Q479 637 464 635T439 622L438 407Q438 192 439 192Q443 193 449 195T474 207T507 232T536 276T557 344Q560 365 562 417T573 493Q587 536 620 544Q627 546 671 546H715L722 540V515Q714 509 708 509Q680 505 671 476T658 392T644 307Q599 177 451 153L438 151V106L439 61Q446 54 451 52T476 48T539 46H566V0H554Q530 3 389 3T224 0H212V46H239Q259 46 273 46T298 47T314 48T325 51T331 54T335 57T340 61V151Q126 178 117 406Q115 503 69 509Q55 509 55 526Q55 541 59 543T86 546H107H120Q150 546 161 543T184 528Q198 514 204 493Q212 472 213 420T226 316T272 230Q287 216 303 207T330 194L339 192Q340 192 340 407V622Z\"></path></defs><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><use data-c=\"3A8\" xlink:href=\"#MJX-1-TEX-N-3A8\"></use></g></g></g></svg></mjx-container></span>はゲートとして作用し，logistic sigmoidもしくはtanhで実現</p>\n<p>筆者らの認識</p>\n<p>tanhはupdate gateとして作用し，重要度の幅を-1 to 1に制限</p>\n<p>sigmoidはLSTMのinput gateと似ていて，feature-wise levelでどれくらいの情報を残すか決定</p>\n<h3>Pseudo-highway Connection</h3>\n<p>residual connectionされたgating-modified encodingsでMulti Head Dot Product Attention (MHDPA)の分散表現を豊かにするため，新たな計算グラフの枝を追加し，SDUとIdentityとMHDPAをpost LNを使用してresidual connectionする</p>\n<h2>新規性</h2>\n<p>本来，人にとって，読み物をよりよく理解するためには，global contextだけではなく，ここの単語の意味も必要</p>\n<p>→ Self-gatingなアプローチを提案</p>\n<ol>\n<li>Transformerにおける浅い層において，trainingとvalidationでハイパラチューニングすることなく，より高速な収束を達成</li>\n<li>Transformerでの低レイヤーにおいて，local-range encodingにフォーカスした層を実現</li>\n<li>Self-gating mechanismは，R-TransformerやTransformer-XLのコンポーネントとしてRNN-likeなメカニズムを補完</li>\n</ol>\n<h2>実験</h2>\n<p>SDUを導入し，PTBデータセットにおけるSDUの効果を検証</p>\n<p>sigmoidとtanhを実験</p>\n<h2>まとめ</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/op874d4u.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/e99q8luh.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4xk5l3fv.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/gen0ole9.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/bjrucnwe.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4fv3x3v3.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/c6hfqjx9.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/aptzk9jc.png\" alt=\"\"></p>\n<p>sigmoidによるSDUが安定しているが，データとタスクによってはtanhの方がoutperformすることがある</p>\n<p>いずれのactivationを使っても収束は早い</p>\n<p>enwik8による大規模データでの追実験において，提案手法が浅いレイヤーには寄与することが確かめられた</p>\n<p>SDUで計算量が増えるが，そこまで差はなかった</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n<p>@inproceedings{chai-etal-2020-highway,\ntitle = \"Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\",\nauthor = \"Chai, Yekun  and\nJin, Shuo  and\nHou, Xinwen\",\nbooktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\nmonth = jul,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"<a href=\"https://aclanthology.org/2020.acl-main.616\">https://aclanthology.org/2020.acl-main.616</a>\",\ndoi = \"10.18653/v1/2020.acl-main.616\",\npages = \"6887--6900\",\nabstract = \"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.\",\n}</p>\n</blockquote><style>\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] > svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] > svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] > g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] > rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] > line[data-line], svg[data-table] > g > line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] > g > svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool > mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool > mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n</style>","Title":"【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks","Date":"2023-05-21","Category":"論文","Tags":["transformer","Highway Transformer","Gating Mechanism","Self-Dependency-Units (SDU)"],"Authos":"ゆうぼう","Slug":"Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks","Thumbnail":"/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png","Description":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ","Published":true}],"tag":"Self-Dependency-Units (SDU)","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"],"pages":1,"page":1},"__N_SSG":true}