{"pageProps":{"TaggedPostData":[{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p>Data State Tracking (以下DST) on Task-Oriented Dialogue Systemに焦点を当てたsurvey</p>\n<h2>Abstract</h2>\n<p>触れること</p>\n<ul>\n<li>タスク</li>\n<li>データセット</li>\n<li>evaluation metrics</li>\n<li>アプローチ</li>\n</ul>\n<p>本論文では，二つのDSTモデルをしっかり区別する．</p>\n<ul>\n<li>static ontology DST models\n<ul>\n<li>固定された対話状況集合を予測する</li>\n</ul>\n</li>\n<li>dynamic ontology DST models\n<ul>\n<li>オントロジーが変化した時でも対話状況を予測する</li>\n</ul>\n</li>\n</ul>\n<p>Definition of ontology</p>\n<p>a set of concepts and categories in a subject area or domain that shows their properties and the relations between them.</p>\n<p>単一ドメインでも複数ドメインでもトラックすることや新しいドメインにスケーリングすることのモデルの性能について議論する</p>\n<p>Terms: knowledge transfer, zero-shot learning</p>\n<p>カバーしている年代は2013~2020</p>\n<h2>Introduction</h2>\n<p>Task-oriented dialogue system:</p>\n<p>ユーザーがタスクを成し遂げるようにするシステム</p>\n<p>チケット予約，レストラン予約，カスタマーサポートなど</p>\n<p>ユーザの要求を正確にトラッキングする性能は，一貫していて効果的な対話を可能にする</p>\n<p>対話状況をslot-valueで表現するDSTコンポーネントを使った情報をトラッキングする</p>\n<p>↑この精度がとても重要で，下流のコンポーネントがこの状況を利用して，次のactionを決定する</p>\n<p>DSTタスクは，実際Natural Language Understanding (以下NLU)のタスクを統合している</p>\n<p>ただし，単なるslot filling taskよりも複雑になっている</p>\n<p>DST</p>\n<p>現在のturnまで，対話レベルでslot-valueを予測</p>\n<p>Slot Filling</p>\n<p>特定のturnのみ考慮してslot-valueを予測すれば良い</p>\n<p>モデルとしては以下が提案されている</p>\n<p>RNN-based models</p>\n<p>Attention-based models</p>\n<p>Transformer-based models</p>\n<p>ここ最近では，単一ドメインではなく，マルチドメインやflexibleにドメインの移行をするモデリングの研究が盛んらしい</p>\n<h2>Dialogue State Tracking</h2>\n<p>そもそもDSTとは</p>\n<h3>Dialogue State</h3>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">S_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>: dialogue state</p>\n<p>→turn <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span> までにおける対話履歴のsummary</p>\n<p>次の行動を決定するための全ての十分な情報を含んでいる</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span>   : turn</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mi>l</mi><mi>o</mi><mi>t</mi><mo separator=\"true\">,</mo><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(slot, value)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span></span>: このペアで，ユーザの目的を捉える</p>\n<p>slotはOntology <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span></span></span></span></span> の中で事前に定義されていて (ドメイン依存であるが)，</p>\n<p>valueはユーザによって与えられた各スロット <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span></span> で決められる</p>\n<p>レストランの例で言えば</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mo stretchy=\"false\">{</mo><mo stretchy=\"false\">(</mo><mi>F</mi><mi>O</mi><mi>O</mi><mi>D</mi><mo separator=\"true\">,</mo><mi>I</mi><mi>T</mi><mi>A</mi><mi>L</mi><mi>I</mi><mi>A</mi><mi>N</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mo stretchy=\"false\">(</mo><mi>A</mi><mi>R</mi><mi>E</mi><mi>A</mi><mo separator=\"true\">,</mo><mi>C</mi><mi>E</mi><mi>N</mi><mi>T</mi><mi>R</mi><mi>E</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">s_t = \\{(FOOD, ITALIAN), (AREA, CENTRE)\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">FOO</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\">L</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">RE</span><span class=\"mord mathnormal\">A</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">CENTRE</span><span class=\"mclose\">)}</span></span></span></span></span>のようになる</p>\n<p>slotのタイプは二つ</p>\n<ol>\n<li>\n<p>informable\n対話から得られる→FOODやAREA</p>\n</li>\n<li>\n<p>requestable\nシステムが与える→ADRRESSやPHONE</p>\n</li>\n</ol>\n<h3>Dialogue State Tracker</h3>\n<ol>\n<li>\n<p>turn-level prediction\n各ターンで与えられるslot-valueを予測</p>\n</li>\n<li>\n<p>dialogue-level prediction\n各ターンでの完全な対話状況を予測</p>\n</li>\n</ol>\n<h3>Turn-level prediction</h3>\n<p>直近のturn <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span> からslot-valueを予測する</p>\n<p>rule-basedの場合は，そのルールに従って，<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{t-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span>に統合して<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>を得る</p>\n<p>turn <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span> を優先したり，</p>\n<p>確率を利用して統合したり</p>\n<p>learning to updateの場合は，turn-levelの予測を入力として，対話状況を予測する方法を学習する</p>\n<h3>Dialogue level prediction</h3>\n<p>各turn <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span> において，完全な対話履歴を入力として，完全な対話状況 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> を予測する</p>\n<p>直前の対話状況 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{t-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span> を考慮しないため，<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{t-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span>と<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>に一貫性がないこともある</p>\n<h2>Datasets</h2>\n<p><img src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/4stb497n.png\" alt=\"\"></p>\n<ul>\n<li>Dialog State Tracking Challenge (DSTC)</li>\n<li>DSTC2 and DSTC3</li>\n<li>WoZ2.0</li>\n<li>MultiWoZ</li>\n<li>Schema-Guided Dataset (SGD)</li>\n<li>TreeDST</li>\n<li>Machine-to-Machine (M2M)</li>\n</ul>\n<h2>Evaluation Metrics</h2>\n<ul>\n<li>Average Goal Accuracy</li>\n<li>Joint Goal Accuracy</li>\n<li>Requested  Slots F1</li>\n<li>Time Complexity</li>\n</ul>\n<h2>Static Ontology DST Models</h2>\n<p><img src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/672om8je.png\" alt=\"\"></p>\n<p>slot-valueは事前に定義されている</p>\n<p>→</p>\n<p>output layerは</p>\n<ul>\n<li>feed-forward layer\n- slotとvalueが固定なので，それらはembeddingされているため可能</li>\n<li>softmax\n- 全てのslot-valueのペアの確率を求める</li>\n<li>sigmoid\n- それぞれのslot-valueの確率を求める</li>\n</ul>\n<h3>Delexicalization</h3>\n<p>imbalanced training data for slot-valuesに対処する効果的なアプローチ</p>\n<p>入力のslot valuesをラベルの名前に置き換える</p>\n<p>I want Chinese food.</p>\n<p>→ I want F.VALUE F.SLOT.</p>\n<h3>Data-driven DST</h3>\n<p>delexicalizationは確かに効果的だが，手作業でのfeature engineeringが必要になる</p>\n<p>→ data-drivenな手法が提案された</p>\n<h3>Parameter sharing</h3>\n<p>昔のモデルはslotごとにエンコーダが分かれていた</p>\n<p>→そのため全てのslotに対してパラメータを共有する手法が提案された</p>\n<p>StateNet？</p>\n<h3>RNN and latency in DST</h3>\n<p>予測時間が問題だったため，それに対する対策の提案</p>\n<h3>Encoder based on pre-trained LM</h3>\n<p>BERTなどを使うことで，捕捉できるslot valueが増えた</p>\n<h2>Dynamic Ontology DST Models</h2>\n<p>オントロジーが事前定義されていなくてもslot-valueをトラッキングする必要がある</p>\n<p>アプローチは2種</p>\n<ol>\n<li>ユーザの入力からslot-valueをコピー</li>\n<li>outputにslot-valueを生成</li>\n</ol>\n<p>下図は2種のアプローチを合わせたアーキテクチャ</p>\n<p><img src=\"/images/article/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey/rdiiezxm.png\" alt=\"\"></p>\n<p>static ontology vs dynamic ontology</p>\n<p>staticだとvalueが有限だが，</p>\n<p>dynamicだとoutputの語彙数がとても大きくなる</p>\n<h3>Copy and pointer networks</h3>\n</body>\n</html>\n","Title":"【論文まとめ】Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey","Date":"2023-05-21","Category":"論文","Tags":["dialogue system","survey","DST"],"Authos":"ゆうぼう","Slug":"Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey","Thumbnail":"/images/thumbnails/Recent-Neural-Methods-on-Dialogue-State-Tracking-for-Task-Oriented-Dialogue-Systems-A-Survey.png","Description":"Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Surveyのまとめ","Published":true},{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/jjd6raay.png\" alt=\"\"></p>\n<h2>概要</h2>\n<p>対話システムに関するサーベイ論文</p>\n<p>対話システムはNLPタスクの一種</p>\n<p>研究の価値が高いNLPタスクを多く含むため，対話システムは複雑と言える．</p>\n<p>ここ最近で良い成果をあげているもののほとんどがDL</p>\n<p>メインは，モデルタイプとシステムタイプについて述べられる．</p>\n<p>システムタイプ</p>\n<p>タスク指向型</p>\n<p>オープンドメイン型</p>\n<h3>Keywords</h3>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/vcs36qfg.png\" alt=\"\"></p>\n<h3>サーベイの主張の流れ</h3>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/80r4nod6.png\" alt=\"\"></p>\n<h2>まとめ</h2>\n<h3>Introduction</h3>\n<p>対話システムはNLPにおいてホットな話題であり，産業においても需要が非常に高い．</p>\n<p>タスク指向型とオープンドメイン型の対話システムが存在する．</p>\n<p>昔ながらのタスク指向型は，Natural Language Understanding, Dialogue State Tracking, Policy Learning, Natural Language Generationの4つからなっていた</p>\n<p>⇒</p>\n<p>最近のSoTAモデルでは，E2Eのタスク指向型の対話システムが多い．</p>\n<p>オープンドメイン型</p>\n<ul>\n<li>generative systems\n<ul>\n<li>seq2seqなモデル</li>\n<li>ユーザのメッセージや対話履歴を返答系列にマッピングする(Trainingデータに存在しないであろうものも含む)</li>\n<li>柔軟でコンテクストを読んだ返答をするが，時々主張が一貫しない返答や鈍感で面白くない返答を返す．</li>\n</ul>\n</li>\n<li>retrieval-based systems (検索)\n<ul>\n<li>返答の集合の中から，すでに存在する適した返答を探す．</li>\n<li>表面上では良い返答をする．ただし，返答集合は有限集合なので，対話上のコンテクストに対しては関係性があまりみられないこともある．</li>\n</ul>\n</li>\n<li>ensemble systems\n<ul>\n<li>上記二つを含む</li>\n<li>Generatie systemsは検索システムをよくするために使われる．</li>\n<li>検索システムはより適した返答を選ぶために使われる．</li>\n</ul>\n</li>\n</ul>\n<p>古典的な対話システムとして，finite state-basedとstatistical learningとmachine learning-basedが挙げられる．</p>\n<ul>\n<li>Finite State-based\n<ul>\n<li>対話の流れはあらかじめ決められている</li>\n<li>決まったシナリオの中でしか対応ができない．</li>\n</ul>\n</li>\n<li>Statistical Learning-based\n<ul>\n<li>Finite State-basedよりは柔軟である．あらかじめ対応が決められていないから．</li>\n</ul>\n</li>\n<li>machine learning-based\n<ul>\n<li>Deep learningが主流？</li>\n</ul>\n</li>\n</ul>\n<p>NLPの中には対話システムに近い領域がある．</p>\n<ul>\n<li>Q &#x26; A</li>\n<li>reading comprehension</li>\n<li>dialogue disentanglement</li>\n<li>visual dialogue</li>\n<li>visual Q &#x26; A</li>\n<li>dialogue reasoning</li>\n<li>conversational semantic parsing</li>\n<li>dialogue relation extraction</li>\n<li>dialogue sentiment analysis</li>\n<li>hate speech detection</li>\n<li>MISC detection (???)</li>\n</ul>\n<h3>Neural Models in Dialogue Sustems</h3>\n<ul>\n<li>CNN\n<ul>\n<li>ここ数年NLPの分野での応用も多いらしい</li>\n<li>フレーズや文章，パラグラフには意味づけをするのに有用でCNNがヒラルキーなモデルになる</li>\n<li>CNNは一条に乏しいため，最近のSoTAにおいては，テキストをencoderにかけたのちにCNNを用いてヒエラルキーな特徴抽出を行っている．</li>\n<li>欠点として入力系列の長さは固定長のため以下の使用例\n<ul>\n<li>encoderの出力をCNNでベクトル化</li>\n<li>contextと返答の候補を行列にして，CNNで近さを図ることによって，妥当な候補を選び出す</li>\n</ul>\n</li>\n<li>基本的にCNNとencoderはセットか？</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/hdigizvu.png\" alt=\"\"></p>\n<ul>\n<li>RNN and Vanilla seq2seq\n<ul>\n<li>系列として扱えるのが利点と考えるべき</li>\n<li>HMMや古典的な系列モデルだと，推論時のアルゴリズムの複雑さや考えるべき状態空間の増大に合わせて行列サイズが大きくなりすぎて，大きな状態空間を必要とするデータには対応しがたい．</li>\n<li>マルコフモデルは限られた条件下においては強力なモデルになりうる．</li>\n<li>RNNは最近では提案されないが，NLPタスクにおいては未だ現役として活躍することもある</li>\n<li>Jordan-Type &#x26; Elman-Type RNN</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/ovxdz4bq.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-graphql\">\t- Jordan-<span class=\"hljs-keyword\">Type</span> RNN\n</code></pre>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/o372ifqn.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t\t-</span> 最新の隠れ層の状態は，Input<span class=\"hljs-emphasis\">_tとOutput_</span>t-1による\n<span class=\"hljs-code\">\t\t\t\n</span>\n<span class=\"hljs-bullet\">\t-</span> Elman-Type RNN\n</code></pre>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/oewhruxt.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t\t-</span> 最新の隠れ層の状態は，Input<span class=\"hljs-emphasis\">_tとHidden_</span>t-1による\n<span class=\"hljs-bullet\">\t-</span> いずれにしてもシンプルなRNNは勾配消失か勾配爆発が大抵おこる\n<span class=\"hljs-code\">\t\n</span>\n<span class=\"hljs-bullet\">-</span> LSTM\n</code></pre>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/k612y88q.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t-</span> Gates\n<span class=\"hljs-bullet\">\t\t-</span> 入力ゲート\n<span class=\"hljs-bullet\">\t\t-</span> 忘却ゲート\n<span class=\"hljs-bullet\">\t\t-</span> 出力ゲート\n\n\n<span class=\"hljs-bullet\">-</span> GRU; Gated Recurrent Unit\n</code></pre>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/42latyhe.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t-</span> Gates\n<span class=\"hljs-bullet\">\t\t-</span> 更新ゲート\n<span class=\"hljs-bullet\">\t\t-</span> リセットゲート\n<span class=\"hljs-bullet\">\t-</span> パラメータが少ないため，\n<span class=\"hljs-bullet\">\t\t-</span> 早い\n<span class=\"hljs-bullet\">\t\t-</span> 汎化性がみられる\n<span class=\"hljs-bullet\">\t-</span> ただし，\n<span class=\"hljs-bullet\">\t\t-</span> 大きなデータセットには対応しきれないこともある\n<span class=\"hljs-code\">\t\t\n</span>\n<span class=\"hljs-bullet\">-</span> Bi-directional RNN\n</code></pre>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/t4zdtd2s.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t-</span> 双方向を考慮したRNN\n<span class=\"hljs-code\">\t\t\n</span>\n<span class=\"hljs-bullet\">-</span> seq2seq; Encoder-Decoder model\n<span class=\"hljs-bullet\">\t-</span> 初めは機械翻訳のために提案された手法\n<span class=\"hljs-bullet\">\t-</span> Encoderにより入力系列をベクトル化，その隠れ状態をDecodeして生成することを目指す\n</code></pre>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/yubh8sbs.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t-</span> Encode時\n<span class=\"hljs-bullet\">\t\t-</span> t時刻のinputとt-1時刻のhiddenによって，t時刻のhiddenが決まる\n<span class=\"hljs-bullet\">\t-</span> Decode時\n<span class=\"hljs-bullet\">\t\t-</span> t時刻のhiddenとt-1時刻のoutputによって，t時刻のoutputをデコードする\n<span class=\"hljs-bullet\">\t-</span> 入力系列と出力系列の長さが固定長である必要はない．\n<span class=\"hljs-bullet\">\t\t-</span> その代わり，適応させる系列長と出力される系列長は同じになることは保証されない\n</code></pre>\n<ul>\n<li>Hierarchical Recurrent Encoder-Decoder; HRED</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/l6wqmq77.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-diff\"><span class=\"hljs-deletion\">- コンテクストを理解するためのseq2seqモデル</span>\n<span class=\"hljs-deletion\">- クエリの履歴を理解する？</span>\n<span class=\"hljs-deletion\">- トークンレベルとターンレベルで学習する</span>\n</code></pre>\n<ul>\n<li>Memory Networks</li>\n<li>Attention and Transformer\n<ul>\n<li>Attention</li>\n<li>Transformer\n<ul>\n<li>Muti-head Attention</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Pointer Net and CopyNet\n<ul>\n<li>Pointer Net</li>\n<li>CopyNet</li>\n</ul>\n</li>\n<li>Deep RL and GANs\n<ul>\n<li>Deep Q-Networks</li>\n<li>REINFORCE</li>\n<li>GANs</li>\n</ul>\n</li>\n<li>Knowledge Graph Augmented Neural Networks</li>\n</ul>\n<p>2章は途中から読むのやめた．使用されるネットワークよりも課題感の方が知りたい．</p>\n<h3>タスク指向型対話システム</h3>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/xkjm1m9f.png\" alt=\"\"></p>\n<p>ドメインの決まったタスクにおいて特定の問題を解決する．</p>\n<ul>\n<li>Natural Language Understanding</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/anac43sh.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">-</span> 3つのタスクを持つ\n<span class=\"hljs-bullet\">\t-</span> ドメイン分類\n<span class=\"hljs-bullet\">\t-</span> 意図の理解\n<span class=\"hljs-bullet\">\t-</span> スロット埋め\n<span class=\"hljs-bullet\">-</span> IOB; Inside Outside Beginning\n<span class=\"hljs-bullet\">-</span> NER; Named Entity Recognition\n<span class=\"hljs-bullet\">-</span> intent detectionにおいては，Task-Oriented Dialogue BERTがSoTA?\n<span class=\"hljs-bullet\">-</span> Domain classification &#x26; intent detectionは同カテゴリタスク\n\n\n<span class=\"hljs-bullet\">-</span> slot filling task = semantic tagging\n<span class=\"hljs-bullet\">-</span> NLUタスクを解く際に，音声データをそのままInputとして与える研究事例も出ているらしい\n<span class=\"hljs-bullet\">\t-</span> エラーが少なくロバストなモデルになったらしい？\n<span class=\"hljs-bullet\">-</span> Natural Language UnderstandingとNatural Language Generationは逆のプロセスをふむ\n<span class=\"hljs-bullet\">\t-</span> 同時にタスクを学習結果が得られるというアプローチも\n</code></pre>\n<ul>\n<li>Dialogue State Tracking</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/vu1cl18j.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">-</span> ユーザの目的と対話履歴を追跡する\n<span class=\"hljs-bullet\">-</span> NLUとDSTのタスクは近い関係にある．\n<span class=\"hljs-bullet\">-</span> NLUは単語にtagを割り振っていくイメージ\n<span class=\"hljs-bullet\">-</span> DSTはtagのplaceholderを会話の内容から埋めていくイメージ\n<span class=\"hljs-bullet\">-</span> Dialogue Stateには3つの要素からなる\n<span class=\"hljs-bullet\">\t-</span> Goal constraint corresponding with informable slots\n<span class=\"hljs-bullet\">\t\t-</span> 特別なvalueの制約で，ユーザによって言及されるか特別な値をとる\n<span class=\"hljs-bullet\">\t\t-</span> DontcareやNoneが特別な値にあたる\n<span class=\"hljs-bullet\">\t-</span> Requested slots\n<span class=\"hljs-bullet\">\t-</span> Search method of current turn\n<span class=\"hljs-bullet\">-</span> 古典的な手法でいくと，\n<span class=\"hljs-bullet\">\t-</span> ルールベースはエラーが多く，ドメイン適応が大変\n<span class=\"hljs-bullet\">\t-</span> 統計的手法はノイジーな状態や曖昧性に弱い\n<span class=\"hljs-bullet\">-</span> ニューラルネットな手法\n<span class=\"hljs-bullet\">\t-</span> slot-valueのペアを事前定義して学習\n<span class=\"hljs-bullet\">\t\t-</span> valueが大きくなると複雑性が増す\n<span class=\"hljs-bullet\">\t\t-</span> slot-valueのペアを読むだけでよく，2値分類タスクとして解ける\n<span class=\"hljs-bullet\">\t\t-</span> モデルの複雑性は避けられるが，反応速度が遅くなる可能性がある．\n<span class=\"hljs-bullet\">\t-</span> slot-valueのペアを定義せずに，対話の中から直接選ぶ\n</code></pre>\n<ul>\n<li>\n<p>Policy Learning</p>\n<ul>\n<li>DSTモジュールの出力結果からどう行動をとるか</li>\n<li>教師あり学習or 強化学習</li>\n<li>教師ありだとアノテショーンデータセットを作るのがとても大変</li>\n</ul>\n</li>\n<li>\n<p>Natural Language Generation; NLG</p>\n<ul>\n<li>タスク指向型対話システムにおける最終層のモジュール</li>\n<li>最終的な自然言語表現を生成するシステム</li>\n<li>4つのコンポーネントからなる</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/pki8nvzq.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t-</span> Content Determination\n<span class=\"hljs-bullet\">\t-</span> Sentence Planning\n<span class=\"hljs-bullet\">\t-</span> Surface Realization\n<span class=\"hljs-bullet\">\t-</span> Lexicalization, Referring expression, aggregation\n<span class=\"hljs-bullet\">-</span> RNNに基づいた統計言語モデルにおいて，意味的制約や文法構造による返答生成を行うなど\n<span class=\"hljs-bullet\">-</span> コンテクストを理解した返答を生成することは重要である\n<span class=\"hljs-bullet\">-</span> タスク指向型においては，返答の多様性というよりも信頼性のほうが重要視されがち\n<span class=\"hljs-bullet\">-</span> 意味解析をビームサーチを使うことで，意味の正しさを改善する手法も提案された\n</code></pre>\n<ul>\n<li>E2E Methods\n<ul>\n<li>\n<p>end-to-endのパイプラインを組むことで高いパフォーマンスを発揮することがあるが，</p>\n</li>\n<li>\n<p>多くのモジュールを組み込むため，バックプロパゲーションで誤差が伝播しないこともある．</p>\n</li>\n<li>\n<p>すべてのモジュールが，返答の精度を向上するために，対等に重要であるとは限らない．</p>\n</li>\n<li>\n<p>違うドメインに差し替えるとき，オントロジーを事前学習させる必要があるため，困難が生じることもある</p>\n</li>\n<li>\n<p>やり方は大きく分けて2つ</p>\n<ul>\n<li>すべてのモジュールを展開して誤差逆伝播させる？</li>\n<li>知識ベースの検索システムと返答生成の双方を用いてパイプラインを組む</li>\n</ul>\n</li>\n<li>\n<p>タスク指向型においては，外部の知識源が必要なことが多い</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3>オープンドメイン型対話システム</h3>\n<ul>\n<li>雑談対話システム，或いはタスク思考型ではない対話システムのこと</li>\n<li>SoTAを示しているオープンドメインは大抵ニューラルネットで解決している</li>\n<li>完全なるデータドリブンなものが多い</li>\n<li>オープンドメイン型対話システムは，大まか3つに分けられる\n<ul>\n<li>生成システム</li>\n<li>検索ベースシステム</li>\n<li>アンサンブルシステム</li>\n</ul>\n</li>\n</ul>\n<p>３つの話が以下</p>\n<ul>\n<li>生成システム\n<ul>\n<li>訓練コーパスに出てこないような返答に対して，ユーザのメッセージや対話履歴をマッピングするために，seq2seqなモデルを適用する</li>\n</ul>\n</li>\n<li>検索システム\n<ul>\n<li>決まった返答集合の中からすでに存在する返答を探そうとする</li>\n</ul>\n</li>\n<li>アンサンブルシステム\n<ul>\n<li>生成手法と検索手法を合わせる．</li>\n<li>生成された返答と検索された返答とを比べる．</li>\n<li>生成も，検索された返答を洗練するために用いられる．</li>\n</ul>\n</li>\n</ul>\n<p>特徴として，</p>\n<p>生成モデルは</p>\n<p>柔軟でコンテクストを読んだ返答をできるが，ときには理解に欠けていたり，怠けた返答を見せることがある</p>\n<p>検索ベースのモデルは</p>\n<p>人の返答の集合から実際の返答を選ぶため，返答の集合は有限集合であり，コンテクストと相関がないことがある．</p>\n<p>ただし，表面上のレベルでは，首尾一貫した返答することも多い</p>\n<p>以下は，オープンドメイン型対話システムにおける，難しさとホットなトピックをまとめる</p>\n<ul>\n<li>Context Awareness\n<ul>\n<li>対話コンテクストは会話のトピックを決定したり，ユーザの目標を決定したりと重要</li>\n<li>コンテクストを解釈した対話エージェントは，現メッセージだけではなく，対話履歴からももとにして返答する</li>\n<li>生成モデルも検索ベースも，どちらも対話コンテクストモデリングに依存する</li>\n<li>いくつかのモデルではAttentionが使用されているらしい</li>\n<li>構造化されたAttentionを用いることでコンテクストを読み取れる？</li>\n<li>対話をリライトする問題があるらしい\n<ul>\n<li>複数のメッセージから単一のメッセージに変換する目標</li>\n<li>ここではコンテクストを理解させることが重要</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Response Coherence\n<ul>\n<li>首尾一貫した返答は，良い生成器としての一つのクオリティ</li>\n<li>対話の中で，論理的で首尾一貫しているか？という指標</li>\n<li>生成モデルにおいてホットなトピックとなっている（検索ベースはすでに人の返答をりようするのでもともと一貫性はあるという主張）</li>\n<li>一貫性のない文の順序を見つけるタスクを解くことで，返答の一貫性を改善した事例もあり</li>\n</ul>\n</li>\n<li>Response Diversity\n<ul>\n<li>人が多用するような表現は訓練コーパスにも多く含まれ，それらばかりを返答してしまうことが問題となりうる</li>\n<li>かつては条件付き確率において，尤度関数を解くことで尤もらしい返答をもとめていた．</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/7leakwao.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">\t-</span> この手法では，返答の精度の安全性と適切さはトレードオフになっていた？\n<span class=\"hljs-bullet\">-</span> ビームサーチを提案されたことも\n<span class=\"hljs-bullet\">-</span> \n</code></pre>\n<ul>\n<li>Speaker Consistency and Personality-based Response\n<ul>\n<li>システムは，訓練コーパスからサンプリングされた分布に対して学習\n<ul>\n<li>対話者の趣味といった一貫性のないものに対する返答は．．．</li>\n<li>対話者の役割を理解し，その個人に合わせた返答が必要になる</li>\n</ul>\n</li>\n<li>1ステージではなく，3ステージで個人的な嗜好に対応した事例がある</li>\n<li></li>\n</ul>\n</li>\n<li>Empathetic Response\n<ul>\n<li>同情する対話システムは，ユーザの感情の変化や感情に伴った適切な返答をする</li>\n<li>雑談チャットについて，このトピックは重要</li>\n<li>CortanaやAlexaなどの製品にもモジュールが含まれている</li>\n<li>CoBERTのモデルなど</li>\n<li>感情対話システムのデータセットはとぼしいが，新たなデータセットとベンチマークが提供されたらしい</li>\n<li></li>\n</ul>\n</li>\n<li>Conversation Topics\n<ul>\n<li>トピックや目的は，会話に参加した人と会話を続けるための重要な役割を果たす</li>\n<li>トピックを理解させることが重要</li>\n<li></li>\n</ul>\n</li>\n<li>Knowledge-Grounded System\n<ul>\n<li>人は，会話のコンテクストと経験や記憶といったものとを関連付けて，返答をする（機会には難しい）</li>\n<li>生成モデルは，単なる機械翻訳よりも複雑\n<ul>\n<li>より自由度が高く，制約が曖昧なため</li>\n</ul>\n</li>\n<li>故に，雑談チャットは，外部から得られる常識と結びつけて，seq2seqなモデルによって生成する</li>\n<li>メモリーネットワークなどで，知識をグラウンディングする手法</li>\n<li>知識グラフは外部の情報をソースにするものもある．</li>\n<li>graph attentionを用いて，常識をグラフベースで学習する手法も</li>\n<li>主な考え方は，外部の知識グラフを使って，会話の論理の流れをモデリングする指標の一部として扱う</li>\n</ul>\n</li>\n<li>Interactive Training\n<ul>\n<li>別名；human-in-loop training</li>\n<li>アノテーションされたデータセットは限られている\n<ul>\n<li>すべての状況をカバーすることは不可能</li>\n</ul>\n</li>\n<li>ユーザとの対話の中で，システムを改善する</li>\n<li>強化学習における逐次学習を提案</li>\n<li>対話相手と話して，その相手からフィードバックを得る</li>\n<li>教師あり学習をした後，Interactive Trainingによってファインチューニングする</li>\n<li></li>\n</ul>\n</li>\n<li>Visual Dialogue</li>\n</ul>\n<p><img src=\"/images/article/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey/6bhqn4eh.png\" alt=\"\"></p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-bullet\">-</span> Visual Q &#x26; Aなど\n<span class=\"hljs-bullet\">-</span> 画像あり対話システムのほか，映像あり対話システムも面白いトピックだが難題でもある\n<span class=\"hljs-bullet\">\t-</span> 特徴量抽出の複雑さも増す\n<span class=\"hljs-bullet\">-</span> visual dialogueのアノテーションは重労働であり，データセットに乏しいので，現在はデータの不十分さに悩まされている\n<span class=\"hljs-bullet\">-</span> \n</code></pre>\n<h3>評価のアプローチ</h3>\n<p>評価の仕方も重要なパートとなっている</p>\n</body>\n</html>\n","Title":"【論文まとめ】Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey","Date":"2023-05-21","Category":"論文","Tags":["survey","dialogue system"],"Authos":"ゆうぼう","Slug":"Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey","Thumbnail":"/images/thumbnails/Recent-Advances-in-Deep-Learning-Based-Dialogue-Systems-A-Systematic-Survey.png","Description":"Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Surveyのまとめ","Published":true},{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p>まとめること</p>\n<ol>\n<li>Knowledge-Intensive NLPの概要</li>\n<li>Knowledge Sources\n<ol>\n<li>Encyclopedic Knowledge</li>\n<li>Commonsense Knowledge</li>\n<li>最近のKnowledge Sourcesの特徴</li>\n</ol>\n</li>\n<li>Knowledge-Intensive NLP Task\n<ol>\n<li>Knowledge-Intensive NLP Taskの概要</li>\n<li>Knowledge-Intensive NLP Taskの特徴</li>\n</ol>\n</li>\n<li>Knowledge Fusion Methodsについて\n<ol>\n<li>Pre-Fusion Methods</li>\n<li>Post-Fusion Methods</li>\n<li>Hybrid-Fusion Methods</li>\n<li>代表的なモデルの紹介</li>\n</ol>\n</li>\n<li>Challengingなことと今後の方向性\n<ol>\n<li>Unified PLMKEs Across Tasks and Domains</li>\n<li>Reliability of Knowledge Sources</li>\n<li>Reasoning Module Design</li>\n</ol>\n</li>\n</ol>\n<h2>概要</h2>\n<p>事前学習済みモデルにより，モデルのcapacityは増加傾向にあるが，encyclopedicやcommonsenseを用いた，knowledgeableなNLPモデルの需要の高まりが生じている</p>\n<p>**PLMKEs (Pre-trained Language Model-based Knowledge-Enhanced models)**についてまとめたsurvey論文</p>\n<p>linguistic or factual knowledgeは暗示的にモデルのパラメータに保存される</p>\n<p>→事前学習済みのNLPモデルがより汎用的な能力を持つことを一部ではあるが説明できる</p>\n<p>今のpre-trained LMは，明示的なencyclopedicやcommonsenseのレバレッジ能力に欠けている</p>\n<p>PLMKEsは，関連する外部知識を取り出すモジュールと知識を混ぜるモジュールがある</p>\n<p>PLMKEsに関連した重要な3つの要素がある</p>\n<ol>\n<li>Knowledge Sources</li>\n<li>Knowledge-Intensive NLP Tasks</li>\n<li>Knowledge Fusion Methods</li>\n</ol>\n<h2>Knowledge Sources</h2>\n<h3>Encyclopedic knowledge</h3>\n<p>エンティティに関する属性とエンティティ間の関係性をもった知識</p>\n<p>Entity: person → Attributes: age → Relations: educated at</p>\n<p>Wikipediaは大量のencyclopedicな知識を持っている</p>\n<p>人物の経歴やイベントの背景などを含んでいる</p>\n<p>一般的にはtripletsで構成されていることが多い</p>\n<p>e.g. &#x3C;Tom Hanks, occupation, actor></p>\n<p>Wikidataのような知識データがPLMKEsに広く使用されている</p>\n<h3>Commonsense Knowledge</h3>\n<p>日常生活のなかでの状況に関する知識</p>\n<p>イベントとその影響を記す</p>\n<p>e.g. mop up the floor if we split food over it / study hard to win scholarship / goat has four legs</p>\n<p>commonsenseの特徴</p>\n<p>多くの人の間で共有されている知識であり，コミュニケーションの中で暗示的に想定されている知識である</p>\n<p>commonsenseもtripletsで表現される</p>\n<p>最近のPLMKEsでは，ConceptNetとATOMICが外部知識として使用されることが多い</p>\n<h3>Knowledge Sourcesの特徴</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/uthim0s7.png\" alt=\"\"></p>\n<p>large-scaleでdiverse</p>\n<p>現在のソースはより正確で安定的に作られている</p>\n<p>アノテーションのプロセスは部分的に自動化されていて，非エキスパートにもaccessibleになっている</p>\n<p>知識データがカバーするドメインは多様</p>\n<p>オープンドメインのものもあれば，specificなドメインのものも</p>\n<p>Wikipedia, DBPedia, Freebaseなどはオープンドメイン</p>\n<p>UMLSやAMinerなどはbiomedicineやscienceの特定ドメイン</p>\n<p>domain-specificなアプリケーションをブーストできる知識</p>\n<p>commonsenseに関しては</p>\n<p>ConceptNetやTransOMCSは複数のドメインのcommonsenseをカバー</p>\n<p>ATOMICやASERはある特定のタイプのcommonsenseにフォーカスした知識ソース</p>\n<h2>Knowledge-Intensive NLP Task</h2>\n<h3>概要</h3>\n<p>Knowledge-intensive NLP taskは必要とする知識ソースの種類で2つに分けられ，さらに詳細に分けることができる</p>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/gqzadptj.png\" alt=\"\"></p>\n<ul>\n<li>\n<p>encyclopedic knowledge-intensive NLP task\nencyclopedicの知識ソースを利用する</p>\n<ul>\n<li>open-domain QA</li>\n<li>fact verification</li>\n<li>entity linking</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/62ojprnq.png\" alt=\"\"></p>\n<ul>\n<li>\n<p>commonsense knowledge-intensive NLP task\ncommonsenseの知識ソースを利用する</p>\n<p>commonsenseの多様性のために，タスクのタイプ自体も多様化している</p>\n<p>モデルが正確に日常のシナリオを理解し，応答するか否かのテストにフォーカスしたタスク</p>\n<ul>\n<li>General Commonsense</li>\n<li>Social Commonsense</li>\n<li>Physical Commonsense</li>\n<li>Temporal Commonsense</li>\n</ul>\n</li>\n</ul>\n<h3>Knowledge-Intensive Taskの特徴</h3>\n<p>実際は，モデルにとってだけではなく，人間にとってもいかなる知識の参照なしに問題に答えるのは難しい．（バラクオバマの誕生日はいつ？など</p>\n<p>しかも，外部知識が必要なのにinputとして必要な外部知識が渡されないため，とてもチャレンジングなタスクになっている</p>\n<p>そもそも必要な外部知識にグラウンディングするモジュールをPLMKEsの設計に加えることを考慮するようになっている</p>\n<h2>Knowledge Fusion Methods</h2>\n<p>モデルが知識を統合するステージは二箇所あり，</p>\n<ul>\n<li>Pre-fusion; pre-training</li>\n<li>Post-fusion; fine-tuning</li>\n</ul>\n<p>の二通りが考えられる（もしくはその両方のステージ</p>\n<h3>Pre-Fusion Methods</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/kb8yzih8.png\" alt=\"\"></p>\n<p>pre-trainingのステージで知識を統合する手法</p>\n<p>モデルに知識を入力するため，構造化された知識データを非構造化データのテキストコーパスへと処理→モデルに入力</p>\n<p>テキストデータとして知識を入力するため，大きくモデルのアーキテクチャを変更する必要はない</p>\n<p>ただし，知識グラフのような構造化データを非構造化データへ変えることは難しいこともある</p>\n<p>簡単な対処法はエンティティと関係性を結合するか，流暢な文章をconditional text generation modelに生成させるか</p>\n<p>Zhang et al. 2019 | Agarwal et al. 2021 を参照（必要になれば読む</p>\n<h3>Post-Fusion Methods</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/m33733t8.png\" alt=\"\"></p>\n<p>まず，関連知識をキャプチャする</p>\n<p>次に取得した関連知識をGNNなどのエンコーダでembeddingを得る</p>\n<ul>\n<li>それを追加特徴量としてpre-trained LMに与える（図でいうA）</li>\n<li>直接pre-trained LMに入力する（図でいうB）</li>\n</ul>\n<h3>Hybrid-Fusion Methods</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/6xhcy94c.png\" alt=\"\"></p>\n<p>pre-trainingとfine-tuningの両方のステージで知識を統合する</p>\n<p>追加の学習されるretrieverによりaugmentされたpre-trained modelは，fine-tuningのステージでより効果的にretrieverからの知識を活用できる</p>\n<p>retrieval-augmented pre-trainingでhybrid-fusionが広く使われている</p>\n<h3>代表的なモデル</h3>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/codzwuzx.png\" alt=\"\"></p>\n<p><img src=\"/images/article/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models/myiyapki.png\" alt=\"\"></p>\n<p>Table4/5はSOTAモデルを示す</p>\n<p>encyclopedic knowledge-intensive taskにおいては，BOOLQをのぞき，全てpost-fusionを採用</p>\n<p>commonsense knowledge-intensive taskにおいては，CommonsenseQAをのぞき，全てpre-fusionを採用</p>\n<p>pre-fusionとpost-fusionの違いは何？</p>\n<p>pre-fusionは，知識を事前学習のパラメータに暗示的に保存数る</p>\n<p>最終的にどの知識がパラメータに保存するのかを決定するのは難しい</p>\n<p>知識の引き出しや利用の難しさが増す</p>\n</body>\n</html>\n","Title":"【論文まとめ】A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models","Date":"2023-05-21","Category":"論文","Tags":["survey","NLP","knowledge-base","PLMKE","commonsense","encyclopedic","Knowledge-Intensive NLP"],"Authos":"ゆうぼう","Slug":"A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models","Thumbnail":"/images/thumbnails/A-Survey-of-Knowledge-Intensive-NLP-with-Pre-Trained-Language-Models.png","Description":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Modelsのまとめ","Published":true}],"tag":"survey","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","CSS","dialogue system","DST","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true}