{"pageProps":{"TaggedPostData":[{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<h2>概要</h2>\n<p>LSTM-styleなSDUを提案</p>\n<p>ゲートとしてSDUをTransformer内部に適用することにより，ハイパラをチューニングすることなく，Transformerの浅い層において，内在的な意味の重要性を捉え，より早い収束を可能に</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/8k6bi4pv.png\" alt=\"\"></p>\n<h3>Self-Dependency Units (SDU)</h3>\n<p>sigmoid gatesを導入する</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ψ</mi></mrow><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Ψ</span></span></span></span></span>はゲートとして作用し，logistic sigmoidもしくはtanhで実現</p>\n<p>筆者らの認識</p>\n<p>tanhはupdate gateとして作用し，重要度の幅を-1 to 1に制限</p>\n<p>sigmoidはLSTMのinput gateと似ていて，feature-wise levelでどれくらいの情報を残すか決定</p>\n<h3>Pseudo-highway Connection</h3>\n<p>residual connectionされたgating-modified encodingsでMulti Head Dot Product Attention (MHDPA)の分散表現を豊かにするため，新たな計算グラフの枝を追加し，SDUとIdentityとMHDPAをpost LNを使用してresidual connectionする</p>\n<h2>新規性</h2>\n<p>本来，人にとって，読み物をよりよく理解するためには，global contextだけではなく，ここの単語の意味も必要</p>\n<p>→ Self-gatingなアプローチを提案</p>\n<ol>\n<li>Transformerにおける浅い層において，trainingとvalidationでハイパラチューニングすることなく，より高速な収束を達成</li>\n<li>Transformerでの低レイヤーにおいて，local-range encodingにフォーカスした層を実現</li>\n<li>Self-gating mechanismは，R-TransformerやTransformer-XLのコンポーネントとしてRNN-likeなメカニズムを補完</li>\n</ol>\n<h2>実験</h2>\n<p>SDUを導入し，PTBデータセットにおけるSDUの効果を検証</p>\n<p>sigmoidとtanhを実験</p>\n<h2>まとめ</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/wkvb0w3b.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/78a3mp9o.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/44cjh1ml.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/pyc6iyml.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/pn7oohqa.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/jq3ijo8p.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/jqxalgb9.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/w2x338gk.png\" alt=\"\"></p>\n<p>sigmoidによるSDUが安定しているが，データとタスクによってはtanhの方がoutperformすることがある</p>\n<p>いずれのactivationを使っても収束は早い</p>\n<p>enwik8による大規模データでの追実験において，提案手法が浅いレイヤーには寄与することが確かめられた</p>\n<p>SDUで計算量が増えるが，そこまで差はなかった</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n</body>\n</html>\n","Title":"【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks","Date":"2023-05-21","Category":"論文","Tags":["transformer","Highway Transformer","Gating Mechanism","Self-Dependency-Units (SDU)"],"Authos":"ゆうぼう","Slug":"Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks","Thumbnail":"/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png","Description":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ","Published":true}],"tag":"Gating Mechanism","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","CSS","dialogue system","DST","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true}