{"pageProps":{"postData":{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<h2>概要</h2>\n<p>Document-level neural machine translationにおいて，Multi-Hopなアーキテクチャを導入することにより，従来手法と比べて精度の高い文脈を考慮した機械翻訳を実現</p>\n<p>翻訳者のように，頭の中に翻訳のドラフトを作り，文脈に合わせて適切に修正する流れ（human-like draft-editing）を明示的にモデリング</p>\n<p>大きな事前学習済みモデルを使うことなく，使用に足る機械翻訳モデルを実現</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/m53mefg3.png\" alt=\"\"></p>\n<p>アーキテクチャ周りのこと</p>\n<h3>Sentence Encoder</h3>\n<p>source-sideとtarget-sideでそれぞれPretrained Encoderがあり，source contextとtarget draftの分散表現をそれぞれ得る</p>\n<h3>Multi-Hop Encoder</h3>\n<p>source-contextにおいて文章ごとのreasoningをして，現在の文章の分散表現を得る</p>\n<h3>Multi-Hop Decoder</h3>\n<p>target-side draftから情報を取得して，翻訳の確率分布を得る</p>\n<p>そのほかアーキテクチャの工夫</p>\n<h3>Contet Gating</h3>\n<p>contextual informationを過剰にutilizeしすぎないように，context gating machanismを採用</p>\n<p>contextと現在の文章間の重みを動的にコントロールする</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>a</mi></msub><msubsup><mi>A</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>+</mo><msub><mi>W</mi><mi>b</mi></msub><msubsup><mi>B</mi><mrow><mi>s</mi><mo>−</mo><mi>i</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\alpha = \\sigma(W_a A_s^{(n)} + W_b B_{s-i}^{(n)})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2948em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">a</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.5834em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">n</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1166em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.38em;vertical-align:-0.3352em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">b</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231em;margin-left:-0.0502em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\">i</span></span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">n</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3352em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span> where sigma is logistic sigmoid function</p>\n<h2>新規性</h2>\n<p>Docment-level NMTにおける従来手法の問題点</p>\n<ol>\n<li>文章間のreasoningの特徴づけを明示的に行うことなく，単純にcontextの分散表現を導入</li>\n<li>推論時にはアクセスできないのに，訓練時には追加入力としてのtarget contextにground-truthなデータを入力\n↑　訓練時と推論時において状況が異なる</li>\n</ol>\n<p>Document-level NMTにおいてMulti-Hop reasoningをモデリングしたMulti-Hop Transformerの提案と提案モデルによるDocument-level NMTの大きな性能改善</p>\n<p>target contextにground-truthで訓練すると推論時にはアクセスできないため，他の翻訳モデルの翻訳結果を使用することで，訓練時と推論時の状況を同じにした</p>\n<h2>実験</h2>\n<p>Baseline</p>\n<p>Transformer</p>\n<p>CA-Transformer</p>\n<p>CA-HAN</p>\n<p>CADec</p>\n<p>計算量のオーバーヘッドを改善するためSentence Encoderはそれぞれのsideでパラメータを共有</p>\n<h2>まとめ</h2>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/ttppissj.png\" alt=\"\"></p>\n<p>large-scaleな事前学習済み言語モデルを使用することなく，SoTA翻訳クオリティを達成</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/5tcp1vq8.png\" alt=\"\"></p>\n<p>contextを付与するためのAttentionの構造は，ConcatやHierarchicalよりもMulti-HopなAttentionが効果があり</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/02f89lfl.png\" alt=\"\"></p>\n<p>contextを考慮する幅のwindow sizeは大きくするほど効果が上がるわけではなく．3が最も良かった</p>\n<p>4以上にすると悪化傾向らしく，本研究ではwindow size = 3 を採用</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/0ea5zzmc.png\" alt=\"\"></p>\n<p>contextにおいてreasoningするときの方向は，一般的な読み順の通りleft-to-rightで順方向にreasoningさせた方が結果は良かった</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/dxq6ycal.png\" alt=\"\"></p>\n<p>訓練時と推論時にtarget draftに与える文章が異なる問題への対処に関する実験結果</p>\n<p>Referenceはground-truthをtarget draftとして与えて訓練，Draftはpre-trained MT systemが生成した翻訳結果をtarget draftとして与えて訓練したモデル</p>\n<p>Draftの方が結果がよく，pre-trained MT systemの生成結果をtarget draftとする方法によって訓練時と推論時のギャップの橋渡しになることを示唆する結果</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<p><a href=\"/5955ca444629476ebf23e66629a2413f\">Context-Aware Self-Attention Networks</a></p>\n</body>\n</html>\n","Title":"【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation","Date":"2023-05-21","Category":"論文","Tags":["MT","transformer","Multi-Hop Transformer"],"Authos":"ゆうぼう","Slug":"Multi-Hop-Transformer-for-Document-Level-Machine-Translation","Thumbnail":"/images/thumbnails/Multi-Hop-Transformer-for-Document-Level-Machine-Translation.png","Description":"Multi-Hop Transformer for Document-Level Machine Translationのまとめ","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","CSS","dialogue system","DST","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"]},"__N_SSG":true}