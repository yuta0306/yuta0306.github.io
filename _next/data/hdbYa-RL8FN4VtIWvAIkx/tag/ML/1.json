{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．</p>\n<p>この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変か?)</p>\n<h2>やりたかったこと</h2>\n<p>DataLoaderを定義するときに，<code>collate_fn</code>のところで自作collate_fnを指定して，batch単位で流れてくるデータに対してエンコードすること．</p>\n<p>これがやりたいことになります．つまりこんな感じ</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> torch.utils <span class=\"hljs-keyword\">import</span> Dataset, DataLoader\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyDataset</span>(<span class=\"hljs-title class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, *args, **kwargs</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        ...\n    \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"hljs-params\">self</span>):\n        ...\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span class=\"hljs-params\">self, idx: <span class=\"hljs-built_in\">int</span></span>):\n        ...\n\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span class=\"hljs-number\">16</span>, shuffle=<span class=\"hljs-literal\">True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=custom_collate_fn)  <span class=\"hljs-comment\"># &#x3C;--- ここで自作collate_fnを指定して制御</span>\n</code></pre>\n<h2>やってうまくいかなかったこと</h2>\n<p>先にやってうまくいかなかったことを共有しておきます．</p>\n<p>自分が使っているのが，<code>pytorch-lightning</code>なのでそのせいもあるかもしれません．なので，もしかしたら普通に素のPytorchならうまくいくかもしれません．</p>\n<p>教えてください🙏</p>\n<h3>lambda式で制御する (functools.partialを使う)</h3>\n<p>こんなことをしました．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> torch.utils <span class=\"hljs-keyword\">import</span> Dataset, DataLoader\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyDataset</span>(<span class=\"hljs-title class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, *args, **kwargs</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        ...\n    \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"hljs-params\">self</span>):\n        ...\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span class=\"hljs-params\">self, idx: <span class=\"hljs-built_in\">int</span></span>):\n        ...\n        <span class=\"hljs-keyword\">return</span> text, label\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">custom_collate_fn</span>(<span class=\"hljs-params\">data, tokenizer, max_length</span>):\n    texts, labels = <span class=\"hljs-built_in\">zip</span>(*data)\n    texts = <span class=\"hljs-built_in\">list</span>(texts)\n    texts = tokenizer.batch_encode_plus(\n        texts,\n        padding=<span class=\"hljs-literal\">True</span>,\n        truncation=<span class=\"hljs-literal\">True</span>,\n        max_length=max_length,\n        return_tensors=<span class=\"hljs-string\">'pt'</span>,\n    )\n    labels = torch.LongTensor(labels)\n    <span class=\"hljs-keyword\">return</span> texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = <span class=\"hljs-number\">256</span>\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span class=\"hljs-number\">16</span>, shuffle=<span class=\"hljs-literal\">True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=<span class=\"hljs-keyword\">lambda</span> data: custom_collate_fn(data, tokenizer, max_length))\n</code></pre>\n<p><code>pytorch-lightning</code>の仕様だとは思うのですが，<code>pickle</code>で圧縮するらしくそのタイミングでエラーを吐かれました．</p>\n<p>なぜだろう...有識者の方教えてください...</p>\n<h2>【解決策】 classで定義する</h2>\n<p>lambda式でダメだったので，もうクラスの内部に必要なものを保持させておこうということになりました．(僕の中では)</p>\n<p>次のコードのような感じで解決しました．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> torch.utils <span class=\"hljs-keyword\">import</span> Dataset, DataLoader\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyDataset</span>(<span class=\"hljs-title class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, *args, **kwargs</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        ...\n    \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"hljs-params\">self</span>):\n        ...\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span class=\"hljs-params\">self, idx: <span class=\"hljs-built_in\">int</span></span>):\n        ...\n        <span class=\"hljs-keyword\">return</span> text, label\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CollateFn</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, tokenizer, max_length: <span class=\"hljs-built_in\">int</span></span>) -> <span class=\"hljs-literal\">None</span>:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        os.environ[<span class=\"hljs-string\">\"TOKENIZERS_PARALLELISM\"</span>] = <span class=\"hljs-string\">\"true\"</span>  <span class=\"hljs-comment\"># &#x3C;--- 多分これを明示的に指定しないと怒られます (true|false)</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\">self, data</span>):\n        texts, labels = <span class=\"hljs-built_in\">zip</span>(*data)\n        texts = <span class=\"hljs-built_in\">list</span>(texts)\n        texts = self.tokenizer.batch_encode_plus(\n            texts,\n            padding=<span class=\"hljs-literal\">True</span>,\n            truncation=<span class=\"hljs-literal\">True</span>,\n            max_length=self.max_length,\n            return_tensors=<span class=\"hljs-string\">'pt'</span>,\n        )\n        labels = torch.LongTensor(labels)\n        <span class=\"hljs-keyword\">return</span> texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = <span class=\"hljs-number\">256</span>\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span class=\"hljs-number\">16</span>, shuffle=<span class=\"hljs-literal\">True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=CollateFn(tokenizer, max_length))\n</code></pre>\n<h2>まとめ</h2>\n<p>素のPytorchで組めば問題なかったのかもしれませんが，<code>pytorch-lightning</code>を使っている方は同じ状況になるかもしれません．</p>\n<p>その時は，ぜひ参考にclassでcollate_fnで実装してみて解決の一助となれたら幸いです．</p>","Title":"collate_fnで複数の引数を取りたい!!","Date":"2021-12-24","Category":"Python","Tags":["ML","Python","Pytorch"],"Authors":"ゆうぼう","Slug":"pytorch-collate_fn-args","Thumbnail":"/images/thumbnails/pytorch-logo.jpg","Description":"Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変かも)","Published":true},{"contentHtml":"<p>ロジスティック回帰をscikit-learnで実装していると、デフォルトはL2正則化でペナルティを与えています。</p>\n<p>そこで、もっとスパースにしてやろうとL1正則化を行おうとしたのだが、エラーを吐かれた。その時の解決策を共有します。</p>\n<h2>エラーを吐かれた時のバージョン</h2>\n<p>一応最近開発が立て込んでいるので、Anacondaを使って開発環境を分けているのですが、</p>\n<pre><code class=\"hljs language-bash\">(base)$ conda activate ML\n(ML)$ conda list    (pip listでも行ける)\n......\n......\nscikit-learn           0.23.1    \n......\n</code></pre>\n<p>ということで、scikit-learnのバージョンは<em>0.23.1</em>でした。</p>\n<h2>実装してみる</h2>\n<p>それでは実装してみます。至ってシンプルなスクリプトで動く想定でやっていきます。こちらで変化させるハイパーパラメータは以下になります。</p>\n<table>\n<thead>\n<tr>\n<th>パラメータ</th>\n<th>値</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>C(正則化のつよさ)</td>\n<td>1(デフォルト)</td>\n</tr>\n<tr>\n<td>penalty</td>\n<td>L1正則化</td>\n</tr>\n<tr>\n<td>max_iter(イテレーションの上限)</td>\n<td>100000</td>\n</tr>\n</tbody>\n</table>\n<p>また、訓練データ(X_train, y_train)とテストデータ(X_test, y_test)に分けてあることにします。</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> sklearn.linear_model <span class=\"hljs-keyword\">import</span> LogisticRegression\n\nlr_l1 = LogisticRegression(penalty=<span class=\"hljs-string\">'l1'</span>, max_iter=<span class=\"hljs-number\">100000</span>).fit(X_train, y_train)\n</code></pre>\n<p>するとこんなエラーが返ってきます。</p>\n<pre><code class=\"hljs language-bash\">~/opt/anaconda3/envs/ML/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py <span class=\"hljs-keyword\">in</span> fit(self, X, y, sample_weight)\n1302         The SAGA solver supports both float64 and float32 bit arrays.\n1303         <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n-> 1304         solver = _check_solver(self.solver, self.penalty, self.dual)\n1305 \n1306         if not isinstance(self.C, numbers.Number) or self.C &#x3C; 0:\n\n~/opt/anaconda3/envs/ML/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py in _check_solver(solver, penalty, dual)\n441     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):\n442         raise ValueError(\"</span>Solver %s supports only <span class=\"hljs-string\">'l2'</span> or <span class=\"hljs-string\">'none'</span> penalties, <span class=\"hljs-string\">\"\n--> 443                          \"</span>got %s penalty.<span class=\"hljs-string\">\" % (solver, penalty))\n444     if solver != 'liblinear' and dual:\n445         raise ValueError(\"</span>Solver %s supports only <span class=\"hljs-string\">\"\n\nValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n</span></code></pre>\n<p>ファイルのありかは人それぞれですが、エラーは返ってきます。<br>\nデフォルトの<strong>Solverがlbfgs</strong>に変わっていたそうで、l2またはnoneしかペナルティをサポートしていないそうです。</p>\n<p>というわけで、Solverに<strong>liblinear</strong>を指定しないといけないようですね。</p>\n<h2>liblinearを指定する</h2>\n<p>先ほどのエラーをなくすため、新たにパラメータを足します。</p>\n<table>\n<thead>\n<tr>\n<th>パラメータ</th>\n<th>値</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>C(正則化のつよさ)</td>\n<td>1(デフォルト)</td>\n</tr>\n<tr>\n<td>penalty</td>\n<td>L1正則化</td>\n</tr>\n<tr>\n<td>max_iter(イテレーションの上限)</td>\n<td>100000</td>\n</tr>\n<tr>\n<td>solver</td>\n<td>liblinear</td>\n</tr>\n</tbody>\n</table>\n<p>では足します。</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> sklearn.linear_model <span class=\"hljs-keyword\">import</span> LogisticRegression\n\nlr_l1 = LogisticRegression(penalty=<span class=\"hljs-string\">'l1'</span>, solver=<span class=\"hljs-string\">'liblinear'</span>, max_iter=<span class=\"hljs-number\">100000</span>).fit(X_train, y_train)\n</code></pre>\n<p>これで無事動くようになりました!!!</p>\n<h2>まとめ</h2>\n<p>まとめです。<br>\nscikit-learnを用いて、ロジスティック回帰を使う時、さらにL1正則化をかけたい時は**solver='liblinear'**を引数に追加しましょう。</p>\n<p>この周りは色々と変化が早いので、本を買う際にも初版等も確認しつつ買った方が良い気がしました。バージョン確認も大切に。</p>","Title":"ロジスティック回帰でL1正則化を利用できない問題の解決法","Date":"2020-08-01","Category":"Python","Tags":["ML","Python","Scikit-learn"],"Authors":"ゆうぼう","Slug":"logistic-with-l1","Thumbnail":"/images/thumbnails/network.jpg","Description":"ロジスティック回帰をscikit-learnで実装していると、デフォルトはL2正則化でペナルティを与えています。そこで、もっとスパースにしてやろうとL1正則化を行おうとしたのだが、エラーを吐かれた。その時の解決策を共有します。","Published":true},{"contentHtml":"<p>一応参考書通りに学習するのだが、基本的にはいつも最新版をインストールして使う人間なもので、Warning及びErrorとの戦いはよくあることです。ので、Warningとかが出るとうっと身構えてしまうので、困らないように備忘録かつ反面教師として残しておきます。</p>\n<h2>とりあえずmake_forge()をやってみようか</h2>\n<p>とりあえずmake_forge()でforgeデータを生成し、Warningさせてみますかね。。。</p>\n<p>ライブラリはmglearnを使うのでpip環境がある人は、<strong>pip install mglearn</strong>をしてください。その上で、</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> mglearn    <span class=\"hljs-comment\"># mglearnのインポート</span>\nX, y = mglearn.datasets.make_forge()    <span class=\"hljs-comment\"># make_forgeメソッドで生成</span>\n</code></pre>\n<p>以下がアウトプットです。</p>\n<pre><code class=\"hljs language-bash\">/Users/user/opt/anaconda3/envs/ML/lib/python3.6/site-packages/sklearn/utils/deprecation.py:86: FutureWarning: Function make_blobs is deprecated; Please import make_blobs directly from scikit-learn\nwarnings.warn(msg, category=FutureWarning)\n</code></pre>\n<p>なんだかWarningで怒られましたorz</p>\n<p>将来的にsklearn.datasets.makeblobs()と被るよってことを言いたいらしいです。<em>scikit-learnから直接make_blobsをインポートしろ</em>とか言ってますね。\n次からWarningを避けて行きます。</p>\n<h2>make_blobsに乗り換える</h2>\n<p>make_blobsメソッドに乗り換えて行きます。これはscikit-learnのdatasetsモジュールに含まれているみたいなので、こいつをインポートして行きます。</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> sklearn.datasets <span class=\"hljs-keyword\">import</span> make_blobs    <span class=\"hljs-comment\"># これでインポート完了</span>\n</code></pre>\n<p>インポートがうまくいったら次はメソッドを呼び出します。</p>\n<pre><code class=\"hljs language-python\">X, y = make_blobs()\n</code></pre>\n<p>これでデータがうまく生成されたようです。無事Warningも出てきません!!!\n念のため、Xの型をみて行きます。</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"X.shape: {}\"</span>.<span class=\"hljs-built_in\">format</span>(X.shape))\n\n<span class=\"hljs-comment\"># -> X.shape: (100, 2)</span>\n</code></pre>\n<p>詰まるところ、2つの特徴量を持つデータが100個生成されました。100*2の行列ですね。\nmake_forge()のときは2つの特徴量のデータが26個を期待していたようなので、データ量が増えたみたいですね。</p>\n<h2>まとめ</h2>\n<p>mglearn.datasets.make_forge()でWarningを回避する方法のまとめがこちらです。</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> sklearn.datasets <span class=\"hljs-keyword\">import</span> make_blobs    <span class=\"hljs-comment\"># インポート</span>\nX, y = make_blobs()    <span class=\"hljs-comment\"># make_blobsメソッドの実行</span>\n\n<span class=\"hljs-string\">\"\"\"\n確認のおまけ\n\"\"\"</span>\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"X.shape: {}\"</span>.<span class=\"hljs-built_in\">format</span>(X.shape))\n\n<span class=\"hljs-comment\">#-> X.shape: (100, 2)    # 特徴量を2つ持つ100個のデータ</span>\n</code></pre>\n<p>scikit-learn周りは、アップデートが早いので少し古い技術書なだけでも、Future Warningが出たり、Errorが出たりすることが多々あります。\n気をつけながら学習をする必要がありそうですね。では、今回はここまで！</p>","Title":"scikit-learnのmake_blobsに乗り換えよう!!","Date":"2020-07-21","Category":"Python","Tags":["ML","Python","Scikit-learn"],"Authors":"ゆうぼう","Slug":"prefer-to-make_blobs","Thumbnail":"/images/thumbnails/python.jpg","Description":"Pythonではじめる機械学習をやっている最中に、mglearnというライブラリからmake_forge()メソッドでデータを生成することがあったのですが、Warningが出て怒られたので、推奨される形に戻すために互換性のあるコードに直します。","Published":true},{"contentHtml":"<h2>コーパスとは</h2>\n<p>**コーパス(corpus)**とは、集めた文書のことをいいます。</p>\n<p>もともとの原義としては、ある主題とかある作者に関する文書を集めたものがコーパスと呼ばれていたそうです。</p>\n<p>現在はもう少し広義で捉えられ、<strong>文書や音声を集めたデータそのもの、あるいはデータに情報を付与して加工したもの</strong>を総称してコーパスというそうです。</p>\n<p>最近の自然言語処理のタスクの進展は、このコーパスに活用による部分が多いです。</p>\n<h2>生コーパス(raw corpus)</h2>\n<p>前のセクションで話したように、コーパスには加工を加えたものと、そのまま生データのものと二通り考えられました。</p>\n<p>そこで、生データのままの文章や音声を「<strong>生コーパス(raw corpus)</strong>」と呼ぶことができます。</p>\n<h2>翻訳に関するコーパスの分類</h2>\n<p>生コーパスの中でも、その種はいくつか存在します。\nこのトピックでは機械翻訳で扱われるようなコーパスの分類についてお話します。</p>\n<h2>#対訳コーパス／パラレルコーパス</h2>\n<p>まずは、「<strong>対訳コーパス(bilingual corpus)</strong>」または「<strong>パラレルコーパス(parallel corpus)</strong>」です。</p>\n<p>この対訳コーパスとは、翻訳関係にある2言語の文書対を収集した生コーパスになります。このコーパスは、非常に貴重ではありますが、なかなか入手しにくい希少なデータです。しかし、この対訳コーパスは機械翻訳においてとても重要な知識源となっていることは確かです。</p>\n<h2>#コンパラブルコーパス</h2>\n<p>対訳コーパスでは、希少なコーパスであったのに対して、  「<strong>コンパラブルコーパス(comparable corpus)</strong>」は、しっかりとした対訳関係にないにしても、同じトピックに関して2言語の文書対のコーパスです。</p>\n<p>コンパラブルコーパスは、対訳コーパスほどきっちりとした対訳が制約されないので、このような文書は大量に存在します。これらの文書を収集したものがコンパラブルコーパスです。</p>\n<p>例としてわかりやすいのは、Wikipedeaでしょう。  Wikipediaでは言語リンクでつながった複数の言語でのページが存在します。これらの文書対ではきっちりとした対訳は保証されませんが、大量のデータを入手することができ、これも極めて重要な知識源となります。</p>\n<h2>まとめ</h2>\n<p>以上がコーパスに関する簡単な説明でした。他にも均衡コーパス(balanced corpus)や注釈コーパス(annotated corpus)といった分類もあります。</p>\n<p>ここで抑えるべき重要なことは、コーパスとは広義で<strong>文書や音声を集めたデータそのもの、あるいはデータに情報を付与して加工したもの</strong>ということでしょう。</p>\n<p>今回は主にコーパスの説明とともに生コーパスについての説明をしていきました。実際に加工を加えたコーパスに関しては、また別の記事にしたいと思います!</p>","Title":"自然言語処理(NLP)のコーパスって何なん？","Date":"2020-07-11","Category":"ML","Tags":["ML","NLP"],"Authors":"ゆうぼう","Slug":"corpus","Thumbnail":"/images/thumbnails/network.jpg","Description":"自然言語処理という機械学習のタスクにおいて「コーパス」という言葉が出てきます。そのコーパスについてお話をしていきます。","Published":true}],"tag":"ML","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"],"pages":1,"page":1},"__N_SSG":true}