{"pageProps":{"postData":{"contentHtml":"<p>Colab Proでなんとかpytorch-lightningを使ってTPUで実験を回そうと奮闘した結果，得られたベストプラクティス?(怪しい)の共有です．</p>\n<p>もっといい方法があったら教えてください．</p>\n<p>ちなみに，現行コンペで動作確認したので，コード全容の公開はないです．すみません．</p>\n<p><strong>Runtimeは必ずTPUにしてください</strong></p>\n<h2>pytorch-lightningでTPUを使いたいメリット?</h2>\n<p>黙って<em>tensorflow</em>を勉強して書けば，それでいいのでしょうが，<br>\n普段<em>pytorch</em>を使ってコーディングをしていて，僕の場合<em>pytorch-lightning</em>を好んで使っているので，その延長戦でTPUを使いたいという欲がありました．</p>\n<p>また，<em>pytorch-lightning</em>は，<strong>Trainerクラス</strong>をちょろっと書き換えるだけで，CPU/GPU/TPUの変換ができ，マルチGPU等のDDPの処理も自動で書き加えてくれるため，<br>\nかなり良いものかなと思っています．</p>\n<p>CPU/GPU/TPUの書き換えは基本的に以下で十分です．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> pytorch_lightning <span class=\"hljs-keyword\">as</span> pl\n<span class=\"hljs-comment\"># これらは共通</span>\nmodel = LitModel()  <span class=\"hljs-comment\"># pl.LightningModuleを継承した何か</span>\ndm = LitDataset()  <span class=\"hljs-comment\"># pl.LightningDataModuleを継承した何か</span>\n\n<span class=\"hljs-comment\"># CPUを使う</span>\ntrainer = Trainer()\ntrainer.fit(model, dm)\n\n<span class=\"hljs-comment\"># GPUを使う</span>\ntrainer = Trainer(gpus=-<span class=\"hljs-number\">1</span>)  <span class=\"hljs-comment\"># ありったけのGPU使う</span>\ntrainer.fit(model, dm)\n\n<span class=\"hljs-comment\"># Single Core TPUを使う</span>\ntrainer = Trainer(tpu_cores=[<span class=\"hljs-number\">5</span>])  <span class=\"hljs-comment\"># TPUのインデックス指定</span>\ntrainer.fit(model, dm)\n\n<span class=\"hljs-comment\"># Multi Core TPUを使う</span>\ntrainer = Trainer(tpu_cores=<span class=\"hljs-number\">8</span>)  <span class=\"hljs-comment\"># 1 or 8じゃないと怒られます...</span>\ntrainer.fit(model, dm)\n</code></pre>\n<p>ああ，楽だ楽だ．</p>\n<p>TPU簡単に動いてくれたらいいのに(願望)</p>\n<h2>TPUを使うためのセットアップ</h2>\n<p>以下を実行して，必要なライブラリ群を入れてしまいます．</p>\n<p>一応僕が動作確認取れたのは，pytorch-xlaのバージョンが<em>1.8</em>と<em>1.9</em>です．</p>\n<p>Circle CIのTPU環境に合わせたユニットテストは，確か1.8が通っていて，1.9がエラーを吐いていた気がします．</p>\n<p>1.8の方が安全なのですかね？(わかりません)</p>\n<p><em>pytorch-xla 1.8</em></p>\n<pre><code class=\"hljs language-python\">!pip install cloud-tpu-client==<span class=\"hljs-number\">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class=\"hljs-number\">1.8</span>-cp37-cp37m-linux_x86_64.whl\n!pip install -q torch==<span class=\"hljs-number\">1.8</span> torchvision torchtext\n!pip install -q pytorch-lightning==<span class=\"hljs-number\">1.4</span><span class=\"hljs-number\">.9</span> torchmetrics\n</code></pre>\n<p><em>pytorch-xla 1.9</em></p>\n<pre><code class=\"hljs language-python\">!pip install cloud-tpu-client==<span class=\"hljs-number\">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class=\"hljs-number\">1.9</span>-cp37-cp37m-linux_x86_64.whl\n!pip install -q torch==<span class=\"hljs-number\">1.9</span> torchvision torchtext\n!pip install -q pytorch-lightning==<span class=\"hljs-number\">1.4</span><span class=\"hljs-number\">.9</span> torchmetrics\n</code></pre>\n<p>pipでのインストール分割していたり，--quit(-q)をつけているのは特に理由はないです．</p>\n<p>その辺はお好みでやってください．</p>\n<p>一応僕の環境ではこんな感じで，以後のプログラムはうまいこと動作しました．</p>\n<p>おそらく以下のようなエラーは起きますが，特に動作に影響はなさそうでした．あまり不安がらなくても良さそうです．</p>\n<pre><code class=\"hljs language-shell\">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nearthengine-api 0.1.284 requires google-api-python-client&#x3C;2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\n</code></pre>\n<h2>セットアップ後にimport pytorch_lightning</h2>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> pytorch_lightning <span class=\"hljs-keyword\">as</span> pl\n</code></pre>\n<p><em>import</em>すると以下のような出力を吐きます．</p>\n<p>インストールした<em>xla</em>のバージョンが1.9ならば，以下のようなメッセージが出ます．</p>\n<p>少し時間がかかりますが，待ちましょう．</p>\n<p><em>pytorch-xla 1.9</em></p>\n<pre><code class=\"hljs language-shell\">WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\nWARNING:root:TPU has started up successfully with version pytorch-1.9\n</code></pre>\n<p>もし，<em>TPU not Found</em>的なことが出たら，一回<em>Factory reset Runtime</em>して，またインストールからやり直せばうまく行くと思います．</p>\n<h2>importまでうまく行けたら</h2>\n<p><em>import</em>するまで行けたら，このあとは特に問題なく動くはずです．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># ModelとかDataModuleとかは適宜設定してください．</span>\n\ntrainer = Trainer(tpu_cores=<span class=\"hljs-number\">8</span>)\ntrainer.fit(model, dm)\n</code></pre>\n<p>このあとすぐエラー出なきゃ，勝手に動くと思います(雑ww)</p>\n<h2>注意するべきこと</h2>\n<p>幾つか僕が遭遇したエラーを挙げておきます．</p>\n<ul>\n<li>wandb loggerが使えない\n<ul>\n<li>多分Commet loggerも使えない？</li>\n</ul>\n</li>\n<li>マルチコアTPU使うなら，predict時return_predictionsができない\n<ul>\n<li><em>pytorch_lightning.callbacks.BasePredictionWriter</em>を使おう(<a href=\"#BasePredictionWriter%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB\">サンプル</a>)</li>\n</ul>\n</li>\n<li>なぜか知らんけど，<a href=\"#RAM%E3%81%AE%E4%BD%BF%E7%94%A8%E7%8E%8750%25%E3%81%8F%E3%82%89%E3%81%84%E3%81%AB%E3%81%AA%E3%82%8B%E3%81%A8stack%E3%81%99%E3%82%8B%E4%BB%B6%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><strong>RAMの使用率が50%あたりになるとstackする</strong></a>\n<ul>\n<li>こうなると一生動かなくなるので，<strong>最初安定するまで監視が必要</strong></li>\n<li>Colabの下のデバッガみたいなやつが，ずっと*select() > spawn()*的な非同期のところで止まるので，そうなったらRAMチェックしよう!!!</li>\n</ul>\n</li>\n</ul>\n<h2>BasePredictionWriterのサンプル</h2>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> pytorch_lightning.callbacks <span class=\"hljs-keyword\">import</span> BasePredictionWriter\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CustomWriter</span>(<span class=\"hljs-title class_ inherited__\">BasePredictionWriter</span>):\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, output_dir: <span class=\"hljs-built_in\">str</span>, write_interval: <span class=\"hljs-built_in\">str</span> = <span class=\"hljs-string\">'batch'</span></span>):\n        <span class=\"hljs-built_in\">super</span>().__init__(write_interval)\n        self.output_dir = output_dir\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">write_on_batch_end</span>(<span class=\"hljs-params\">\n        self, trainer, pl_module: <span class=\"hljs-string\">'LightningModule'</span>, prediction: <span class=\"hljs-type\">Any</span>, batch_indices: <span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">int</span>], batch: <span class=\"hljs-type\">Any</span>,\n        batch_idx: <span class=\"hljs-built_in\">int</span>, dataloader_idx: <span class=\"hljs-built_in\">int</span>\n    </span>):\n        torch.save(prediction, os.path.join(self.output_dir, dataloader_idx, <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{batch_idx}</span>.pt\"</span>))\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">write_on_epoch_end</span>(<span class=\"hljs-params\">\n        self, trainer, pl_module: <span class=\"hljs-string\">'LightningModule'</span>, predictions: <span class=\"hljs-type\">List</span>[<span class=\"hljs-type\">Any</span>], batch_indices: <span class=\"hljs-type\">List</span>[<span class=\"hljs-type\">Any</span>]\n    </span>):\n        torch.save(predictions, os.path.join(self.output_dir, <span class=\"hljs-string\">\"predictions.pt\"</span>))\n</code></pre>\n<h2>RAMの使用率50%くらいになるとstackする件について</h2>\n<p>はっきり言って，これは本当に原因がわかりません．</p>\n<p>50%くらいで張りついちゃうので，batch_size下げようが，RAMの使用率はあまり変わる気がしません．</p>\n<p>こうなってstackしちゃったら，もうHigh-RAM設定にしてRAMにゆとりを持たせた方がいいと思います．潔く．</p>\n<p>High-RAMでも張りついちゃう場合は，僕はわかりません．</p>\n<p>RAM周りの最適化とか知見ある方，解決策わかったら共有してくださるととてもありがたいです......</p>\n<h2>まとめ</h2>\n<p>まとめじゃないです．嘘つきました．</p>\n<p>でも，まとめないとなんかあれなので，CPU/GPU/TPUで使いまわせるように，セットアップの部分整理します．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> sys\n\n<span class=\"hljs-keyword\">if</span> os.environ.get(<span class=\"hljs-string\">'COLAB_TPU_ADDR'</span>, <span class=\"hljs-literal\">False</span>):\n    !pip install cloud-tpu-client==<span class=\"hljs-number\">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class=\"hljs-number\">1.8</span>-cp37-cp37m-linux_x86_64.whl\n    !pip install -q torch==<span class=\"hljs-number\">1.8</span> torchvision torchtext\n\n!pip install pytorch-lightning==<span class=\"hljs-number\">1.4</span><span class=\"hljs-number\">.9</span> torchmetrics\n</code></pre>\n<p>もし，KaggleとColabでも行けるようにしたいのであれば，<code>'google.colab' in sys.modules</code>でやるといいと思います．</p>\n<p>Colab TPU使って，pytorch-lightningでうまく実験回しまくっている方いたら，色々意見ください．お願いします :)</p>\n<p>知見たまって，エラーハンドリングとかわかってきたら，この記事更新するか，新たに書き始めます．よろしくお願いします．</p>","Title":"Pytorch LightningでTPUを回す on Colab Pro in 2021","Date":"2021-10-11","Category":"Python","Tags":["pytorch-lightning","Colab"],"Authors":"ゆうぼう","Slug":"pytorch-lightning_on_colab-tpu_2021","Thumbnail":"/images/thumbnails/pytorch-lightning_on_colab-pro-tpu.png","Description":"Colab Proでなんとかpytorch-lightningを使ってTPUで実験を回そうと奮闘した結果，得られたベストプラクティスの共有です．もっといい方法があったら教えてください．","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"]},"__N_SSG":true}