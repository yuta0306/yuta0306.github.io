{"pageProps":{"postData":{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Prediction of Shared Laughter for Human-Robot Dialogue</p>\n<p>研究会: ICMI</p>\n<p>年度: 2020</p>\n<p>キーワード: laughter, shared laughter</p>\n<p>URL: <a href=\"http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI20.pdf\">http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/LAL-ICMI20.pdf</a></p>\n<p>DOI: <a href=\"https://doi.org/10.1145/3395035.3425265\">https://doi.org/10.1145/3395035.3425265</a></p>\n<p>データセット:</p>\n<h2>提案手法</h2>\n<p>会話ロボットがshared laughterを自動生成することを目的にする</p>\n<h3>Shared Laughter Model</h3>\n<p><img src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/ktkpx7fr.png\" alt=\"\"></p>\n<p>ユーザの最初の笑いを検知して，システムが笑うモデル</p>\n<p>3種類のモジュールが存在する</p>\n<ol>\n<li>ユーザの最初の笑いを検出</li>\n<li>shared laughterを生成するかどうかを決定</li>\n<li>どのタイプの笑いをするべきか決定</li>\n</ol>\n<h3>Data Collection and Analysis</h3>\n<p>収集方法：ERICA</p>\n<p>teleoperateしたのは女性</p>\n<p>対象：61人の男性</p>\n<p>シナリオ：speed dating</p>\n<p>好きな趣味，好きなこと，嫌いなことに関してカジュアルなチャット</p>\n<p>アノテーション：</p>\n<p>2種類のタイプに笑いを分けた</p>\n<ol>\n<li>isolated laugh\n1. 笑い単体で起こる笑い</li>\n<li>speech laugh\n1. 話しながら起こる笑い\n<img src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/ztqx5kie.png\" alt=\"\"></li>\n</ol>\n<p><img src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/pncunm6h.png\" alt=\"\"></p>\n<p>集まったデータの中で，1206件のinitial laughが確認</p>\n<p>698件がself laughで508件がshared laugh</p>\n<h3>Model Creation</h3>\n<p>特徴量2種類：</p>\n<p><strong>Audio-based features</strong></p>\n<p>40のacoustic メルフィルタバンクの平均と標準偏差</p>\n<p><strong>Prosodic features</strong></p>\n<p>全IPUにまたがるピッチとパワーの値を使った合計で14つの以下の指標</p>\n<p>平均／中央値／標準偏差／最大値／最小値／範囲</p>\n<p><strong>モデル</strong>：</p>\n<p>LR／SVM</p>\n<p>データサンプルが小さかったからか，deep learningの手法は弱かった</p>\n<h2>新規性</h2>\n<p>ユーザに合わせて毎回笑いを生成するのではなく，適切なタイミングでshared laughterを自動生成することをロボットに持たせたいという目的をもった研究</p>\n<h2>評価方法</h2>\n<p><img src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/1mjd5yfi.png\" alt=\"\"></p>\n<p>オフラインとオンラインの二つのタイプで評価</p>\n<p><img src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/my7ehxxr.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue/5m9f1ed7.png\" alt=\"\"></p>\n<p>特徴量としてlaughter typeを加えることでrecallが改善する傾向</p>\n<p>負に歪んだピッチの分布の時は笑いがシェアされがちで笑いが長く続きがち</p>\n<p>laughter detectionとlaugh type classificationのエラーによってパフォーマンスが落ちることは明らかだが，それでもベースラインをoutperform</p>\n<p>最もよかったonline modelはacousticとprosodicの特徴量を両方使ったもの</p>\n<h2>何がすごかった？</h2>\n<p>prosodicの分析からわかったことが，initial laughのacousiticsはresponse laughを呼び起こすいくつかの特徴があること</p>\n<p>まだ改善の余地はあるものの，acousticとprosodicの特徴量の両方を使うことはパフォーマンスの改善に役立った</p>\n<p>shared laughterのタイミングについてはかけた研究である</p>\n<p>今後の課題である</p>\n<h2>次に読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n<p><em>@inproceedings{10.1145/3395035.3425265,\nauthor = {Lala, Divesh and Inoue, Koji and Kawahara, Tatsuya},\ntitle = {Prediction of Shared Laughter for Human-Robot Dialogue},\nyear = {2020},\nisbn = {9781450380027},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {<a href=\"https://doi.org/10.1145/3395035.3425265%7D\">https://doi.org/10.1145/3395035.3425265}</a>,\ndoi = {10.1145/3395035.3425265},\nabstract = {Shared laughter is a phenomenon in face-to-face human dialogue which increases engagement and rapport, and so should be considered for conversation robots and agents. Our aim is to create a model of shared laughter generation for conversational robots. As part of this system, we train models which predict if shared laughter will occur, given that the user has laughed. Models trained using combinations of acoustic, prosodic features and laughter type were compared with online versions considered to better quantify their performance in a real system. We find that these models perform better than the random chance, with the multimodal combination of acoustic and prosodic features performing the best.},\nbooktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},\npages = {62–66},\nnumpages = {5},\nkeywords = {shared laughter, machine learning, human-robot dialogue, conversation},\nlocation = {Virtual Event, Netherlands},\nseries = {ICMI '20 Companion}\n}</em></p>\n</blockquote>","Title":"【論文まとめ】Prediction of Shared Laughter for Human-Robot Dialogue","Date":"2023-05-21","Category":"論文","Tags":["laughter","shared laughter"],"Authos":"ゆうぼう","Slug":"Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue","Thumbnail":"/images/thumbnails/Prediction-of-Shared-Laughter-for-Human-Robot-Dialogue.png","Description":"Prediction of Shared Laughter for Human-Robot Dialogueのまとめ","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","超解像"]},"__N_SSG":true}