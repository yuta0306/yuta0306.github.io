{"pageProps":{"TaggedPostData":[{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</p>\n<p>研究会: ACL</p>\n<p>年度: 2020</p>\n<p>キーワード: transformer, Highway Transformer, Gating Mechanism, Self-Dependency-Units (SDU)</p>\n<p>URL: <a href=\"https://aclanthology.org/2020.acl-main.616.pdf\">https://aclanthology.org/2020.acl-main.616.pdf</a></p>\n<p>DOI: <a href=\"http://dx.doi.org/10.18653/v1/2020.acl-main.616\">http://dx.doi.org/10.18653/v1/2020.acl-main.616</a></p>\n<p>コード: <a href=\"https://github.com/cyk1337/Highway-Transformer\">https://github.com/cyk1337/Highway-Transformer</a></p>\n<p>データセット: Penn Tree Bank (PTB), enwik8</p>\n<h2>概要</h2>\n<p>LSTM-styleなSDUを提案</p>\n<p>ゲートとしてSDUをTransformer内部に適用することにより，ハイパラをチューニングすることなく，Transformerの浅い層において，内在的な意味の重要性を捉え，より早い収束を可能に</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/qrpajdel.png\" alt=\"\"></p>\n<h3>Self-Dependency Units (SDU)</h3>\n<p>sigmoid gatesを導入する</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ψ</mi></mrow><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Ψ</span></span></span></span></span>はゲートとして作用し，logistic sigmoidもしくはtanhで実現</p>\n<p>筆者らの認識</p>\n<p>tanhはupdate gateとして作用し，重要度の幅を-1 to 1に制限</p>\n<p>sigmoidはLSTMのinput gateと似ていて，feature-wise levelでどれくらいの情報を残すか決定</p>\n<h3>Pseudo-highway Connection</h3>\n<p>residual connectionされたgating-modified encodingsでMulti Head Dot Product Attention (MHDPA)の分散表現を豊かにするため，新たな計算グラフの枝を追加し，SDUとIdentityとMHDPAをpost LNを使用してresidual connectionする</p>\n<h2>新規性</h2>\n<p>本来，人にとって，読み物をよりよく理解するためには，global contextだけではなく，ここの単語の意味も必要</p>\n<p>→ Self-gatingなアプローチを提案</p>\n<ol>\n<li>Transformerにおける浅い層において，trainingとvalidationでハイパラチューニングすることなく，より高速な収束を達成</li>\n<li>Transformerでの低レイヤーにおいて，local-range encodingにフォーカスした層を実現</li>\n<li>Self-gating mechanismは，R-TransformerやTransformer-XLのコンポーネントとしてRNN-likeなメカニズムを補完</li>\n</ol>\n<h2>実験</h2>\n<p>SDUを導入し，PTBデータセットにおけるSDUの効果を検証</p>\n<p>sigmoidとtanhを実験</p>\n<h2>まとめ</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/op874d4u.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/e99q8luh.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4xk5l3fv.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/gen0ole9.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/bjrucnwe.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4fv3x3v3.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/c6hfqjx9.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/aptzk9jc.png\" alt=\"\"></p>\n<p>sigmoidによるSDUが安定しているが，データとタスクによってはtanhの方がoutperformすることがある</p>\n<p>いずれのactivationを使っても収束は早い</p>\n<p>enwik8による大規模データでの追実験において，提案手法が浅いレイヤーには寄与することが確かめられた</p>\n<p>SDUで計算量が増えるが，そこまで差はなかった</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n<p>@inproceedings{chai-etal-2020-highway,</p>\n</blockquote>\n<pre><code class=\"hljs language-ini\"><span class=\"hljs-attr\">title</span> = <span class=\"hljs-string\">\"Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\"</span>,\n<span class=\"hljs-attr\">author</span> = <span class=\"hljs-string\">\"Chai, Yekun  and\n  Jin, Shuo  and\n  Hou, Xinwen\"</span>,\n<span class=\"hljs-attr\">booktitle</span> = <span class=\"hljs-string\">\"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\"</span>,\n<span class=\"hljs-attr\">month</span> = jul,\n<span class=\"hljs-attr\">year</span> = <span class=\"hljs-string\">\"2020\"</span>,\n<span class=\"hljs-attr\">address</span> = <span class=\"hljs-string\">\"Online\"</span>,\n<span class=\"hljs-attr\">publisher</span> = <span class=\"hljs-string\">\"Association for Computational Linguistics\"</span>,\n<span class=\"hljs-attr\">url</span> = <span class=\"hljs-string\">\"https://aclanthology.org/2020.acl-main.616\"</span>,\n<span class=\"hljs-attr\">doi</span> = <span class=\"hljs-string\">\"10.18653/v1/2020.acl-main.616\"</span>,\n<span class=\"hljs-attr\">pages</span> = <span class=\"hljs-string\">\"6887--6900\"</span>,\n<span class=\"hljs-attr\">abstract</span> = <span class=\"hljs-string\">\"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.\"</span>,\n</code></pre>\n<p>}</p>\n</body>\n</html>\n","Title":"【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks","Date":"2023-05-21","Category":"論文","Tags":["transformer","Highway Transformer","Gating Mechanism","Self-Dependency-Units (SDU)"],"Authos":"ゆうぼう","Slug":"Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks","Thumbnail":"/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png","Description":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ","Published":true}],"tag":"Gating Mechanism","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","CSS","dialogue system","DST","empathetic dialogue system","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true}