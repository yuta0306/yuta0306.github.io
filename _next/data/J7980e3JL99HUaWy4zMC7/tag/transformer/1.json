{"pageProps":{"TaggedPostData":[{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Multi-Hop Transformer for Document-Level Machine Translation</p>\n<p>研究会: NAACL</p>\n<p>年度: 2021</p>\n<p>キーワード: MT, transformer, Multi-Hop Transformer</p>\n<p>URL: <a href=\"https://aclanthology.org/2021.naacl-main.309.pdf\">https://aclanthology.org/2021.naacl-main.309.pdf</a></p>\n<p>DOI: <a href=\"http://dx.doi.org/10.18653/v1/2021.naacl-main.309\">http://dx.doi.org/10.18653/v1/2021.naacl-main.309</a></p>\n<p>データセット: TED Talk, OpenSubtitles, Europarl7</p>\n<h2>概要</h2>\n<p>Document-level neural machine translationにおいて，Multi-Hopなアーキテクチャを導入することにより，従来手法と比べて精度の高い文脈を考慮した機械翻訳を実現</p>\n<p>翻訳者のように，頭の中に翻訳のドラフトを作り，文脈に合わせて適切に修正する流れ（human-like draft-editing）を明示的にモデリング</p>\n<p>大きな事前学習済みモデルを使うことなく，使用に足る機械翻訳モデルを実現</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/p7s4t8vi.png\" alt=\"\"></p>\n<p>アーキテクチャ周りのこと</p>\n<h3>Sentence Encoder</h3>\n<p>source-sideとtarget-sideでそれぞれPretrained Encoderがあり，source contextとtarget draftの分散表現をそれぞれ得る</p>\n<h3>Multi-Hop Encoder</h3>\n<p>source-contextにおいて文章ごとのreasoningをして，現在の文章の分散表現を得る</p>\n<h3>Multi-Hop Decoder</h3>\n<p>target-side draftから情報を取得して，翻訳の確率分布を得る</p>\n<p>そのほかアーキテクチャの工夫</p>\n<h3>Contet Gating</h3>\n<p>contextual informationを過剰にutilizeしすぎないように，context gating machanismを採用</p>\n<p>contextと現在の文章間の重みを動的にコントロールする</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>a</mi></msub><msubsup><mi>A</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>+</mo><msub><mi>W</mi><mi>b</mi></msub><msubsup><mi>B</mi><mrow><mi>s</mi><mo>−</mo><mi>i</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\alpha = \\sigma(W_a A_s^{(n)} + W_b B_{s-i}^{(n)})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2948em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">a</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.5834em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">n</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1166em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.38em;vertical-align:-0.3352em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">b</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231em;margin-left:-0.0502em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\">i</span></span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">n</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3352em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span> where sigma is logistic sigmoid function</p>\n<h2>新規性</h2>\n<p>Docment-level NMTにおける従来手法の問題点</p>\n<ol>\n<li>文章間のreasoningの特徴づけを明示的に行うことなく，単純にcontextの分散表現を導入</li>\n<li>推論時にはアクセスできないのに，訓練時には追加入力としてのtarget contextにground-truthなデータを入力\n↑　訓練時と推論時において状況が異なる</li>\n</ol>\n<p>Document-level NMTにおいてMulti-Hop reasoningをモデリングしたMulti-Hop Transformerの提案と提案モデルによるDocument-level NMTの大きな性能改善</p>\n<p>target contextにground-truthで訓練すると推論時にはアクセスできないため，他の翻訳モデルの翻訳結果を使用することで，訓練時と推論時の状況を同じにした</p>\n<h2>実験</h2>\n<p>Baseline</p>\n<p>Transformer</p>\n<p>CA-Transformer</p>\n<p>CA-HAN</p>\n<p>CADec</p>\n<p>計算量のオーバーヘッドを改善するためSentence Encoderはそれぞれのsideでパラメータを共有</p>\n<h2>まとめ</h2>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/am19iape.png\" alt=\"\"></p>\n<p>large-scaleな事前学習済み言語モデルを使用することなく，SoTA翻訳クオリティを達成</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/z4jm21k3.png\" alt=\"\"></p>\n<p>contextを付与するためのAttentionの構造は，ConcatやHierarchicalよりもMulti-HopなAttentionが効果があり</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/tito19hb.png\" alt=\"\"></p>\n<p>contextを考慮する幅のwindow sizeは大きくするほど効果が上がるわけではなく．3が最も良かった</p>\n<p>4以上にすると悪化傾向らしく，本研究ではwindow size = 3 を採用</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/pc76zuqn.png\" alt=\"\"></p>\n<p>contextにおいてreasoningするときの方向は，一般的な読み順の通りleft-to-rightで順方向にreasoningさせた方が結果は良かった</p>\n<p><img src=\"/images/article/Multi-Hop-Transformer-for-Document-Level-Machine-Translation/bx5z85gj.png\" alt=\"\"></p>\n<p>訓練時と推論時にtarget draftに与える文章が異なる問題への対処に関する実験結果</p>\n<p>Referenceはground-truthをtarget draftとして与えて訓練，Draftはpre-trained MT systemが生成した翻訳結果をtarget draftとして与えて訓練したモデル</p>\n<p>Draftの方が結果がよく，pre-trained MT systemの生成結果をtarget draftとする方法によって訓練時と推論時のギャップの橋渡しになることを示唆する結果</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<p><a href=\"/5955ca444629476ebf23e66629a2413f\">Context-Aware Self-Attention Networks</a></p>\n<h2>引用</h2>\n<blockquote>\n<p>@inproceedings{zhang-etal-2021-multi,</p>\n</blockquote>\n<p>title = \"Multi-Hop Transformer for Document-Level Machine Translation\",\nauthor = \"Zhang, Long and\nZhang, Tong and\nZhang, Haibo and\nYang, Baosong and\nYe, Wei and\nZhang, Shikun\",\nbooktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\nmonth = jun,\nyear = \"2021\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"<a href=\"https://aclanthology.org/2021.naacl-main.309\">https://aclanthology.org/2021.naacl-main.309</a>\",\ndoi = \"10.18653/v1/2021.naacl-main.309\",\npages = \"3953--3963\",\nabstract = \"Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior {--} human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.\",\n}</p>\n</body>\n</html>\n","Title":"【論文まとめ】Multi-Hop Transformer for Document-Level Machine Translation","Date":"2023-05-21","Category":"論文","Tags":["MT","transformer","Multi-Hop Transformer"],"Authos":"ゆうぼう","Slug":"Multi-Hop-Transformer-for-Document-Level-Machine-Translation","Thumbnail":"/images/thumbnails/Multi-Hop-Transformer-for-Document-Level-Machine-Translation.png","Description":"Multi-Hop Transformer for Document-Level Machine Translationのまとめ","Published":true},{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</p>\n<p>研究会: ACL</p>\n<p>年度: 2020</p>\n<p>キーワード: transformer, Highway Transformer, Gating Mechanism, Self-Dependency-Units (SDU)</p>\n<p>URL: <a href=\"https://aclanthology.org/2020.acl-main.616.pdf\">https://aclanthology.org/2020.acl-main.616.pdf</a></p>\n<p>DOI: <a href=\"http://dx.doi.org/10.18653/v1/2020.acl-main.616\">http://dx.doi.org/10.18653/v1/2020.acl-main.616</a></p>\n<p>コード: <a href=\"https://github.com/cyk1337/Highway-Transformer\">https://github.com/cyk1337/Highway-Transformer</a></p>\n<p>データセット: Penn Tree Bank (PTB), enwik8</p>\n<h2>概要</h2>\n<p>LSTM-styleなSDUを提案</p>\n<p>ゲートとしてSDUをTransformer内部に適用することにより，ハイパラをチューニングすることなく，Transformerの浅い層において，内在的な意味の重要性を捉え，より早い収束を可能に</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/qrpajdel.png\" alt=\"\"></p>\n<h3>Self-Dependency Units (SDU)</h3>\n<p>sigmoid gatesを導入する</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ψ</mi></mrow><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Ψ</span></span></span></span></span>はゲートとして作用し，logistic sigmoidもしくはtanhで実現</p>\n<p>筆者らの認識</p>\n<p>tanhはupdate gateとして作用し，重要度の幅を-1 to 1に制限</p>\n<p>sigmoidはLSTMのinput gateと似ていて，feature-wise levelでどれくらいの情報を残すか決定</p>\n<h3>Pseudo-highway Connection</h3>\n<p>residual connectionされたgating-modified encodingsでMulti Head Dot Product Attention (MHDPA)の分散表現を豊かにするため，新たな計算グラフの枝を追加し，SDUとIdentityとMHDPAをpost LNを使用してresidual connectionする</p>\n<h2>新規性</h2>\n<p>本来，人にとって，読み物をよりよく理解するためには，global contextだけではなく，ここの単語の意味も必要</p>\n<p>→ Self-gatingなアプローチを提案</p>\n<ol>\n<li>Transformerにおける浅い層において，trainingとvalidationでハイパラチューニングすることなく，より高速な収束を達成</li>\n<li>Transformerでの低レイヤーにおいて，local-range encodingにフォーカスした層を実現</li>\n<li>Self-gating mechanismは，R-TransformerやTransformer-XLのコンポーネントとしてRNN-likeなメカニズムを補完</li>\n</ol>\n<h2>実験</h2>\n<p>SDUを導入し，PTBデータセットにおけるSDUの効果を検証</p>\n<p>sigmoidとtanhを実験</p>\n<h2>まとめ</h2>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/op874d4u.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/e99q8luh.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4xk5l3fv.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/gen0ole9.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/bjrucnwe.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/4fv3x3v3.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/c6hfqjx9.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks/aptzk9jc.png\" alt=\"\"></p>\n<p>sigmoidによるSDUが安定しているが，データとタスクによってはtanhの方がoutperformすることがある</p>\n<p>いずれのactivationを使っても収束は早い</p>\n<p>enwik8による大規模データでの追実験において，提案手法が浅いレイヤーには寄与することが確かめられた</p>\n<p>SDUで計算量が増えるが，そこまで差はなかった</p>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n<p>@inproceedings{chai-etal-2020-highway,</p>\n</blockquote>\n<pre><code class=\"hljs language-ini\"><span class=\"hljs-attr\">title</span> = <span class=\"hljs-string\">\"Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\"</span>,\n<span class=\"hljs-attr\">author</span> = <span class=\"hljs-string\">\"Chai, Yekun  and\n  Jin, Shuo  and\n  Hou, Xinwen\"</span>,\n<span class=\"hljs-attr\">booktitle</span> = <span class=\"hljs-string\">\"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\"</span>,\n<span class=\"hljs-attr\">month</span> = jul,\n<span class=\"hljs-attr\">year</span> = <span class=\"hljs-string\">\"2020\"</span>,\n<span class=\"hljs-attr\">address</span> = <span class=\"hljs-string\">\"Online\"</span>,\n<span class=\"hljs-attr\">publisher</span> = <span class=\"hljs-string\">\"Association for Computational Linguistics\"</span>,\n<span class=\"hljs-attr\">url</span> = <span class=\"hljs-string\">\"https://aclanthology.org/2020.acl-main.616\"</span>,\n<span class=\"hljs-attr\">doi</span> = <span class=\"hljs-string\">\"10.18653/v1/2020.acl-main.616\"</span>,\n<span class=\"hljs-attr\">pages</span> = <span class=\"hljs-string\">\"6887--6900\"</span>,\n<span class=\"hljs-attr\">abstract</span> = <span class=\"hljs-string\">\"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.\"</span>,\n</code></pre>\n<p>}</p>\n</body>\n</html>\n","Title":"【論文まとめ】Highway Transformer: Self-Gating Enhanced Self-Attentive Networks","Date":"2023-05-21","Category":"論文","Tags":["transformer","Highway Transformer","Gating Mechanism","Self-Dependency-Units (SDU)"],"Authos":"ゆうぼう","Slug":"Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks","Thumbnail":"/images/thumbnails/Highway-Transformer-Self-Gating-Enhanced-Self-Attentive-Networks.png","Description":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networksのまとめ","Published":true}],"tag":"transformer","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","CSS","dialogue system","DST","empathetic dialogue system","encyclopedic","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","ML","MT","Multi-Hop Transformer","multi-modal","MySQL","NLI","NLP","Node","node.js","npm","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","subprocess","Super-Resolution","survey","tensorflow","Tkinter","transformer","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true}