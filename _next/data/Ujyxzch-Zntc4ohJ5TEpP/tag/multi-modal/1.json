{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</p>\n<p>研究会: EMNLP</p>\n<p>年度: 2023</p>\n<p>キーワード: LLM, multi-modal, Demo</p>\n<p>URL: <a href=\"https://arxiv.org/pdf/2306.02858.pdf\">https://arxiv.org/pdf/2306.02858.pdf</a></p>\n<p>DOI: <a href=\"https://doi.org/10.48550/arXiv.2306.02858\">https://doi.org/10.48550/arXiv.2306.02858</a></p>\n<p>コード: <a href=\"https://github.com/damo-nlp-sg/video-llama\">https://github.com/damo-nlp-sg/video-llama</a></p>\n<p>データセット: MSR-VTT, MSVD, VideoInstruct, ActivityNet-QA</p>\n<h2>概要</h2>\n<p>ビデオ中の動画と音声を理解できるVideo-LLaMAを提案</p>\n<p>Video Q-Former (BLIP2)とAudio Q-Former (Imagebind)を用いて，動画のシーン間の変化を捉えたり，audio-visualな情報を統合したりする</p>\n<p>Video-LLaMAは動画を理解して，ビデオ中の動画や音声に基づいた意味のある応答を生成できる</p>\n<p><a href=\"https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA\">デモ on huggingface</a></p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/sxpfrxf6.png\" alt=\"\"></p>\n<h3>Architecture</h3>\n<p>図の通り，Vision-Language BranchとAudio-Language Branchに分岐</p>\n<p><strong>Vision-Language Branch</strong></p>\n<ol>\n<li>各フレームをフリーズしたBLIP2に入力（EVA-CLIPのViT-G/14とpre-trained Q-Former）</li>\n<li>positional embeddingを適用</li>\n<li>Video Q-Former</li>\n<li>線形層</li>\n<li>LLMへ</li>\n</ol>\n<p><strong>Audio-Language Branch</strong></p>\n<ol>\n<li>2秒ごとに音声をクリップ</li>\n<li>各クリップ音声を128 binsのメルスペクトログラムに変換</li>\n<li>Imagebind (as Audio Encoder)</li>\n<li>Imagebindの出力に対して，learnableなpositional embeddingを加算</li>\n<li>Audio Q-former</li>\n<li>線形層</li>\n<li>LLMへ</li>\n</ol>\n<h3>Multi-branch Cross-modal Training</h3>\n<p><strong>Vision-Language Branchの学習</strong></p>\n<p>事前学習→インストラクションチューニング</p>\n<p>事前学習データセット：Webvid-2M，CC595k（CC3Mからフィルタされたもの）</p>\n<p>インストラクションデータセット：MIniGPT4，LLaVA，Video-Chat</p>\n<p>インストラクションチューニングすると，Video-LLaMAは良い能力を発揮</p>\n<p><strong>Audio-Language Branchの学習</strong></p>\n<p>audio-textなデータが少ないことが課題</p>\n<p>→<strong>異なるモダリティを同じ埋め込み空間にalignmentするImagebindをAudio Encoderとして用い，visual-textデータを使って学習</strong></p>\n<p>音声データで学習しないが，推論時は音声を理解することができる</p>\n<h2>新規性</h2>\n<ul>\n<li>与えられたビデオの動画と音声を同時に処理して，会話ができるVideo-LLaMAを提案</li>\n<li>vision-language alignmentとaudio-language alignmentの両方を達成するmulti-branch cross-modal pre-training frameworkを提案</li>\n</ul>\n<h2>Examples</h2>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/4pkwtgeu.png\" alt=\"\"></p>\n<p>動画と音声の両方を理解できている例</p>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/ke9lg4al.png\" alt=\"\"></p>\n<p>Temporal dynamicsを理解できている例</p>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/dkti6vzg.png\" alt=\"\"></p>\n<p>staticな画像を理解できている例</p>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/55eess6q.png\" alt=\"\"></p>\n<p>一般的な知識を示せている例</p>\n<p>chat形式の例は論文のappendixを参照</p>\n<h2>まとめ</h2>\n<p>動画と音声を理解できるVideo-LLaMAを提案</p>\n<p>Vision-Language BranchとAudio-Language Branchで分岐して，動画と音声を理解するアーキテクチャを提案し，Imagebindをvisual-textデータで学習することでaudio-textデータの少なさをカバー</p>\n<p>Hallcinationがあることや，映画やテレビのような長い動画を処理できないことがlimitaitions</p>\n<h2>その他</h2>\n<p><img src=\"/images/article/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding/tq9elvwf.png\" alt=\"\"></p>\n<p>ポピュラーなマルチモーダルLLM</p>\n<p><strong>所感</strong></p>\n<p>Imagebindで同じ埋め込み空間に異なるモダリティの埋め込みを押し込んでいるのを利用して，audio-textデータの少なさをカバーしているのが上手いのだろうが，それでうまくいくことにちょっと気持ち悪さが残った（個人的に，大規模なaudio-textデータ構築へのモチベがより大きくなるなど）</p>\n<p>素人感想だと，Audio-Language BranchにImagebindを使うのなら，Vision-Lannguage BranchもImagebindで良いのでは？と思った</p>\n<p>とはいえ，temporalな情報をLLMで扱う手法はかなり参考になる</p>\n<h2>次読みたい論文</h2>\n<p><strong><a href=\"https://arxiv.org/abs/2305.05665\">ImageBind: One Embedding Space To Bind Them All</a></strong></p>\n<p><strong><a href=\"https://arxiv.org/abs/2304.12995\">AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</a></strong></p>\n<h2>引用</h2>\n<blockquote>\n<p>@article{zhang2023video,\ntitle={Video-llama: An instruction-tuned audio-visual language model for video understanding},\nauthor={Zhang, Hang and Li, Xin and Bing, Lidong},\njournal={arXiv preprint arXiv:2306.02858},\nyear={2023}\n}</p>\n</blockquote>","Title":"【論文まとめ】Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","Date":"2023-11-02","Category":"論文","Tags":["LLM","multi-modal","Demo"],"Authos":"ゆうぼう","Slug":"Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding","Thumbnail":"/images/thumbnails/Video-LLaMA-An-Instruction-tuned-Audio-Visual-Language-Model-for-Video-Understanding.png","Description":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understandingのまとめ","Published":true},{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms</p>\n<p>研究会: WACV</p>\n<p>年度: 2021</p>\n<p>キーワード: humor detection, multi-modal</p>\n<p>URL: <a href=\"https://openaccess.thecvf.com/content/WACV2021/papers/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.pdf\">https://openaccess.thecvf.com/content/WACV2021/papers/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.pdf</a></p>\n<p>DOI: <a href=\"http://dx.doi.org/10.1109/WACV48630.2021.00062\">http://dx.doi.org/10.1109/WACV48630.2021.00062</a></p>\n<p>データセット: MHD (Multimodal Humor Dataset)</p>\n<h2>概要</h2>\n<p>マルチモダールなユーモアデータセット(<strong>MHD; Multimodal Humor Dataset</strong>)（The Big Bang Theoryを使用）を構築</p>\n<p>海外のSitcoms (Situation comedies) では笑い声がドラマ内に含まれている</p>\n<p>→ sitcomsは定期的に作成されていて，この笑い声を自動で追加するタスクがクリティカルなタスク</p>\n<p>→ <strong>笑い声の自動挿入のタスクを自動化することが狙い</strong></p>\n<p>構築されたデータセットを用いて，マルチモーダルを利用したAttentionベースのモデルを構</p>\n<p>→SoTA &#x26; データセット分析</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/w0i199qh.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/ak7naea6.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/sl4l2p5h.png\" alt=\"\"></p>\n<h3>データセットのこと</h3>\n<p>対話のチャンクに対してlaughter tracksを使用してラベルを付与</p>\n<p>笑い声をアノテーションすることがは間接的に人手でのアノテーションと同じになるという過程</p>\n<p>→ 笑い声の起こる直前の発話の集合をユーモアとしてラベル付け</p>\n<p>Attributes</p>\n<ol>\n<li>Scene</li>\n<li>Speaker</li>\n<li>Recipients</li>\n<li>Participants</li>\n<li>Dialogue Turns</li>\n<li>Dialogue Start/End time</li>\n<li>Humor Start/End time\n対話のチャンクに複数のlaughter tracksがある場合，最後のみ適用</li>\n</ol>\n<p>データ分析の結果はFig 3.を参照のこと</p>\n<h3>モデルのこと</h3>\n<p><img src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/ve5n04t6.png\" alt=\"\"></p>\n<h2>新規性</h2>\n<ul>\n<li>手動でアノテーションされたマルチモーダルな大規模ユーモアデータセットを構築</li>\n<li>これまでのSoTA手法を実験しつつ，multimodal self attention based modelを提案</li>\n<li>提案手法の汎化性能を検証</li>\n</ul>\n<h2>実験</h2>\n<p>5 turns / dialogueとする</p>\n<p>humor : non-humor = 1 : 2としてサンプリング</p>\n<p>humorのラベルが85%と高く，かなり不均衡のため</p>\n<p>実験モデル</p>\n<p>{Attention, Fusion, Sequential} with {only Text, only Video, both of them}</p>\n<p>評価指標：</p>\n<p>Accuracy, ROC, F1</p>\n<h2>まとめ</h2>\n<p><img src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/nlp8wlr7.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/flj6yume.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms/rizjcdv5.png\" alt=\"\"></p>\n<p>提案手法のMSAMが強い</p>\n<p>表情や動作のようなvisual特徴量がユーモアの合図になっていることがある</p>\n<p>→ visual特徴量を使うことが有効である</p>\n<p>@Table 6.より，dialogueのターン数を長くするとよりcontextualにできるが，長くしすぎても精度が落ちている</p>\n<p>→ dialogue 5, 6がピークになっている→ ゆえにturn数を5として本研究は進められている</p>\n<h3>Discussion</h3>\n<ul>\n<li>良いモデルはテキストと視覚的な特徴量の重みづけの仕方を正しく考慮しなければならない</li>\n<li>失敗例への対策\n<ul>\n<li>よりlong tailなユーモアにロバストにならなければいけない\n<ul>\n<li>例）Sheldonは滅多にブランケットを羽織らない→羽織った時面白くなる</li>\n</ul>\n</li>\n<li>知識ベースの弱さへの改善\n<ul>\n<li>sitcomsは皮肉での笑いが多い（知識がないと伝わらないことがある</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n<p>@INPROCEEDINGS{9423266, author={Patro, Badri N. and Lunayach, Mayank and Srivastava, Deepankar and Sarvesh, Sarvesh and Singh, Hunar and Namboodiri, Vinay P.}, booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}, title={Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms}, year={2021}, volume={}, number={}, pages={576-585}, doi={10.1109/WACV48630.2021.00062}}</p>\n</blockquote>","Title":"【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms","Date":"2023-05-21","Category":"論文","Tags":["humor detection","multi-modal"],"Authos":"ゆうぼう","Slug":"Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms","Thumbnail":"/images/thumbnails/Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms.png","Description":"Multimodal Humor Dataset: Predicting Laughter tracks for Sitcomsのまとめ","Published":true}],"tag":"multi-modal","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Overleaf","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","論文執筆","超解像"],"pages":1,"page":1},"__N_SSG":true}