{"pageProps":{"postData":{"contentHtml":"<p>Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．</p>\n<p>この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変か?)</p>\n<h2>やりたかったこと</h2>\n<p>DataLoaderを定義するときに，<code>collate_fn</code>のところで自作collate_fnを指定して，batch単位で流れてくるデータに対してエンコードすること．</p>\n<p>これがやりたいことになります．つまりこんな感じ</p>\n<pre><code><span>from</span> torch.utils <span>import</span> Dataset, DataLoader\n\n<span><span>class</span> <span>MyDataset</span>(<span>Dataset</span>):</span>\n    <span><span>def</span> <span>__init__</span>(<span>self, *args, **kwargs</span>):</span>\n        <span>super</span>().__init__()\n        ...\n    \n    <span><span>def</span> <span>__len__</span>(<span>self</span>):</span>\n        ...\n\n    <span><span>def</span> <span>__getitem__</span>(<span>self, idx: <span>int</span></span>):</span>\n        ...\n\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span>16</span>, shuffle=<span>True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=custom_collate_fn)  <span># &#x3C;--- ここで自作collate_fnを指定して制御</span></code></pre>\n<h2>やってうまくいかなかったこと</h2>\n<p>先にやってうまくいかなかったことを共有しておきます．</p>\n<p>自分が使っているのが，<code>pytorch-lightning</code>なのでそのせいもあるかもしれません．なので，もしかしたら普通に素のPytorchならうまくいくかもしれません．</p>\n<p>教えてください🙏</p>\n<h3>lambda式で制御する (functools.partialを使う)</h3>\n<p>こんなことをしました．</p>\n<pre><code><span>from</span> torch.utils <span>import</span> Dataset, DataLoader\n<span>from</span> transformers <span>import</span> AutoTokenizer\n\n<span><span>class</span> <span>MyDataset</span>(<span>Dataset</span>):</span>\n    <span><span>def</span> <span>__init__</span>(<span>self, *args, **kwargs</span>):</span>\n        <span>super</span>().__init__()\n        ...\n    \n    <span><span>def</span> <span>__len__</span>(<span>self</span>):</span>\n        ...\n\n    <span><span>def</span> <span>__getitem__</span>(<span>self, idx: <span>int</span></span>):</span>\n        ...\n        <span>return</span> text, label\n\n<span><span>def</span> <span>custom_collate_fn</span>(<span>data, tokenizer, max_length</span>):</span>\n    texts, labels = <span>zip</span>(*data)\n    texts = <span>list</span>(texts)\n    texts = tokenizer.batch_encode_plus(\n        texts,\n        padding=<span>True</span>,\n        truncation=<span>True</span>,\n        max_length=max_length,\n        return_tensors=<span>'pt'</span>,\n    )\n    labels = torch.LongTensor(labels)\n    <span>return</span> texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = <span>256</span>\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span>16</span>, shuffle=<span>True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=<span>lambda</span> data: custom_collate_fn(data, tokenizer, max_length))</code></pre>\n<p><code>pytorch-lightning</code>の仕様だとは思うのですが，<code>pickle</code>で圧縮するらしくそのタイミングでエラーを吐かれました．</p>\n<p>なぜだろう...有識者の方教えてください...</p>\n<h2>【解決策】 classで定義する</h2>\n<p>lambda式でダメだったので，もうクラスの内部に必要なものを保持させておこうということになりました．(僕の中では)</p>\n<p>次のコードのような感じで解決しました．</p>\n<pre><code><span>from</span> torch.utils <span>import</span> Dataset, DataLoader\n<span>from</span> transformers <span>import</span> AutoTokenizer\n\n<span><span>class</span> <span>MyDataset</span>(<span>Dataset</span>):</span>\n    <span><span>def</span> <span>__init__</span>(<span>self, *args, **kwargs</span>):</span>\n        <span>super</span>().__init__()\n        ...\n    \n    <span><span>def</span> <span>__len__</span>(<span>self</span>):</span>\n        ...\n\n    <span><span>def</span> <span>__getitem__</span>(<span>self, idx: <span>int</span></span>):</span>\n        ...\n        <span>return</span> text, label\n\n<span><span>class</span> <span>CollateFn</span>:</span>\n    <span><span>def</span> <span>__init__</span>(<span>self, tokenizer, max_length: <span>int</span></span>) -> <span>None</span>:</span>\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        os.environ[<span>\"TOKENIZERS_PARALLELISM\"</span>] = <span>\"true\"</span>  <span># &#x3C;--- 多分これを明示的に指定しないと怒られます (true|false)</span>\n\n    <span><span>def</span> <span>__call__</span>(<span>self, data</span>):</span>\n        texts, labels = <span>zip</span>(*data)\n        texts = <span>list</span>(texts)\n        texts = self.tokenizer.batch_encode_plus(\n            texts,\n            padding=<span>True</span>,\n            truncation=<span>True</span>,\n            max_length=self.max_length,\n            return_tensors=<span>'pt'</span>,\n        )\n        labels = torch.LongTensor(labels)\n        <span>return</span> texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = <span>256</span>\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span>16</span>, shuffle=<span>True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=CollateFn(tokenizer, max_length))</code></pre>\n<h2>まとめ</h2>\n<p>素のPytorchで組めば問題なかったのかもしれませんが，<code>pytorch-lightning</code>を使っている方は同じ状況になるかもしれません．</p>\n<p>その時は，ぜひ参考にclassでcollate_fnで実装してみて解決の一助となれたら幸いです．</p>\n","Title":"collate_fnで複数の引数を取りたい!!","Date":"2021-12-24","Category":"Python","Tags":["ML","Python","Pytorch"],"Authors":"ゆうぼう","Slug":"pytorch-collate_fn-args","Thumbnail":"/images/thumbnails/pytorch-logo.jpg","Description":"Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変かも)","Published":true},"categories":["Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","conda","CSS","ffmpeg","Flask","Go","Google Colaboratory","Heroku","HTML","JavaScript","JSON","Kaggle","Linux","Mac","make","map","MeCab","ML","MySQL","NLP","node.js","Pandas","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","SISR","subprocess","Super-Resolution","tensorflow","Tkinter","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"]},"__N_SSG":true}