{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media</p>\n<p>研究会: Information Processing &#x26; Management</p>\n<p>年度: 2022</p>\n<p>キーワード: COMET, mental health, NLP, mental state knowledge, mentalisation, Contrasive Learning, MentalRoBERTa, KC-Net</p>\n<p>URL: <a href=\"https://www.sciencedirect.com/science/article/pii/S0306457322000796\">https://www.sciencedirect.com/science/article/pii/S0306457322000796</a></p>\n<p>DOI: <a href=\"https://doi.org/10.1016/j.ipm.2022.102961\">https://doi.org/10.1016/j.ipm.2022.102961</a></p>\n<p>データセット: Depression_Mixed, Dreaddit, SQuAD</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/asccglgh.png\" alt=\"\"></p>\n<p>上の流れで学習して，メンタル状態を外部知識のEmbeddingを利用しながら捉える</p>\n<ol>\n<li>\n<p>Data Preprocessing</p>\n<ul>\n<li>nltk sentence tokenizerを使ってpostを文区切にする\n<ul>\n<li>→文ごとのmental stateを捉えるため</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Context-aware post (CAP) encoder\nRoBERTaをdomain-specificなデータで学習した<strong>MentalRoBERTa</strong>なるものがあるのでそれを使って，context-awareなエンコーダとして使用する</p>\n</li>\n<li>\n<p>Mental satte knowledge infusion\nmental stateの知識を捉えるため，ATOMICで学習されたGPTベースのCOMETを使用する</p>\n<p>理由：</p>\n</li>\n</ol>\n<p>↑mental stateとmental health conditionの関係を捉えるために，ConceptNetではなくATOMICで学習されたものを使った</p>\n<ul>\n<li>ConceptNet：一般的な言語の概念を含む</li>\n<li>ATOMIC：human interactionを捉えたcommonsenseを含む\n<ol>\n<li>\n<p>Feature extraction\n以下の5つのaspectを使用した</p>\n<ul>\n<li>intent of S</li>\n<li>effect on S</li>\n<li>reaction of S</li>\n<li>effect on others</li>\n<li>reaction of others</li>\n</ul>\n<p><strong>面白ポイント：COMETのlm_headを削除し，Transformerの内部のみをEncoderとして扱う</strong></p>\n</li>\n</ol>\n</li>\n</ul>\n<p>直接的にpost representationをモデルに統合できて，mental-related variablesを適応することが期待できる</p>\n<pre><code class=\"hljs language-markdown\"><span class=\"hljs-code\">\tCAP embeddingsによるtoken-level representationは，max poolingによってsentence-level representationとされる\n</span>\n<span class=\"hljs-code\">\t$\\hat{H}_j^i = max\\_pooling(H[P_{j-1}^i : P_j^i])$\n</span>\n<span class=\"hljs-bullet\">2.</span> Knowledge-aware mentalisation\n<span class=\"hljs-code\">\t5つの独立したGRUを使用して，mentalのaspect毎に学習するスタイル\n</span></code></pre>\n<p>これでpost-level representationになる</p>\n<pre><code class=\"hljs language-arduino\">\tその後GRUによるmental aspectごとのpost-level representationとmax poolingされたsentence-level representationをAttentionすることで統合する\n</code></pre>\n<ol start=\"4\">\n<li>Supervised contrasive learning\nより文章のsemantic meaningに注意して学習するために，contrasive learningを使用した</li>\n</ol>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/9nzqkqxg.png\" alt=\"\"></p>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/u6fwk2ku.png\" alt=\"\"></p>\n<h2>新規性</h2>\n<ul>\n<li>mental state knolwedgeを使うことでスピーカー（実験ではpostした人）のmental stateを明示的にモデル化する</li>\n<li>model state knowledgeを理解し，使うモデルの能力を強くするため，knowledge-aware dot-product attentionに基づくmentalisation moduleを導入</li>\n</ul>\n<h2>評価方法</h2>\n<p>baseline</p>\n<ul>\n<li>CNN</li>\n<li>GRU</li>\n<li>BiLSTM_Attn</li>\n<li>LR+Features (Logistic Regression)</li>\n<li>EMO_INF</li>\n<li>BERT</li>\n<li>RoBERTa</li>\n<li>MentalRoBERTa</li>\n</ul>\n<p>Precision / Recall / F1を比較</p>\n<h2>何がすごかった？</h2>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/82djhuwa.png\" alt=\"\"></p>\n<p><img src=\"/images/article/A-mental-state-Knowledge%E2%80%93aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media/9rsi0ppo.png\" alt=\"\"></p>\n<ul>\n<li>label情報を完全に利用するためのsupervised contrasive learningを使用することでclass-specificな特徴量を捉える必要性を議論</li>\n<li>SOTAモデル on three stress and depression detection datasets</li>\n</ul>\n<h2>次に読みたい論文</h2>\n<p>CEM: Commonsense-aware Empathetic Response Generation</p>\n<h2>引用</h2>\n<blockquote>\n<p>@article{YANG2022102961,\ntitle = {A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media},\njournal = {Information Processing &#x26; Management},\nvolume = {59},\nnumber = {4},\npages = {102961},\nyear = {2022},\nissn = {0306-4573},\ndoi = {<a href=\"https://doi.org/10.1016/j.ipm.2022.102961\">https://doi.org/10.1016/j.ipm.2022.102961</a>},\nurl = {<a href=\"https://www.sciencedirect.com/science/article/pii/S0306457322000796\">https://www.sciencedirect.com/science/article/pii/S0306457322000796</a>},\nauthor = {Kailai Yang and Tianlin Zhang and Sophia Ananiadou},\nkeywords = {Mental health, Natural language processing, Mental state knowledge, Mentalisation, Contrastive learning},\nabstract = {Stress and depression detection on social media aim at the analysis of stress and identification of depression tendency from social media posts, which provide assistance for the early detection of mental health conditions. Existing methods mainly model the mental states of the post speaker implicitly. They also lack the ability to mentalise for complex mental state reasoning. Besides, they are not designed to explicitly capture class-specific features. To resolve the above issues, we propose a mental state Knowledge–aware and Contrastive Network (KC-Net). In detail, we first extract mental state knowledge from a commonsense knowledge base COMET, and infuse the knowledge using Gated Recurrent Units (GRUs) to explicitly model the mental states of the speaker. Then we propose a knowledge–aware mentalisation module based on dot-product attention to accordingly attend to the most relevant knowledge aspects. A supervised contrastive learning module is also utilised to fully leverage label information for capturing class-specific features. We test the proposed methods on a depression detection dataset Depression_Mixed with 3165 Reddit and blog posts, a stress detection dataset Dreaddit with 3553 Reddit posts, and a stress factors recognition dataset SAD with 6850 SMS-like messages. The experimental results show that our method achieves new state-of-the-art results on all datasets: 95.4% of F1 scores on Depression_Mixed, 83.5% on Dreaddit and 77.8% on SAD, with 2.07% average improvement. Factor-specific analysis and ablation study prove the effectiveness of all proposed modules, while UMAP analysis and case study visualise their mechanisms. We believe our work facilitates detection and analysis of depression and stress on social media data, and shows potential for applications on other mental health conditions.}\n}</p>\n</blockquote>","Title":"【論文まとめ】A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social media","Date":"2023-05-21","Category":"論文","Tags":["COMET","mental health","NLP","mental state knowledge","mentalisation","Contrasive Learning","MentalRoBERTa","KC-Net"],"Authos":"ゆうぼう","Slug":"A-mental-state-Knowledge–aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media","Thumbnail":"/images/thumbnails/A-mental-state-Knowledge–aware-and-Contrastive-Network-for-early-stress-and-depression-detection-on-social-media.png","Description":"A mental state Knowledge–aware and Contrastive Network for early stress and depression detection on social mediaのまとめ","Published":true}],"tag":"mental state knowledge","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Overleaf","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","論文執筆","超解像"],"pages":1,"page":1},"__N_SSG":true}