{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>本記事において使用される図表は，原著論文内の図表を引用しています．</p>\n<p>また，本記事の内容は，著者が論文を読み，メモとして短くまとめたものになります．必ずしも内容が正しいとは限らないこと，ご了承ください．</p>\n<h2>論文情報</h2>\n<p>タイトル: Connecting Speech Encoder and Large Language Model for ASR</p>\n<p>研究会: arxiv</p>\n<p>年度: 2023</p>\n<p>キーワード: ASR, LLM</p>\n<p>URL: <a href=\"https://arxiv.org/pdf/2309.13963.pdf\">https://arxiv.org/pdf/2309.13963.pdf</a></p>\n<p>DOI: <a href=\"https://doi.org/10.48550/arXiv.2309.13963\">https://doi.org/10.48550/arXiv.2309.13963</a></p>\n<p>データセット: LibriSpeech, Common Voice, GigaSpeech</p>\n<h2>概要</h2>\n<p>ASRモデルのエンコーダとLLMを結合して，ASRモデルを構築（Whisper → Vicuna）</p>\n<p>結合の方法として，全結合層，マルチヘッドクロスアテンシション，Q-Formerの3種類を試し，性能を比較</p>\n<p>→ 組み合わせとしては，Wisper large-v2，80個の学習クエリを用いたQ-FormerとVicuna 13Bの組み合わせが良さそう</p>\n<p>→ また，LLMのモデルサイズよりも，ASRモデルの性能の方がクリティカル（Whisper large-v2 > medium > base）</p>\n<p>Seg-QF (Segment-level Q-Former)を提案</p>\n<p>→ ASRモデルに入力可能な系列長を超えて処理ができる（i.e., Whisperは30sの制限）</p>\n<p>→ 長い音声入力に対して，うまくいけばASR性能を向上できるが，入力が長すぎるとLLMのhallucinationを悪化させることも観測</p>\n<h2>提案手法</h2>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/1iwl044a.png\" alt=\"\"></p>\n<p>ASRモデルエンコーダとLLMの結合に3種類を試す</p>\n<ol>\n<li>全結合層\n<ol>\n<li>Conv1d → 線形層 → ReLU → 線形層</li>\n</ol>\n</li>\n<li>マルチヘッドクロスアテンション\n<ol>\n<li>Conv1d → マルチヘッドクロスアテンション</li>\n<li>クエリにはエンコーダの出力をConv1dにかけた埋め込み，キーとバリューはVicunaのテキスト埋め込み</li>\n</ol>\n</li>\n<li>Q-Former</li>\n</ol>\n<h3>Seg-QF (Segment-level Q-Former)</h3>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/q4cga6ej.png\" alt=\"\"></p>\n<p>ASRの入力系列長を超えた時に，セグメントごとに処理をするアーキテクチャ</p>\n<p>各セグメントの位置情報を埋め込むため，sinusoid positional encodingをASRエンコーダの出力に対して適用</p>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/sh6r2b3c.png\" alt=\"\"></p>\n<p>数式としてはこんな感じで，最後に各セグメントのQ-Formerの出力をconcatして，LLMに渡す</p>\n<h2>新規性</h2>\n<ul>\n<li>ASRモデルとLLMを結合してASRモデルを構築する際，2つのモデルの結合方法として3種類を調査\n<ol>\n<li>全結合層</li>\n<li>マルチヘッドクロスアテンション</li>\n<li>Q-Former</li>\n</ol>\n</li>\n<li>Whisper large-v2 + Q-Former + Vicuna 13BでWhisper large-v2よりもWERが向上</li>\n<li>ASRモデルの入力可能系列を超えても処理ができるSegment-level Q-Formerを提案\n<ul>\n<li>長い音声に対してもASRが可能に．ただし，長すぎる音声はLLMのhallucinationが悪化する課題あり</li>\n</ul>\n</li>\n</ul>\n<h2>実験</h2>\n<h3>結合方法の比較</h3>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/p3kesn55.png\" alt=\"\"></p>\n<p>（多分FCのパラメータ数はどっちかがミス？）</p>\n<p>全結合層かQ-Formerを用いるとトークンが多い方がWERは良い</p>\n<p>マルチヘッドアテンション（CA）は，パラメータ数が多すぎるし，しかも性能も悪い</p>\n<p>→ <strong>モデル検証には，Q-Formerが採用</strong></p>\n<h3>Q-Formerの学習するクエリ数の調査</h3>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/fdfvl51a.png\" alt=\"\"></p>\n<p>30s以内の音声に対しては，80まではWERが改善し続けている</p>\n<h3>LLMとASRモデルのエンコーダのサイズの調査</h3>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/lo6a096t.png\" alt=\"\"></p>\n<p>Whisper large-v2を使うと，LLMは7bでも良い性能が出ている</p>\n<p>→ LLMのサイズより，speech encoderの性能の方が重要そう</p>\n<h3>4000h大規模訓練データで学習後の性能評価</h3>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/smbe1p7e.png\" alt=\"\"></p>\n<p>学習時はLibriSpeech, Common Voice, GigaSpeechで学習しているため，これらはin-domainで，CallHome, Swichboardがout-of-domain</p>\n<p>in-domainでも，ベースラインのWhisper large-v2より性能が高くなっている</p>\n<p>out-of-domainだと，最大で12％のWERの改善が見られている</p>\n<h3>Segment-level Q-Formerの性能評価</h3>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/b7s5s5ua.png\" alt=\"\"></p>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/e5h86q3t.png\" alt=\"\"></p>\n<p>30sより長い音声で推論する時，ファインチューニングしていないと性能が悪い</p>\n<p>長い音声でファインチューニングすると，全結合層かQ-Formerを用いると，長い音声でも良い性能を示せている</p>\n<p>ただし，120sまで極端に長くなると，性能が悪くなる</p>\n<p>長時間の音声に対しては，デコードがうまくいくとWERは良いが（Fig. 3.），出力が繰り返されたり，長いチャンクが欠落するケースが多く観測される</p>\n<h2>その他</h2>\n<h3>学習方法</h3>\n<p><img src=\"/images/article/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR/uzgui2mw.png\" alt=\"\"></p>\n<p>音声のない部分をマスクしてゼロパディングすると，Q-Formerが音声の終わりの方を無視するように学習してしまう問題が起きる</p>\n<p>→ ランダムで音声を結合することで，deletion error rateとWERを改善できたため，論文中の学習はrandom audio concatenationのストラテジーで学習している</p>\n<h2>次読みたい論文</h2>\n<h2>引用</h2>\n<blockquote>\n<p>@misc{yu2023connecting,\ntitle={Connecting Speech Encoder and Large Language Model for ASR},\nauthor={Wenyi Yu and Changli Tang and Guangzhi Sun and Xianzhao Chen and Tian Tan and Wei Li and Lu Lu and Zejun Ma and Chao Zhang},\nyear={2023},\neprint={2309.13963},\narchivePrefix={arXiv},\nprimaryClass={<a href=\"http://eess.as/\">eess.AS</a>}\n}</p>\n</blockquote>","Title":"【論文まとめ】Connecting Speech Encoder and Large Language Model for ASR","Date":"2023-11-02","Category":"論文","Tags":["ASR","LLM"],"Authos":"ゆうぼう","Slug":"Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR","Thumbnail":"/images/thumbnails/Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR.png","Description":"Connecting Speech Encoder and Large Language Model for ASRのまとめ","Published":true}],"tag":"ASR","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Overleaf","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","論文執筆","超解像"],"pages":1,"page":1},"__N_SSG":true}