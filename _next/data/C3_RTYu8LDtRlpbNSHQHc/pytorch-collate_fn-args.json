{"pageProps":{"postData":{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<p>Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．</p>\n<p>この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変か?)</p>\n<h2>やりたかったこと</h2>\n<p>DataLoaderを定義するときに，<code>collate_fn</code>のところで自作collate_fnを指定して，batch単位で流れてくるデータに対してエンコードすること．</p>\n<p>これがやりたいことになります．つまりこんな感じ</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> torch.utils <span class=\"hljs-keyword\">import</span> Dataset, DataLoader\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyDataset</span>(<span class=\"hljs-title class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, *args, **kwargs</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        ...\n    \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"hljs-params\">self</span>):\n        ...\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span class=\"hljs-params\">self, idx: <span class=\"hljs-built_in\">int</span></span>):\n        ...\n\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span class=\"hljs-number\">16</span>, shuffle=<span class=\"hljs-literal\">True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=custom_collate_fn)  <span class=\"hljs-comment\"># &#x3C;--- ここで自作collate_fnを指定して制御</span>\n</code></pre>\n<h2>やってうまくいかなかったこと</h2>\n<p>先にやってうまくいかなかったことを共有しておきます．</p>\n<p>自分が使っているのが，<code>pytorch-lightning</code>なのでそのせいもあるかもしれません．なので，もしかしたら普通に素のPytorchならうまくいくかもしれません．</p>\n<p>教えてください🙏</p>\n<h3>lambda式で制御する (functools.partialを使う)</h3>\n<p>こんなことをしました．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> torch.utils <span class=\"hljs-keyword\">import</span> Dataset, DataLoader\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyDataset</span>(<span class=\"hljs-title class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, *args, **kwargs</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        ...\n    \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"hljs-params\">self</span>):\n        ...\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span class=\"hljs-params\">self, idx: <span class=\"hljs-built_in\">int</span></span>):\n        ...\n        <span class=\"hljs-keyword\">return</span> text, label\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">custom_collate_fn</span>(<span class=\"hljs-params\">data, tokenizer, max_length</span>):\n    texts, labels = <span class=\"hljs-built_in\">zip</span>(*data)\n    texts = <span class=\"hljs-built_in\">list</span>(texts)\n    texts = tokenizer.batch_encode_plus(\n        texts,\n        padding=<span class=\"hljs-literal\">True</span>,\n        truncation=<span class=\"hljs-literal\">True</span>,\n        max_length=max_length,\n        return_tensors=<span class=\"hljs-string\">'pt'</span>,\n    )\n    labels = torch.LongTensor(labels)\n    <span class=\"hljs-keyword\">return</span> texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = <span class=\"hljs-number\">256</span>\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span class=\"hljs-number\">16</span>, shuffle=<span class=\"hljs-literal\">True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=<span class=\"hljs-keyword\">lambda</span> data: custom_collate_fn(data, tokenizer, max_length))\n</code></pre>\n<p><code>pytorch-lightning</code>の仕様だとは思うのですが，<code>pickle</code>で圧縮するらしくそのタイミングでエラーを吐かれました．</p>\n<p>なぜだろう...有識者の方教えてください...</p>\n<h2>【解決策】 classで定義する</h2>\n<p>lambda式でダメだったので，もうクラスの内部に必要なものを保持させておこうということになりました．(僕の中では)</p>\n<p>次のコードのような感じで解決しました．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> torch.utils <span class=\"hljs-keyword\">import</span> Dataset, DataLoader\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyDataset</span>(<span class=\"hljs-title class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, *args, **kwargs</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        ...\n    \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"hljs-params\">self</span>):\n        ...\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span class=\"hljs-params\">self, idx: <span class=\"hljs-built_in\">int</span></span>):\n        ...\n        <span class=\"hljs-keyword\">return</span> text, label\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CollateFn</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, tokenizer, max_length: <span class=\"hljs-built_in\">int</span></span>) -> <span class=\"hljs-literal\">None</span>:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        os.environ[<span class=\"hljs-string\">\"TOKENIZERS_PARALLELISM\"</span>] = <span class=\"hljs-string\">\"true\"</span>  <span class=\"hljs-comment\"># &#x3C;--- 多分これを明示的に指定しないと怒られます (true|false)</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"hljs-params\">self, data</span>):\n        texts, labels = <span class=\"hljs-built_in\">zip</span>(*data)\n        texts = <span class=\"hljs-built_in\">list</span>(texts)\n        texts = self.tokenizer.batch_encode_plus(\n            texts,\n            padding=<span class=\"hljs-literal\">True</span>,\n            truncation=<span class=\"hljs-literal\">True</span>,\n            max_length=self.max_length,\n            return_tensors=<span class=\"hljs-string\">'pt'</span>,\n        )\n        labels = torch.LongTensor(labels)\n        <span class=\"hljs-keyword\">return</span> texts, labels\n\ntokenizer = AutoTokenizer.from_pretrained(...)\nmax_length = <span class=\"hljs-number\">256</span>\ndataloader = DataLoader(dataset=MyDataset(), batch_size=<span class=\"hljs-number\">16</span>, shuffle=<span class=\"hljs-literal\">True</span>,\n            num_workers=os.cpu_count(),\n            collate_fn=CollateFn(tokenizer, max_length))\n</code></pre>\n<h2>まとめ</h2>\n<p>素のPytorchで組めば問題なかったのかもしれませんが，<code>pytorch-lightning</code>を使っている方は同じ状況になるかもしれません．</p>\n<p>その時は，ぜひ参考にclassでcollate_fnで実装してみて解決の一助となれたら幸いです．</p>\n</body>\n</html>\n","Title":"collate_fnで複数の引数を取りたい!!","Date":"2021-12-24","Category":"Python","Tags":["ML","Python","Pytorch"],"Authors":"ゆうぼう","Slug":"pytorch-collate_fn-args","Thumbnail":"/images/thumbnails/pytorch-logo.jpg","Description":"Transformersを使って入力テキストをtokenizeするときに，データセットのサイズが大きかったので，バッチ単位でエンコードしたかった時がありました．この時，collate_fnに対して複数の引数を与えたかった状況の時の対処法です．(日本語変かも)","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET,mental health,NLP,mental state knowledge,mentalisation,Contrasive Learning,MentalRoBERTa,KC-Net","conda","CSS","dialogue system","dialogue system,Internet-Augmented","dialogue system,knowledge-base","dialogue system,NLI","dialogue system,persona,Prompt-Tuning","dialogue system,survey,DST","DST","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","humor detection,multi-modal","JavaScript","JSON","Kaggle","laughter,shared laughter","Linux","Mac","make","map","MeCab","ML","MT,transformer,Multi-Hop Transformer","multi-modal","MySQL","NLP","Node","node.js","npm","Pandas","Poetry","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","SISR","subprocess","Super-Resolution","survey","survey,dialogue system","survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP","tensorflow","Tkinter","transformer","transformer,Highway Transformer,Gating Mechanism,Self-Dependency-Units (SDU)","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"]},"__N_SSG":true}