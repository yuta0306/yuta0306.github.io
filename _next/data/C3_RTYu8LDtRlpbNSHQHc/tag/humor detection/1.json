{"pageProps":{"TaggedPostData":[{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<h2>概要</h2>\n<p>マルチモダールなユーモアデータセット(<strong>MHD; Multimodal Humor Dataset</strong>)（The Big Bang Theoryを使用）を構築</p>\n<p>海外のSitcoms (Situation comedies) では笑い声がドラマ内に含まれている</p>\n<p>→ sitcomsは定期的に作成されていて，この笑い声を自動で追加するタスクがクリティカルなタスク</p>\n<p>→ <strong>笑い声の自動挿入のタスクを自動化することが狙い</strong></p>\n<p>構築されたデータセットを用いて，マルチモーダルを利用したAttentionベースのモデルを構</p>\n<p>→SoTA &#x26; データセット分析</p>\n<h2>提案手法</h2>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e9dbc394-bcf4-4316-8d7d-26b3a8df346a/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T191045Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=875fcef8e4c34d0244652019e2b3b6e83e11f873d4b7fdd4f6d41626a426947d&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f4ba132d-d530-41ca-8574-543ce3f59b7b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.53.14.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T191049Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=30e66c37ab34dc93b2d82b9f35ca0987d5c57b5fc45ea877041f0344b2aa6548&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/90a64f3d-7b08-4725-abe3-c1a208d7ef88/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.31.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T191049Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=56532addba0d167bc67f748d52ce045a1e6fba89d117ea47209aa29bbe6e78e2&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<h3>データセットのこと</h3>\n<p>対話のチャンクに対してlaughter tracksを使用してラベルを付与</p>\n<p>笑い声をアノテーションすることがは間接的に人手でのアノテーションと同じになるという過程</p>\n<p>→ 笑い声の起こる直前の発話の集合をユーモアとしてラベル付け</p>\n<p>Attributes</p>\n<ol>\n<li>Scene</li>\n<li>Speaker</li>\n<li>Recipients</li>\n<li>Participants</li>\n<li>Dialogue Turns</li>\n<li>Dialogue Start/End time</li>\n<li>Humor Start/End time</li>\n</ol>\n<p>対話のチャンクに複数のlaughter tracksがある場合，最後のみ適用</p>\n<p>データ分析の結果はFig 3.を参照のこと</p>\n<h3>モデルのこと</h3>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2362b493-1c85-4602-ba74-d181ad8ced3d/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.01.01.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T191059Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=68e23c4842088e321a7cc6ad05f2acdee6532c032005ca8cb42788bd72189555&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<h2>新規性</h2>\n<ul>\n<li>手動でアノテーションされたマルチモーダルな大規模ユーモアデータセットを構築</li>\n<li>これまでのSoTA手法を実験しつつ，multimodal self attention based modelを提案</li>\n<li>提案手法の汎化性能を検証</li>\n</ul>\n<h2>実験</h2>\n<p>5 turns / dialogueとする</p>\n<p>humor : non-humor = 1 : 2としてサンプリング</p>\n<p>humorのラベルが85%と高く，かなり不均衡のため</p>\n<p>実験モデル</p>\n<p>{Attention, Fusion, Sequential} with {only Text, only Video, both of them}</p>\n<p>評価指標：</p>\n<p>Accuracy, ROC, F1</p>\n<h2>まとめ</h2>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/28252124-9d19-4422-a8a3-b60fc13c8c83/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.08.22.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T191109Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=ec270447a882b5eab596d1efdba311a8ff756fa2cc5c5b10b00f9468be752803&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5652a72f-b61e-4e35-9a75-8acc0515616b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T191110Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=0fe8d621ae3114e7a98d82e7b35357d0d6c1459d988c0f55a9dbb12ec249d6f9&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/97772d42-0cc9-4b9e-bdbe-e76c5fcc5514/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.24.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T191111Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=8f39111d3b8e6aa30d6582ef9fa29f12fa86d6386f52b359bd80f29d69e1dafe&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>提案手法のMSAMが強い</p>\n<p>表情や動作のようなvisual特徴量がユーモアの合図になっていることがある</p>\n<p>→ visual特徴量を使うことが有効である</p>\n<p>@Table 6.より，dialogueのターン数を長くするとよりcontextualにできるが，長くしすぎても精度が落ちている</p>\n<p>→ dialogue 5, 6がピークになっている→ ゆえにturn数を5として本研究は進められている</p>\n<h3>Discussion</h3>\n<ul>\n<li>良いモデルはテキストと視覚的な特徴量の重みづけの仕方を正しく考慮しなければならない</li>\n<li>失敗例への対策</li>\n<li>よりlong tailなユーモアにロバストにならなければいけない</li>\n<li>例）Sheldonは滅多にブランケットを羽織らない→羽織った時面白くなる</li>\n<li>知識ベースの弱さへの改善</li>\n<li>sitcomsは皮肉での笑いが多い（知識がないと伝わらないことがある</li>\n</ul>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<p>Here is a summary of the paper based on the web search results:</p>\n<h2>Title: Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms</h2>\n<p>URL: <a href=\"https://ieeexplore.ieee.org/document/9423266\">https://ieeexplore.ieee.org/document/9423266</a>\nConference or Journal: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)\nPublished at: 14 June 2021\nKeywords: multimodal humor, laughter prediction, sitcoms, Big Bang Theory, self-attention\nCited by: 0 (as of 27 April 2023)</p>\n<p>The paper aims to automate the task of adding laughter tracks to sitcoms by annotating an existing sitcom (Big Bang Theory) and evaluating various state-of-the-art baselines. The paper also proposes a novel multimodal self-attention based model that outperforms other models.</p>\n<p>The paper introduces a new dataset and task of predicting laughter tracks for sitcoms, which is a challenging semantic and practical problem. The paper also proposes a novel multimodal self-attention based model that leverages both text and video modalities.</p>\n<p>The proposed method consists of three main components: a multimodal encoder, a self-attention layer, and a binary classifier. The multimodal encoder encodes the text and video features separately using LSTM and CNN respectively, and then concatenates them. The self-attention layer computes the attention weights for each modality and each time step, and then applies them to the encoded features. The binary classifier takes the attended features as input and outputs a probability of laughter for each time step.</p>\n<p>The paper conducts experiments on the Big Bang Theory dataset, which contains 10 episodes from season 1 with manual annotations of laughter tracks. The paper compares the proposed method with several baselines, including LSTM, BERT, CNN-LSTM, and CNN-BERT. The paper uses accuracy, precision, recall, F1-score, and ROC-AUC as evaluation metrics.</p>\n<p>The paper reports that the proposed method achieves the best performance on all metrics, followed by CNN-LSTM and CNN-BERT. The paper also shows that using both text and video modalities improves the performance over using only one modality. The paper further analyzes the attention weights and finds that they are able to capture some humorous cues in the text and video.</p>\n<p>The paper concludes that predicting laughter tracks for sitcoms is a novel and interesting task that requires multimodal understanding of humor. The paper also concludes that the proposed multimodal self-attention based model is effective and interpretable for this task.</p>\n<p>Some possible papers to read next are:</p>\n<ul>\n<li>Humor Recognition using Deep Learning by Weller et al., 2019</li>\n<li>A Multimodal Dataset for Authoring and Editing Jokes by Chakrabarty et al., 2020</li>\n<li>Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems by Le et al., 2019</li>\n</ul>\n<p>ソース: Bing との会話 2023/4/27(1) Multimodal Humor Dataset: Predicting Laughter tracks for .... <a href=\"https://ieeexplore.ieee.org/document/9423266\">https://ieeexplore.ieee.org/document/9423266</a> アクセス日時 2023/4/27.\n(2) Multimodal humor dataset: Predicting laughter tracks for sitcoms. <a href=\"https://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms\">https://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms</a> アクセス日時 2023/4/27.\n(3) Multimodal Humor Dataset: Predicting Laughter Tracks for .... <a href=\"https://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html\">https://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html</a> アクセス日時 2023/4/27.\n(4) Multimodal Humor Dataset: Predicting Laughter tracks for .... <a href=\"https://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22\">https://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22</a> アクセス日時 2023/4/27.</p>\n</body>\n</html>\n","Title":"【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms","Date":"2023-05-21","Category":"論文","Tags":["humor detection","multi-modal"],"Authos":"ゆうぼう","Slug":"Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms","Thumbnail":"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9d8a6445-fdde-46c8-b0b9-72b1f53e4491/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-08_21.44.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230521T191039Z&X-Amz-Expires=3600&X-Amz-Signature=4f4bc4025bf9505ee1318bdbd65422ff21071f8b17540644c4abcf65009097a0&X-Amz-SignedHeaders=host&x-id=GetObject","Description":"Multimodal Humor Dataset: Predicting Laughter tracks for Sitcomsのまとめ","Published":true},{"contentHtml":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css\">\n</head>\n<body>\n<h2>概要</h2>\n<p>マルチモダールなユーモアデータセット(<strong>MHD; Multimodal Humor Dataset</strong>)（The Big Bang Theoryを使用）を構築</p>\n<p>海外のSitcoms (Situation comedies) では笑い声がドラマ内に含まれている</p>\n<p>→ sitcomsは定期的に作成されていて，この笑い声を自動で追加するタスクがクリティカルなタスク</p>\n<p>→ <strong>笑い声の自動挿入のタスクを自動化することが狙い</strong></p>\n<p>構築されたデータセットを用いて，マルチモーダルを利用したAttentionベースのモデルを構</p>\n<p>→SoTA &#x26; データセット分析</p>\n<h2>提案手法</h2>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e9dbc394-bcf4-4316-8d7d-26b3a8df346a/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180341Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=5b27ee778b4b52d926cc58b26a85b63ef0e845ca327755aed28fb3059d42d555&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f4ba132d-d530-41ca-8574-543ce3f59b7b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.53.14.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180344Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=9e5f87561ac85c1eda997f5e70d3637833b25116e77abd2a6af1f01e4400a41c&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/90a64f3d-7b08-4725-abe3-c1a208d7ef88/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_9.54.31.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180348Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=2af88be8e09237c755126c35e5078a9639502857fb07a760c48c50037aa5448d&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<h3>データセットのこと</h3>\n<p>対話のチャンクに対してlaughter tracksを使用してラベルを付与</p>\n<p>笑い声をアノテーションすることがは間接的に人手でのアノテーションと同じになるという過程</p>\n<p>→ 笑い声の起こる直前の発話の集合をユーモアとしてラベル付け</p>\n<p>Attributes</p>\n<ol>\n<li>Scene</li>\n<li>Speaker</li>\n<li>Recipients</li>\n<li>Participants</li>\n<li>Dialogue Turns</li>\n<li>Dialogue Start/End time</li>\n<li>Humor Start/End time</li>\n</ol>\n<p>対話のチャンクに複数のlaughter tracksがある場合，最後のみ適用</p>\n<p>データ分析の結果はFig 3.を参照のこと</p>\n<h3>モデルのこと</h3>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2362b493-1c85-4602-ba74-d181ad8ced3d/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.01.01.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180414Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=4b9c37202b1c79aa7b8c8fd0daf1a02543384956e8839ea12043668fb8886f60&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<h2>新規性</h2>\n<ul>\n<li>手動でアノテーションされたマルチモーダルな大規模ユーモアデータセットを構築</li>\n<li>これまでのSoTA手法を実験しつつ，multimodal self attention based modelを提案</li>\n<li>提案手法の汎化性能を検証</li>\n</ul>\n<h2>実験</h2>\n<p>5 turns / dialogueとする</p>\n<p>humor : non-humor = 1 : 2としてサンプリング</p>\n<p>humorのラベルが85%と高く，かなり不均衡のため</p>\n<p>実験モデル</p>\n<p>{Attention, Fusion, Sequential} with {only Text, only Video, both of them}</p>\n<p>評価指標：</p>\n<p>Accuracy, ROC, F1</p>\n<h2>まとめ</h2>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/28252124-9d19-4422-a8a3-b60fc13c8c83/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.08.22.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180430Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=13d483e8c4cbfb256f50730b13afc36c6da8e156ebac43d3fd7feb7ac6ceadde&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5652a72f-b61e-4e35-9a75-8acc0515616b/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.11.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180432Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=1265762aa3394b65c75eddd17c718fd880950065f9214246790590ceae6008ce&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p><img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/97772d42-0cc9-4b9e-bdbe-e76c5fcc5514/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-09_10.05.24.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&#x26;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&#x26;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&#x26;X-Amz-Date=20230521T180433Z&#x26;X-Amz-Expires=3600&#x26;X-Amz-Signature=c5470ae7c6d1c7389fa4ff5c3a966286463bf12e37511a2ab1e73d215e2c8f01&#x26;X-Amz-SignedHeaders=host&#x26;x-id=GetObject\" alt=\"\"></p>\n<p>提案手法のMSAMが強い</p>\n<p>表情や動作のようなvisual特徴量がユーモアの合図になっていることがある</p>\n<p>→ visual特徴量を使うことが有効である</p>\n<p>@Table 6.より，dialogueのターン数を長くするとよりcontextualにできるが，長くしすぎても精度が落ちている</p>\n<p>→ dialogue 5, 6がピークになっている→ ゆえにturn数を5として本研究は進められている</p>\n<h3>Discussion</h3>\n<ul>\n<li>良いモデルはテキストと視覚的な特徴量の重みづけの仕方を正しく考慮しなければならない</li>\n<li>失敗例への対策</li>\n<li>よりlong tailなユーモアにロバストにならなければいけない</li>\n<li>例）Sheldonは滅多にブランケットを羽織らない→羽織った時面白くなる</li>\n<li>知識ベースの弱さへの改善</li>\n<li>sitcomsは皮肉での笑いが多い（知識がないと伝わらないことがある</li>\n</ul>\n<h2>その他（なぜ通ったか？等）</h2>\n<h2>次読みたい論文</h2>\n<p>Here is a summary of the paper based on the web search results:</p>\n<h2>Title: Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms</h2>\n<p>URL: <a href=\"https://ieeexplore.ieee.org/document/9423266\">https://ieeexplore.ieee.org/document/9423266</a>\nConference or Journal: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)\nPublished at: 14 June 2021\nKeywords: multimodal humor, laughter prediction, sitcoms, Big Bang Theory, self-attention\nCited by: 0 (as of 27 April 2023)</p>\n<p>The paper aims to automate the task of adding laughter tracks to sitcoms by annotating an existing sitcom (Big Bang Theory) and evaluating various state-of-the-art baselines. The paper also proposes a novel multimodal self-attention based model that outperforms other models.</p>\n<p>The paper introduces a new dataset and task of predicting laughter tracks for sitcoms, which is a challenging semantic and practical problem. The paper also proposes a novel multimodal self-attention based model that leverages both text and video modalities.</p>\n<p>The proposed method consists of three main components: a multimodal encoder, a self-attention layer, and a binary classifier. The multimodal encoder encodes the text and video features separately using LSTM and CNN respectively, and then concatenates them. The self-attention layer computes the attention weights for each modality and each time step, and then applies them to the encoded features. The binary classifier takes the attended features as input and outputs a probability of laughter for each time step.</p>\n<p>The paper conducts experiments on the Big Bang Theory dataset, which contains 10 episodes from season 1 with manual annotations of laughter tracks. The paper compares the proposed method with several baselines, including LSTM, BERT, CNN-LSTM, and CNN-BERT. The paper uses accuracy, precision, recall, F1-score, and ROC-AUC as evaluation metrics.</p>\n<p>The paper reports that the proposed method achieves the best performance on all metrics, followed by CNN-LSTM and CNN-BERT. The paper also shows that using both text and video modalities improves the performance over using only one modality. The paper further analyzes the attention weights and finds that they are able to capture some humorous cues in the text and video.</p>\n<p>The paper concludes that predicting laughter tracks for sitcoms is a novel and interesting task that requires multimodal understanding of humor. The paper also concludes that the proposed multimodal self-attention based model is effective and interpretable for this task.</p>\n<p>Some possible papers to read next are:</p>\n<ul>\n<li>Humor Recognition using Deep Learning by Weller et al., 2019</li>\n<li>A Multimodal Dataset for Authoring and Editing Jokes by Chakrabarty et al., 2020</li>\n<li>Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems by Le et al., 2019</li>\n</ul>\n<p>ソース: Bing との会話 2023/4/27(1) Multimodal Humor Dataset: Predicting Laughter tracks for .... <a href=\"https://ieeexplore.ieee.org/document/9423266\">https://ieeexplore.ieee.org/document/9423266</a> アクセス日時 2023/4/27.\n(2) Multimodal humor dataset: Predicting laughter tracks for sitcoms. <a href=\"https://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms\">https://researchportal.bath.ac.uk/en/publications/multimodal-humor-dataset-predicting-laughter-tracks-for-sitcoms</a> アクセス日時 2023/4/27.\n(3) Multimodal Humor Dataset: Predicting Laughter Tracks for .... <a href=\"https://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html\">https://openaccess.thecvf.com/content/WACV2021/html/Patro_Multimodal_Humor_Dataset_Predicting_Laughter_Tracks_for_Sitcoms_WACV_2021_paper.html</a> アクセス日時 2023/4/27.\n(4) Multimodal Humor Dataset: Predicting Laughter tracks for .... <a href=\"https://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22\">https://www.semanticscholar.org/paper/Multimodal-Humor-Dataset%3A-Predicting-Laughter-for-Patro-Lunayach/a8cd2a93dc7f798e0c5280f2e7bc3fdc66bc4c22</a> アクセス日時 2023/4/27.</p>\n</body>\n</html>\n","Title":"【論文まとめ】Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms","Date":"2023-05-21","Category":"論文","Tags":"humor detection,multi-modal","Authos":"ゆうぼう","Slug":"Multimodal-Humor-Dataset-Predicting-Laughter-tracks-for-Sitcoms","Thumbnail":"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9d8a6445-fdde-46c8-b0b9-72b1f53e4491/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-10-08_21.44.00.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230521%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230521T180328Z&X-Amz-Expires=3600&X-Amz-Signature=47425b16d3accc25b1ca817eec4d5bf1148e6cf566a7737adf14227c8da0f921&X-Amz-SignedHeaders=host&x-id=GetObject","Description":"Multimodal Humor Dataset: Predicting Laughter tracks for Sitcomsのまとめ","Published":true}],"tag":"humor detection","categories":["論文","Web","JavaScript","Competition","Cloud","Linux","Python","ML","Go","SQL"],"tags":["Apache","Appium","atmaCup","AWS","CentOS7","CentOS8","Colab","COMET,mental health,NLP,mental state knowledge,mentalisation,Contrasive Learning,MentalRoBERTa,KC-Net","conda","CSS","dialogue system","dialogue system,Internet-Augmented","dialogue system,knowledge-base","dialogue system,NLI","dialogue system,persona,Prompt-Tuning","dialogue system,survey,DST","DST","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","humor detection,multi-modal","JavaScript","JSON","Kaggle","laughter,shared laughter","Linux","Mac","make","map","MeCab","ML","MT,transformer,Multi-Hop Transformer","multi-modal","MySQL","NLP","Node","node.js","npm","Pandas","Poetry","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","SISR","subprocess","Super-Resolution","survey","survey,dialogue system","survey,NLP,knowledge-base,PLMKE,commonsense,encyclopedic,Knowledge-Intensive NLP","tensorflow","Tkinter","transformer","transformer,Highway Transformer,Gating Mechanism,Self-Dependency-Units (SDU)","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","超解像"],"pages":1,"page":1},"__N_SSG":true}