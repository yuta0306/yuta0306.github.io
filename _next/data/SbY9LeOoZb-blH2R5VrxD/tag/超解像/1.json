{"pageProps":{"TaggedPostData":[{"contentHtml":"<p>※諸注意</p>\n<blockquote>\n<p>cf. @solafune(<a href=\"https://solafune.com\">https://solafune.com</a>) コンテストの参加以外を目的とした利用及び商用利用は禁止されています。商用利用・その他当コンテスト以外で利用したい場合はお問い合わせください。(<a href=\"https://solafune.com\">https://solafune.com</a>)</p>\n</blockquote>\n<p><a href=\"https://solafune.com/\">Solafune</a>というプラットフォームでのMScupで低解像度を超解像化する珍しいコンペが出てきて，楽しそうだと（実際楽しかったけど）2ヶ月従事したものの11位で終わってしまいました😭</p>\n<p>でも，学べることがたくさんあったし，画像系コンペに出てこなかったので，画像系でも活かせそうな知見が得られて楽しいコンペでした．</p>\n<p>実験してみて効いたこと効かなかったことや学んだことをまとめます．</p>\n<h2>結果報告</h2>\n<p>先に結果を報告してしまいます．</p>\n<p>タイトルですでに出オチですが，<strong>11位フィニッシュ</strong>でした🎉</p>\n<p>PublicもPrivateも共に11位だったので，Trust CV，Trust LBなコンペだったと思います．そのためとても取り組みやすかったです．</p>\n<p>また，ライセンス表記に対してシビアではありましたが，運営さんの方から使っていいライセンスが詳しく話されていたので，これに関しても取り組みやすかったと思っています．</p>\n<p>結果で見れば悔しい結果ですが，総評してとても楽しかったです🤗</p>\n<h2>コンペ概要</h2>\n<p>概要は以下です．</p>\n<blockquote>\n<p>衛星画像を活用する際の課題の一つに「解像度」の問題があります。人工衛星は広域を撮影できる一方で、解像度の低さがボトルネックになります。近年、解像度が高い衛星画像の提供・販売が始まっていますが、利用可能な高解像データは世界で最も解像度が高い画像でも30cm程度です。また、高解像度のデータは値段が高く、利用規約にも様々な制限が課せられています。さらに、現時点では高解像度の画像データは法律で利用を禁止している国もあります。 そのため、今回のコンテストでは高解像度のデータを安く利用するための手段として、超解像を扱います。将来的に高解像度の衛星画像を扱えるようになることを想定して、本コンテストでは25 cm解像度の画像データを扱います。「超解像」という技術を通して、衛星データや航空写真などの地理空間データの社会実装が加速することを期待しています。</p>\n</blockquote>\n<p>与えられて低解像度の画像に対して，高解像化をかけるモデル，アルゴリズムを作成し，SSIMが高くすることが課題でした．</p>\n<p>SSIMは以下で表され，これを最大化することを目的とします．</p>\n<p><span class=\"math math-inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -1.481ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"32.581ex\" height=\"3.922ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1078.9 14400.8 1733.4\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><path id=\"MJX-1-TEX-I-1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"></path><path id=\"MJX-1-TEX-I-1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"></path><path id=\"MJX-1-TEX-I-1D440\" d=\"M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z\"></path><path id=\"MJX-1-TEX-N-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path><path id=\"MJX-1-TEX-I-1D465\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"></path><path id=\"MJX-1-TEX-N-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path><path id=\"MJX-1-TEX-I-1D466\" d=\"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path><path id=\"MJX-1-TEX-N-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path><path id=\"MJX-1-TEX-N-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path><path id=\"MJX-1-TEX-N-32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"></path><path id=\"MJX-1-TEX-I-1D707\" d=\"M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z\"></path><path id=\"MJX-1-TEX-N-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path><path id=\"MJX-1-TEX-I-1D450\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"></path><path id=\"MJX-1-TEX-N-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path><path id=\"MJX-1-TEX-I-1D70E\" d=\"M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z\"></path></defs><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><use data-c=\"1D446\" xlink:href=\"#MJX-1-TEX-I-1D446\"></use></g><g data-mml-node=\"mi\" transform=\"translate(645,0)\"><use data-c=\"1D446\" xlink:href=\"#MJX-1-TEX-I-1D446\"></use></g><g data-mml-node=\"mi\" transform=\"translate(1290,0)\"><use data-c=\"1D43C\" xlink:href=\"#MJX-1-TEX-I-1D43C\"></use></g><g data-mml-node=\"mi\" transform=\"translate(1794,0)\"><use data-c=\"1D440\" xlink:href=\"#MJX-1-TEX-I-1D440\"></use></g><g data-mml-node=\"mo\" transform=\"translate(2845,0)\"><use data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"></use></g><g data-mml-node=\"mi\" transform=\"translate(3234,0)\"><use data-c=\"1D465\" xlink:href=\"#MJX-1-TEX-I-1D465\"></use></g><g data-mml-node=\"mo\" transform=\"translate(3806,0)\"><use data-c=\"2C\" xlink:href=\"#MJX-1-TEX-N-2C\"></use></g><g data-mml-node=\"mi\" transform=\"translate(4250.7,0)\"><use data-c=\"1D466\" xlink:href=\"#MJX-1-TEX-I-1D466\"></use></g><g data-mml-node=\"mo\" transform=\"translate(4740.7,0)\"><use data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"></use></g><g data-mml-node=\"mo\" transform=\"translate(5407.4,0)\"><use data-c=\"3D\" xlink:href=\"#MJX-1-TEX-N-3D\"></use></g><g data-mml-node=\"mfrac\" transform=\"translate(6463.2,0)\"><g data-mml-node=\"mrow\" transform=\"translate(652.8,548.6) scale(0.707)\"><g data-mml-node=\"mo\"><use data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"></use></g><g data-mml-node=\"mn\" transform=\"translate(389,0)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g><g data-mml-node=\"msub\" transform=\"translate(889,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D707\" xlink:href=\"#MJX-1-TEX-I-1D707\"></use></g><g data-mml-node=\"mi\" transform=\"translate(636,-150) scale(0.707)\"><use data-c=\"1D465\" xlink:href=\"#MJX-1-TEX-I-1D465\"></use></g></g><g data-mml-node=\"msub\" transform=\"translate(1979.5,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D707\" xlink:href=\"#MJX-1-TEX-I-1D707\"></use></g><g data-mml-node=\"mi\" transform=\"translate(636,-150) scale(0.707)\"><use data-c=\"1D466\" xlink:href=\"#MJX-1-TEX-I-1D466\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(3011.9,0)\"><use data-c=\"2B\" xlink:href=\"#MJX-1-TEX-N-2B\"></use></g><g data-mml-node=\"msub\" transform=\"translate(3789.9,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D450\" xlink:href=\"#MJX-1-TEX-I-1D450\"></use></g><g data-mml-node=\"mn\" transform=\"translate(466,-150) scale(0.707)\"><use data-c=\"31\" xlink:href=\"#MJX-1-TEX-N-31\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(4659.5,0)\"><use data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"></use></g><g data-mml-node=\"mo\" transform=\"translate(5048.5,0)\"><use data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"></use></g><g data-mml-node=\"mn\" transform=\"translate(5437.5,0)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g><g data-mml-node=\"msub\" transform=\"translate(5937.5,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D70E\" xlink:href=\"#MJX-1-TEX-I-1D70E\"></use></g><g data-mml-node=\"TeXAtom\" transform=\"translate(604,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><use data-c=\"1D465\" xlink:href=\"#MJX-1-TEX-I-1D465\"></use></g><g data-mml-node=\"mi\" transform=\"translate(572,0)\"><use data-c=\"1D466\" xlink:href=\"#MJX-1-TEX-I-1D466\"></use></g></g></g><g data-mml-node=\"mo\" transform=\"translate(7342.4,0)\"><use data-c=\"2B\" xlink:href=\"#MJX-1-TEX-N-2B\"></use></g><g data-mml-node=\"msub\" transform=\"translate(8120.4,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D450\" xlink:href=\"#MJX-1-TEX-I-1D450\"></use></g><g data-mml-node=\"mn\" transform=\"translate(466,-150) scale(0.707)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(8990,0)\"><use data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"></use></g></g><g data-mml-node=\"mrow\" transform=\"translate(220,-377.4) scale(0.707)\"><g data-mml-node=\"mo\"><use data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"></use></g><g data-mml-node=\"msubsup\" transform=\"translate(389,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D707\" xlink:href=\"#MJX-1-TEX-I-1D707\"></use></g><g data-mml-node=\"mn\" transform=\"translate(636,289) scale(0.707)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g><g data-mml-node=\"mi\" transform=\"translate(636,-247) scale(0.707)\"><use data-c=\"1D465\" xlink:href=\"#MJX-1-TEX-I-1D465\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(1479.5,0)\"><use data-c=\"2B\" xlink:href=\"#MJX-1-TEX-N-2B\"></use></g><g data-mml-node=\"msubsup\" transform=\"translate(2257.5,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D707\" xlink:href=\"#MJX-1-TEX-I-1D707\"></use></g><g data-mml-node=\"mn\" transform=\"translate(636,289) scale(0.707)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g><g data-mml-node=\"mi\" transform=\"translate(636,-247) scale(0.707)\"><use data-c=\"1D466\" xlink:href=\"#MJX-1-TEX-I-1D466\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(3297,0)\"><use data-c=\"2B\" xlink:href=\"#MJX-1-TEX-N-2B\"></use></g><g data-mml-node=\"msub\" transform=\"translate(4075,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D450\" xlink:href=\"#MJX-1-TEX-I-1D450\"></use></g><g data-mml-node=\"mn\" transform=\"translate(466,-150) scale(0.707)\"><use data-c=\"31\" xlink:href=\"#MJX-1-TEX-N-31\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(4944.6,0)\"><use data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"></use></g><g data-mml-node=\"mo\" transform=\"translate(5333.6,0)\"><use data-c=\"28\" xlink:href=\"#MJX-1-TEX-N-28\"></use></g><g data-mml-node=\"msubsup\" transform=\"translate(5722.6,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D70E\" xlink:href=\"#MJX-1-TEX-I-1D70E\"></use></g><g data-mml-node=\"mn\" transform=\"translate(604,289) scale(0.707)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g><g data-mml-node=\"mi\" transform=\"translate(604,-247) scale(0.707)\"><use data-c=\"1D465\" xlink:href=\"#MJX-1-TEX-I-1D465\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(6781,0)\"><use data-c=\"2B\" xlink:href=\"#MJX-1-TEX-N-2B\"></use></g><g data-mml-node=\"msubsup\" transform=\"translate(7559,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D70E\" xlink:href=\"#MJX-1-TEX-I-1D70E\"></use></g><g data-mml-node=\"mn\" transform=\"translate(604,289) scale(0.707)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g><g data-mml-node=\"mi\" transform=\"translate(604,-247) scale(0.707)\"><use data-c=\"1D466\" xlink:href=\"#MJX-1-TEX-I-1D466\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(8566.6,0)\"><use data-c=\"2B\" xlink:href=\"#MJX-1-TEX-N-2B\"></use></g><g data-mml-node=\"msub\" transform=\"translate(9344.6,0)\"><g data-mml-node=\"mi\"><use data-c=\"1D450\" xlink:href=\"#MJX-1-TEX-I-1D450\"></use></g><g data-mml-node=\"mn\" transform=\"translate(466,-150) scale(0.707)\"><use data-c=\"32\" xlink:href=\"#MJX-1-TEX-N-32\"></use></g></g><g data-mml-node=\"mo\" transform=\"translate(10214.1,0)\"><use data-c=\"29\" xlink:href=\"#MJX-1-TEX-N-29\"></use></g></g><rect width=\"7697.6\" height=\"60\" x=\"120\" y=\"220\"></rect></g></g></g></svg></mjx-container></span></p>\n<h2>My Solution</h2>\n<p>僕のSolutionは大したことができませんでした．というのも，たくさんの実験を試したのですが，ほとんどうまくいかず悪化し，結局Seed Averagingなどの手抜きアイデアが最終サブになってしまったからです．</p>\n<p>とは言いつつも，おそらく僕独自の手法も入っているのではないかと思うので参考になればと思います．</p>\n<h3>モデル概要</h3>\n<p>訓練時は以下のようなモデル設計になっています．</p>\n<p><img src=\"/images/article/mscup/mscup_train.jpeg\" alt=\"訓練時モデル\"></p>\n<p>推論時は<strong>Seed Averaging</strong>と**TTA（Test Time Augmentation）**を利用して，テストデータでの推論時にロバストに推論ができるようにしました．Seed Averagingは5つくらいのモデルの平均をとったはずです．</p>\n<p>Seed AveragingやTTAは正直つまらない解法だと思っていますが，<strong>Sobel Filterを利用したLoss関数</strong>や<strong>CutMixやCutout</strong>などのData Augmentationが個人的な解法のウリではあります．</p>\n<p>また，僕がコンペを進める中での一番力を入れたことは，<strong>Loss関数による精度向上</strong>です．モデルも比較しましたが，結局SOTAモデルのSwinIRが圧巻で，Augmentationも思ったほど効かなかったので，Loss関数で制御することが最もの試みでした．これは独自の取り組みだと思っています（思いたい）</p>\n<p><a href=\"#%E5%AE%9F%E9%A8%93%E4%B8%AD%E3%81%AE%E8%AA%B2%E9%A1%8C%E6%84%9F%E3%81%A8%E5%B7%A5%E5%A4%AB\">次のセクション</a>で，なぜその実装をしたかと結果を話したいと思います．</p>\n<h3>実験中の課題感と対処法</h3>\n<p>実験している中での課題をまずは簡単に．</p>\n<ol>\n<li><a href=\"#%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88%E3%81%8C%E5%B0%91%E3%81%AA%E3%81%84\">データセットが少ない</a>\n<ul>\n<li>画像が大きいので，クロップすればデータはたくさんあるのですが，多様性という面では多いとは言えなかったかもしれない．</li>\n</ul>\n</li>\n<li><strong>回転のAugmentationを加えるとノイズになる</strong>\n<ul>\n<li>エッジの再現性が重要となる超解像タスクでは，回転後のギザギザがノイズになってしまう．</li>\n<li>初めて知ったけど， 超解像タスクでは90度単位での回転が基本動作．画像見比べてなるほどってなった．</li>\n</ul>\n</li>\n<li><strong>超解像時にエイリアシングが起きたとき，エッジの向きに再現性がない</strong>\n<ul>\n<li>データを見ていてこれが問題と感じて，Sobel Filterを施そうと...</li>\n<li>細くは改めて話します．</li>\n</ul>\n</li>\n<li>Augmentationなんでもいいと思いきや，CutBlurやMixupはノイズだった\n<ul>\n<li>実装の苦労の割に大幅悪化で辛かった...</li>\n</ul>\n</li>\n</ol>\n<p>それでは上記について，細かく述べていこうと思います．</p>\n<h4>データセットが少ない</h4>\n<p>訓練データは60シーンあり，クロップすればデータは多く感じるものの，やはり多様性という時点では少ないなぁという印象でした．</p>\n<p>上位者の公開Solutionでは外部データの利用が鍵になったという指摘もあり，ここの調べを怠ったのが大敗につながってしまったのかなとも思っています．</p>\n<p>そうは言っても全く対処しなかったわけではなく，「<a href=\"https://arxiv.org/abs/2004.00448\">Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy</a>」で提案されたData Augmentationの手法をPytorchパイプラインに合うように再実装し，実験はしました．</p>\n<p>効き目は以下のような感じでした．</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>手法</th><th>説明</th><th align=\"center\">効き目</th></tr></thead><tbody><tr><td>CutBlur</td><td>低解像度に高解像度の画像を部分的に差し込む（またはその逆）</td><td align=\"center\">×</td></tr><tr><td>Cutout</td><td>一定確率でピクセル値を0にする</td><td align=\"center\">△</td></tr><tr><td>CutMix</td><td>異なる画像を部分的に差し込む</td><td align=\"center\">○</td></tr><tr><td>Mixup</td><td>beta分布に従って異なる画像を混合する</td><td align=\"center\">×</td></tr><tr><td>CutMixup</td><td>CutMixとCutupを同時に行う</td><td align=\"center\">×</td></tr><tr><td>Blend</td><td>一様分布に従ってサンプリングされた色を混合する</td><td align=\"center\">△</td></tr><tr><td>RGBPermutation</td><td>RGBの順番を変える</td><td align=\"center\">△</td></tr></tbody></table>\n<p>実際に効いているかはそこまで詳しく見ることはできていませんが，スコアや生成された高解像度画像を見ているとこんな感じだったと思います．</p>\n<p>効き目の詳細は改めて話します．</p>\n<h4>回転のAugmentationを加えるとノイズになる</h4>\n<p>これは最初気づかなくて，エッジがめちゃくちゃ鈍った画像が生成されたので分析した結果わかったことです．</p>\n<p>画像ドメインを持っている人ならば当たり前に感じてしまうかもしれませんが，画像にドメインのない人にとっては最後まで気づかなくてもおかしくなかったと思っています．</p>\n<p>ライブラリの仕様でも，回転を加える関数には，<em>interpolation</em>などのデータ補間のための引数があるはずです．</p>\n<p>例えば<em>torchvision</em>ですが，ランダムでの回転は以下で実装されます．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> torchvision.transforms <span class=\"hljs-keyword\">as</span> T\n\n<span class=\"hljs-comment\"># ニアレストネイバーでの補間</span>\ntransforms = T.RandomRotation(degrees=<span class=\"hljs-number\">180</span>, interpolation=T.InterpolationMode.NEAREST)\n<span class=\"hljs-comment\"># バイリニアでの補間</span>\ntransforms = T.RandomRotation(degrees=<span class=\"hljs-number\">180</span>, interpolation=T.InterpolationMode.BILINEAR)\n</code></pre>\n<p>この実装を見る限りでも，エッジの部分が補間されることで元の入力画像との再現が取れなくなってしまうことは想像できると思います．</p>\n<p>これは学びでしたが，超解像タスクにおいては，<strong>回転のAugmentationは90度単位</strong>というのが主流のようでした．</p>\n<h4>超解像時にエイリアシングが起きたとき，エッジの向きに再現性がない</h4>\n<p>本当は画像を見せた方が早いのですが，今回のコンペで使用された画像は公開できないので，手書きで頑張って表現しようと思います．</p>\n<p>田んぼみたいに，ある程度イネを植えている方向に規則的な向きがあるような感覚を想像してください．</p>\n<p>実験中下のような画像が生成されてしまいました．</p>\n<p><img src=\"/images/article/mscup/edge.png\" alt=\"エッジが再現できない生成画像\"></p>\n<p>SSIMを最適化するのですが，<a href=\"https://kornia.readthedocs.io/en/latest/index.html\">kornia</a>で実装されていた<em>SSIM Loss</em>では，入力画像をGaussian Filterで平滑化してからSSIMの計算をかけていたので，ぼやけた状態でうまくreconstructできるかを学習してしまっていたのだと思います．</p>\n<p>そこで僕は追加で<em>Smooth L1 Loss</em>をさらに追加しました．こちらはピクセルごとでロスが計算されるので細かいところまで再現できるかなと思ったのですが，MSEなどのピクセルを見るLoss関数は画像全体がぼやけてしまうらしいです．</p>\n<p>結果行き着いた発想が，「<strong>エッジに対してLossを計算してしまえばいいじゃない？？</strong>」でした．</p>\n<p>これは結果的にスコア微増に起因したのですが，前に示した図のようなエイリアシングが取れていることが多く，実装と結果が直感的だったので最後まで採用しました．数学的に正しい感じは全くありませんが，なんかうまくいったので（正しいかは知りません．こいつほんまに微分可能なんか？ってお気持ちのままやってました笑）</p>\n<p>実装上は，Sobel Filterをかけて，エッジ抽出された画像に対してSmooth L1 Lossを計算するように計算しました．</p>\n<p>当時の実装を下記に載っけておきます．</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MeanSobelError</span>(nn.Module):\n   <span class=\"hljs-string\">\"\"\"\n   Sobel Gradient Loss Function\n   \"\"\"</span>\n   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self,\n      normalized: <span class=\"hljs-built_in\">bool</span> = <span class=\"hljs-literal\">True</span>,\n      eps: <span class=\"hljs-built_in\">float</span> = <span class=\"hljs-number\">1e-6</span>,\n      reduction: <span class=\"hljs-built_in\">str</span> = <span class=\"hljs-string\">'mean'</span>,\n   </span>):\n      <span class=\"hljs-built_in\">super</span>(MeanSobelError, self).__init__()\n      self.<span class=\"hljs-built_in\">filter</span> = kornia.filters.Sobel(\n         normalized=normalized,\n         eps=eps,\n      )\n      self.lossfn = nn.SmoothL1Loss(reduction=reduction)\n\n   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self,\n      y_pred: torch.Tensor,\n      y_true: torch.Tensor,\n   </span>):\n      filtered_pred = self.<span class=\"hljs-built_in\">filter</span>(y_pred)\n      filtered_true = self.<span class=\"hljs-built_in\">filter</span>(y_true)\n\n      loss = self.lossfn(filtered_pred, filtered_true)\n      <span class=\"hljs-keyword\">return</span> loss\n</code></pre>\n<h4>Augmentationなんでもいいと思いきや，CutBlurやMixupはノイズだった</h4>\n<p>SISRタスクにおいて提案されていたAugmentationは全部効くだろう！というスタンスで，<strong>コンペ終盤に時間をめちゃくちゃかけて実装</strong>し，全部ごちゃ混ぜでやってみたのですが，スコアが激減....</p>\n<p>幾つかパターンを試しましたが，CutBlurやMixup系はスコアの悪化の原因だったっぽいです．</p>\n<ul>\n<li>Mixup\n<ul>\n<li>画像を割合で混合するので，エッジが鈍りがちになってしまいました．</li>\n<li>車や車線など，エッジがはっきりした画像生成が必要だったため，ノイズになってしまったのかもしれません．</li>\n</ul>\n</li>\n<li>CutBlur\n<ul>\n<li>こちらは単純にモデル作成が下手くそなのかも知れないです．</li>\n<li>SwinIRに入力する前に，ダウンサンプリング層を用意する必要がありました．</li>\n<li>↑これがうまく学習できず，生成される画像の色がおかしかったです．\n<ul>\n<li>一応ダウンサンプリング層のみのwarmupなども試したが，うまく適合できず，この実装はボツに...（めっちゃ時間かけたのに...）</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>こんな感じで効くAugmentationと効かないAugmentationがあったので，早い段階でこの実装と実験ができていれば，もっと豊かな実験ができたかなと反省しています．</p>\n<p>Augmentationはいずれコンペ中は確実に試すことになると思うので，早めに効くかどうか先にやっておくといいのかも（？）</p>\n<h2>まとめ</h2>\n<p>最後に今回のコンペで得た知見をまとめたいと思います．</p>\n<ul>\n<li>Augmentationはタスク依存．課題感を意識したり，早めに取り組んだ方がいいかも（？）</li>\n<li>データセットのやりくりはサボらない\n<ul>\n<li>外部データの調査とかEDAとか</li>\n</ul>\n</li>\n<li>スコアだけでなく，予測値も確認してみる\n<ul>\n<li>今回はラベル分布とかそういうのではないので，生成画像もしっかり確認するなど</li>\n<li>それを割としっかりやったのでSobel Filterに行き着いたと思っています．</li>\n</ul>\n</li>\n<li>論文とにかく読んでアイデアを得よう\n<ul>\n<li>めっちゃ読んで，実装して効かなかったアイデアが8割くらいだったけど，楽しかったです．</li>\n<li>今，大学で取り組んでいるNLPの研究にアイデアが入ったりしているので，やり切るって大事！</li>\n</ul>\n</li>\n<li><strong>楽しむ！！！</strong>\n<ul>\n<li>コンペは序盤と終盤がとても辛いです．\n<ul>\n<li>序盤：バグが取れない．公開のやつに勝てない．</li>\n<li>終盤：スコアが伸び悩む．</li>\n</ul>\n</li>\n<li>でも，やり切れば結果楽しい（MScupが楽しかった説に一票ですが😂）</li>\n<li>CVやLBをおかずに飯が食えるぞ？</li>\n</ul>\n</li>\n</ul>\n<p>僕が今回のMScupを通して学んだことはこんなところでしょうか．次もしSolafuneのコンペに参加することがあれば，しっかり賞金圏に入りたいと思います！</p>\n<p>最後に今回のコンペで使ったライブラリや実装したコードを載せて終わりにしたいと思います．</p>\n<p>つらつらと思うところを書きましたが，最後まで読んでいただきありがとうございます．意見やご指摘などがありましたら，Twitterなどでご連絡ください．</p>\n<h2>よく使ったライブラリなど</h2>\n<p>今回のコンペでよく使えたライブラリなどを載っけておきます．</p>\n<ul>\n<li><a href=\"https://kornia.readthedocs.io/en/latest/index.html\">kornia</a>\n<ul>\n<li>ssim lossやsobel filterなど，画像処理で手前実装が大変でかゆいところに手が届きました．今後も自分は使うことがありそうだと感じました．</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/JingyunLiang/SwinIR\">SwinIR</a>\n<ul>\n<li>SISRにおけるSOTAモデル．事前学習済みモデルが公開されており，重宝しました．一番強かった...</li>\n<li><a href=\"https://arxiv.org/abs/2108.10257\">paper</a></li>\n</ul>\n</li>\n<li><a href=\"https://huggingface.co/models?other=image-super-resolution\">image-super-resolution</a>\n<ul>\n<li>さまざまなSuper Resolutionタスクでのモデルの再現実装をされています．</li>\n<li>今回のコンペでは精度が微妙だったのでサブには使いませんでしたが，SISRタスクで遊びたい時に簡単に扱えると思います．</li>\n</ul>\n</li>\n<li>Pytorch Lightning\n<ul>\n<li>単純にファン．もう楽</li>\n</ul>\n</li>\n<li>Pytorch</li>\n<li>cv2\n<ul>\n<li>RGBの順番気をつけましょう．これで2週間くらい無駄にしました笑笑</li>\n</ul>\n</li>\n</ul>\n<h2>実装したコード</h2>\n<p><a href=\"https://github.com/yuta0306/SRAugmentation\">SRAugmentation</a>にて公開しています．</p>\n<p>初めてGitHubでスターをもらって喜んでいました（小並感</p>\n<p>CutBlurなど，Single Image Super ResolutionにおけるData Augmentationの手法を改めてPytorchのパイプラインに組み込めるように再実装したものです．</p>\n<p>こちらの論文で提案されています．<a href=\"https://arxiv.org/abs/2004.00448\">Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy</a></p><style>\nmjx-container[jax=\"SVG\"] {\n  direction: ltr;\n}\n\nmjx-container[jax=\"SVG\"] > svg {\n  overflow: visible;\n  min-height: 1px;\n  min-width: 1px;\n}\n\nmjx-container[jax=\"SVG\"] > svg a {\n  fill: blue;\n  stroke: blue;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"] {\n  display: block;\n  text-align: center;\n  margin: 1em 0;\n}\n\nmjx-container[jax=\"SVG\"][display=\"true\"][width=\"full\"] {\n  display: flex;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"left\"] {\n  text-align: left;\n}\n\nmjx-container[jax=\"SVG\"][justify=\"right\"] {\n  text-align: right;\n}\n\ng[data-mml-node=\"merror\"] > g {\n  fill: red;\n  stroke: red;\n}\n\ng[data-mml-node=\"merror\"] > rect[data-background] {\n  fill: yellow;\n  stroke: none;\n}\n\ng[data-mml-node=\"mtable\"] > line[data-line], svg[data-table] > g > line[data-line] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {\n  stroke-width: 70px;\n  fill: none;\n}\n\ng[data-mml-node=\"mtable\"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {\n  stroke-dasharray: 140;\n}\n\ng[data-mml-node=\"mtable\"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {\n  stroke-linecap: round;\n  stroke-dasharray: 0,140;\n}\n\ng[data-mml-node=\"mtable\"] > g > svg {\n  overflow: visible;\n}\n\n[jax=\"SVG\"] mjx-tool {\n  display: inline-block;\n  position: relative;\n  width: 0;\n  height: 0;\n}\n\n[jax=\"SVG\"] mjx-tool > mjx-tip {\n  position: absolute;\n  top: 0;\n  left: 0;\n}\n\nmjx-tool > mjx-tip {\n  display: inline-block;\n  padding: .2em;\n  border: 1px solid #888;\n  font-size: 70%;\n  background-color: #F8F8F8;\n  color: black;\n  box-shadow: 2px 2px 5px #AAAAAA;\n}\n\ng[data-mml-node=\"maction\"][data-toggle] {\n  cursor: pointer;\n}\n\nmjx-status {\n  display: block;\n  position: fixed;\n  left: 1em;\n  bottom: 1em;\n  min-width: 25%;\n  padding: .2em .4em;\n  border: 1px solid #888;\n  font-size: 90%;\n  background-color: #F8F8F8;\n  color: black;\n}\n\nforeignObject[data-mjx-xml] {\n  font-family: initial;\n  line-height: normal;\n  overflow: visible;\n}\n\nmjx-container[jax=\"SVG\"] path[data-c], mjx-container[jax=\"SVG\"] use[data-c] {\n  stroke-width: 3;\n}\n</style>","Title":"【🛰MScup】超解像コンペで11位だったけど，やったことまとめまくる","Date":"2022-02-13","Category":"Competition","Tags":["データ分析","超解像","SISR","Super-Resolution","Pytorch"],"Authors":"ゆうぼう","Slug":"mscup-feedback","Thumbnail":"/images/thumbnails/mscup.jpg","Description":"MScupで低解像度を超解像化する珍しいコンペが出てきて，2ヶ月従事したけど11位で終わってしまいました．でも，学べることがたくさんあったし，画像系コンペに出てこなかったので，画像系でも活かせそうな知見が得られて楽しいコンペでした．実験してみて効いたこと効かなかったことや学んだことをまとめます．","Published":true}],"tag":"超解像","categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Overleaf","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","論文執筆","超解像"],"pages":1,"page":1},"__N_SSG":true}