<!DOCTYPE html><html lang="ja" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Pytorch LightningでTPUを回す on Colab Pro in 2021<!-- --> | <!-- -->ゆうぼうの書跡棚</title><meta name="description" content="Colab Proでなんとかpytorch-lightningを使ってTPUで実験を回そうと奮闘した結果，得られたベストプラクティスの共有です．もっといい方法があったら教えてください．"/><meta name="og:description" content="Colab Proでなんとかpytorch-lightningを使ってTPUで実験を回そうと奮闘した結果，得られたベストプラクティスの共有です．もっといい方法があったら教えてください．"/><meta property="og:type" content="website"/><meta name="author" content="ゆうぼう"/><meta property="og:title" content="Pytorch LightningでTPUを回す on Colab Pro in 2021"/><meta property="og:image" content="https://yuta0306.github.io/images/thumbnails/pytorch-lightning_on_colab-pro-tpu.png"/><meta property="og:url" content="https://yuta0306.github.io/pytorch-lightning_on_colab-tpu_2021"/><script src="/js/toc.js"></script><meta name="next-head-count" content="11"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="HandheldFriendly" content="True"/><meta name="description" content="スキルや知識をつけて将来ナマケモノになるまでの技術ブログです．主に，機械学習やPython, JavaScriptによる開発についてまとめます．"/><meta name="author" content="ゆうぼう"/><meta name="twitter:card" content="summary"/><meta name="robots" content="index, follow"/><link rel="alternate" type="application/rss+xml" href="https:/yuta0306.github.io/feed.xml" title="RSS2.0"/><meta name="generator" content="Next.js"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png"/><link rel="manifest" href="/favicon_io/site.webmanifest"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-147997959-2"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-147997959-2', {
                    page_path: window.location.pathname,
                  });</script><script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><link rel="preload" href="/_next/static/css/75506965150cde8a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/75506965150cde8a.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a6e19106a865540a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a6e19106a865540a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-7c8966651ff4862e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6312d3a6c9934c88.js" defer=""></script><script src="/_next/static/chunks/664-60e06c839f82ba03.js" defer=""></script><script src="/_next/static/chunks/768-c61e8ddb09e59da7.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-b275297c06761584.js" defer=""></script><script src="/_next/static/lR7wjVyUe1SvaFyxOcA82/_buildManifest.js" defer=""></script><script src="/_next/static/lR7wjVyUe1SvaFyxOcA82/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Home_container__bCOhY"><div class="header_container__IoqX_"><div class="header_container__inner__pDDDU"><header class="header_header__pKEQL"><a href="/"><div class="header_header__title__uoTF0">ゆうぼうの書跡棚</div></a></header><div class="header_hamburger__kYfxY"><div><img src="/icons/hamburger.png"/></div></div><nav class="header_nav__closed__1h469"><ul class="header_nav__list__eqFqF"><li class="header_nav__item__FNSzb"><a href="/about">About</a></li><li class="header_nav__item_active__qVXxE"><a href="/">Blog</a></li><li class="header_nav__item__FNSzb"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdgyok9pi697ZJvVizRNEw0qghDWz517k1FrbcRmfvvERlraA/viewform">Contact</a></li></ul></nav></div></div><div class="header_category__WFSrL"><ul class="header_category__items____MmN"></ul></div><div class="main_main__VZQGI"><div class="main_main__container__PFqpL"><main class="main_main__container__inner__PWn1D" role="main" itemProp="mainContentOfPage" itemscope="" itemType="http://schema.org/Blog"><div class="main_content__V_9fG"><div itemscope="" itemType="http://schema.org/BlogPosting"><div style="background:url(/images/thumbnails/pytorch-lightning_on_colab-pro-tpu.png);overflow:hidden"><div class="Home_thumbnail__xs1Hd" itemscope="" itemProp="image" itemType="https://schema.org/ImageObject"><img src="/images/thumbnails/pytorch-lightning_on_colab-pro-tpu.png" alt="Pytorch LightningでTPUを回す on Colab Pro in 2021" loading="lazy" style="height:100%;width:auto;margin:0 auto;display:block"/></div></div><time dateTime="2021-10-11" style="color:rgb(144, 144, 144)">2021-10-11</time><h1>Pytorch LightningでTPUを回す on Colab Pro in 2021</h1><div id="TOC__mobile"></div><article style="margin-top:4rem" itemscope="" itemProp="text"><p>Colab Proでなんとかpytorch-lightningを使ってTPUで実験を回そうと奮闘した結果，得られたベストプラクティス?(怪しい)の共有です．</p>
<p>もっといい方法があったら教えてください．</p>
<p>ちなみに，現行コンペで動作確認したので，コード全容の公開はないです．すみません．</p>
<p><strong>Runtimeは必ずTPUにしてください</strong></p>
<h2>pytorch-lightningでTPUを使いたいメリット?</h2>
<p>黙って<em>tensorflow</em>を勉強して書けば，それでいいのでしょうが，<br>
普段<em>pytorch</em>を使ってコーディングをしていて，僕の場合<em>pytorch-lightning</em>を好んで使っているので，その延長戦でTPUを使いたいという欲がありました．</p>
<p>また，<em>pytorch-lightning</em>は，<strong>Trainerクラス</strong>をちょろっと書き換えるだけで，CPU/GPU/TPUの変換ができ，マルチGPU等のDDPの処理も自動で書き加えてくれるため，<br>
かなり良いものかなと思っています．</p>
<p>CPU/GPU/TPUの書き換えは基本的に以下で十分です．</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> pytorch_lightning <span class="hljs-keyword">as</span> pl
<span class="hljs-comment"># これらは共通</span>
model = LitModel()  <span class="hljs-comment"># pl.LightningModuleを継承した何か</span>
dm = LitDataset()  <span class="hljs-comment"># pl.LightningDataModuleを継承した何か</span>

<span class="hljs-comment"># CPUを使う</span>
trainer = Trainer()
trainer.fit(model, dm)

<span class="hljs-comment"># GPUを使う</span>
trainer = Trainer(gpus=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># ありったけのGPU使う</span>
trainer.fit(model, dm)

<span class="hljs-comment"># Single Core TPUを使う</span>
trainer = Trainer(tpu_cores=[<span class="hljs-number">5</span>])  <span class="hljs-comment"># TPUのインデックス指定</span>
trainer.fit(model, dm)

<span class="hljs-comment"># Multi Core TPUを使う</span>
trainer = Trainer(tpu_cores=<span class="hljs-number">8</span>)  <span class="hljs-comment"># 1 or 8じゃないと怒られます...</span>
trainer.fit(model, dm)
</code></pre>
<p>ああ，楽だ楽だ．</p>
<p>TPU簡単に動いてくれたらいいのに(願望)</p>
<h2>TPUを使うためのセットアップ</h2>
<p>以下を実行して，必要なライブラリ群を入れてしまいます．</p>
<p>一応僕が動作確認取れたのは，pytorch-xlaのバージョンが<em>1.8</em>と<em>1.9</em>です．</p>
<p>Circle CIのTPU環境に合わせたユニットテストは，確か1.8が通っていて，1.9がエラーを吐いていた気がします．</p>
<p>1.8の方が安全なのですかね？(わかりません)</p>
<p><em>pytorch-xla 1.8</em></p>
<pre><code class="hljs language-python">!pip install cloud-tpu-client==<span class="hljs-number">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class="hljs-number">1.8</span>-cp37-cp37m-linux_x86_64.whl
!pip install -q torch==<span class="hljs-number">1.8</span> torchvision torchtext
!pip install -q pytorch-lightning==<span class="hljs-number">1.4</span><span class="hljs-number">.9</span> torchmetrics
</code></pre>
<p><em>pytorch-xla 1.9</em></p>
<pre><code class="hljs language-python">!pip install cloud-tpu-client==<span class="hljs-number">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class="hljs-number">1.9</span>-cp37-cp37m-linux_x86_64.whl
!pip install -q torch==<span class="hljs-number">1.9</span> torchvision torchtext
!pip install -q pytorch-lightning==<span class="hljs-number">1.4</span><span class="hljs-number">.9</span> torchmetrics
</code></pre>
<p>pipでのインストール分割していたり，--quit(-q)をつけているのは特に理由はないです．</p>
<p>その辺はお好みでやってください．</p>
<p>一応僕の環境ではこんな感じで，以後のプログラムはうまいこと動作しました．</p>
<p>おそらく以下のようなエラーは起きますが，特に動作に影響はなさそうでした．あまり不安がらなくても良さそうです．</p>
<pre><code class="hljs language-shell">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
earthengine-api 0.1.284 requires google-api-python-client&#x3C;2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.
</code></pre>
<h2>セットアップ後にimport pytorch_lightning</h2>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> pytorch_lightning <span class="hljs-keyword">as</span> pl
</code></pre>
<p><em>import</em>すると以下のような出力を吐きます．</p>
<p>インストールした<em>xla</em>のバージョンが1.9ならば，以下のようなメッセージが出ます．</p>
<p>少し時間がかかりますが，待ちましょう．</p>
<p><em>pytorch-xla 1.9</em></p>
<pre><code class="hljs language-shell">WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...
WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...
WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...
WARNING:root:TPU has started up successfully with version pytorch-1.9
</code></pre>
<p>もし，<em>TPU not Found</em>的なことが出たら，一回<em>Factory reset Runtime</em>して，またインストールからやり直せばうまく行くと思います．</p>
<h2>importまでうまく行けたら</h2>
<p><em>import</em>するまで行けたら，このあとは特に問題なく動くはずです．</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># ModelとかDataModuleとかは適宜設定してください．</span>

trainer = Trainer(tpu_cores=<span class="hljs-number">8</span>)
trainer.fit(model, dm)
</code></pre>
<p>このあとすぐエラー出なきゃ，勝手に動くと思います(雑ww)</p>
<h2>注意するべきこと</h2>
<p>幾つか僕が遭遇したエラーを挙げておきます．</p>
<ul>
<li>wandb loggerが使えない
<ul>
<li>多分Commet loggerも使えない？</li>
</ul>
</li>
<li>マルチコアTPU使うなら，predict時return_predictionsができない
<ul>
<li><em>pytorch_lightning.callbacks.BasePredictionWriter</em>を使おう(<a href="#BasePredictionWriter%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB">サンプル</a>)</li>
</ul>
</li>
<li>なぜか知らんけど，<a href="#RAM%E3%81%AE%E4%BD%BF%E7%94%A8%E7%8E%8750%25%E3%81%8F%E3%82%89%E3%81%84%E3%81%AB%E3%81%AA%E3%82%8B%E3%81%A8stack%E3%81%99%E3%82%8B%E4%BB%B6%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><strong>RAMの使用率が50%あたりになるとstackする</strong></a>
<ul>
<li>こうなると一生動かなくなるので，<strong>最初安定するまで監視が必要</strong></li>
<li>Colabの下のデバッガみたいなやつが，ずっと*select() > spawn()*的な非同期のところで止まるので，そうなったらRAMチェックしよう!!!</li>
</ul>
</li>
</ul>
<h2>BasePredictionWriterのサンプル</h2>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> pytorch_lightning.callbacks <span class="hljs-keyword">import</span> BasePredictionWriter

<span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomWriter</span>(<span class="hljs-title class_ inherited__">BasePredictionWriter</span>):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, output_dir: <span class="hljs-built_in">str</span>, write_interval: <span class="hljs-built_in">str</span> = <span class="hljs-string">'batch'</span></span>):
        <span class="hljs-built_in">super</span>().__init__(write_interval)
        self.output_dir = output_dir

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">write_on_batch_end</span>(<span class="hljs-params">
        self, trainer, pl_module: <span class="hljs-string">'LightningModule'</span>, prediction: <span class="hljs-type">Any</span>, batch_indices: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], batch: <span class="hljs-type">Any</span>,
        batch_idx: <span class="hljs-built_in">int</span>, dataloader_idx: <span class="hljs-built_in">int</span>
    </span>):
        torch.save(prediction, os.path.join(self.output_dir, dataloader_idx, <span class="hljs-string">f"<span class="hljs-subst">{batch_idx}</span>.pt"</span>))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">write_on_epoch_end</span>(<span class="hljs-params">
        self, trainer, pl_module: <span class="hljs-string">'LightningModule'</span>, predictions: <span class="hljs-type">List</span>[<span class="hljs-type">Any</span>], batch_indices: <span class="hljs-type">List</span>[<span class="hljs-type">Any</span>]
    </span>):
        torch.save(predictions, os.path.join(self.output_dir, <span class="hljs-string">"predictions.pt"</span>))
</code></pre>
<h2>RAMの使用率50%くらいになるとstackする件について</h2>
<p>はっきり言って，これは本当に原因がわかりません．</p>
<p>50%くらいで張りついちゃうので，batch_size下げようが，RAMの使用率はあまり変わる気がしません．</p>
<p>こうなってstackしちゃったら，もうHigh-RAM設定にしてRAMにゆとりを持たせた方がいいと思います．潔く．</p>
<p>High-RAMでも張りついちゃう場合は，僕はわかりません．</p>
<p>RAM周りの最適化とか知見ある方，解決策わかったら共有してくださるととてもありがたいです......</p>
<h2>まとめ</h2>
<p>まとめじゃないです．嘘つきました．</p>
<p>でも，まとめないとなんかあれなので，CPU/GPU/TPUで使いまわせるように，セットアップの部分整理します．</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys

<span class="hljs-keyword">if</span> os.environ.get(<span class="hljs-string">'COLAB_TPU_ADDR'</span>, <span class="hljs-literal">False</span>):
    !pip install cloud-tpu-client==<span class="hljs-number">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class="hljs-number">1.8</span>-cp37-cp37m-linux_x86_64.whl
    !pip install -q torch==<span class="hljs-number">1.8</span> torchvision torchtext

!pip install pytorch-lightning==<span class="hljs-number">1.4</span><span class="hljs-number">.9</span> torchmetrics
</code></pre>
<p>もし，KaggleとColabでも行けるようにしたいのであれば，<code>'google.colab' in sys.modules</code>でやるといいと思います．</p>
<p>Colab TPU使って，pytorch-lightningでうまく実験回しまくっている方いたら，色々意見ください．お願いします :)</p>
<p>知見たまって，エラーハンドリングとかわかってきたら，この記事更新するか，新たに書き始めます．よろしくお願いします．</p></article><div class="socialshare_container__SSXJE"><h3>タメになったらSHARE!!!</h3><div class="socialshare_container__links__JZs4j"><a target="_blank" href="https://twitter.com/share?url=https://yuta0306.github.io/pytorch-lightning_on_colab-tpu_2021"><img src="/icons/twitter.png" loading="lazy" alt="https://yuta0306.github.io/pytorch-lightning_on_colab-tpu_2021をTwitterに共有する"/></a><a target="_blank" href="https://www.facebook.com/share.php?u=https://yuta0306.github.io/pytorch-lightning_on_colab-tpu_2021"><img src="/icons/facebook.png" loading="lazy" alt="https://yuta0306.github.io/pytorch-lightning_on_colab-tpu_2021をFacebookに共有する"/></a><a target="_blank" href="http://b.hatena.ne.jp/entry/https:/yuta0306.github.io/pytorch-lightning_on_colab-tpu_2021"><img src="/icons/hatenablog.png" loading="lazy" alt="https://yuta0306.github.io/pytorch-lightning_on_colab-tpu_2021をはてなブログに共有する"/></a></div></div></div></div></main><aside class="main_sidebar__tM28d"><div class="shortbio_container__4psan" itemscope="" itemProp="author" itemType="http://schema.org/Person"><div class="shortbio_container__image__eljVd"><img src="/images/profile.jpeg" alt="ゆうぼう" loading="lazy"/></div><h3 class="shortbio_author__A2bKB" itemscope="" itemProp="name">ゆうぼう</h3><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">国立大学院M1のナマケモノです．</p></div><div><p class="shortbio_container__paragraph__EJbWG">human-likeな対話システムの研究に従事し，人間とAIの共生社会の構築に人生を捧げたいと考えています．</p></div><div><p class="shortbio_container__paragraph__EJbWG">学部時代はコモンセンスを利用したユーモア検出の研究を行っていました(Knowledge-intensive NLP)．</p></div><div><p class="shortbio_container__paragraph__EJbWG">このブログはNext.jsで書いてます．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div><div><p class="shortbio_container__paragraph__EJbWG">Kaggle等のデータ分析コンペは活動休止中．</p></div><div><p class="shortbio_container__paragraph__EJbWG"></p></div></div><div class="followme_container__T1oVi"><h3 class="followme_container__header__Pt5SP">Follow Me</h3><div class="followme_container__links__b3XW5"><a target="_blank" href="https://github.com/yuta0306"><img src="/icons/github.png" alt="GitHub"/></a><a target="_blank" href="https://kaggle.com/yutasasaki"><img src="/icons/kaggle.png" alt="Kaggle"/></a><a target="_blank" href="https://twitter.com/Sloth65557166"><img src="/icons/twitter.png" alt="Twitter"/></a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div class="categories_container__J8nCF"><h3 class="categories_container__header__zt836">Categories</h3><div class="categories_container__links__MFrVK"><a class="categories_container__link__AuvVr" href="/category/%E8%AB%96%E6%96%87/1">論文</a><a class="categories_container__link__AuvVr" href="/category/Web/1">Web</a><a class="categories_container__link__AuvVr" href="/category/JavaScript/1">JavaScript</a><a class="categories_container__link__AuvVr" href="/category/Competition/1">Competition</a><a class="categories_container__link__AuvVr" href="/category/Cloud/1">Cloud</a><a class="categories_container__link__AuvVr" href="/category/Python/1">Python</a><a class="categories_container__link__AuvVr" href="/category/Linux/1">Linux</a><a class="categories_container__link__AuvVr" href="/category/ML/1">ML</a><a class="categories_container__link__AuvVr" href="/category/Go/1">Go</a><a class="categories_container__link__AuvVr" href="/category/SQL/1">SQL</a></div></div><div class="tags_container___e3ez"><h3 class="tags_container__header__hxPW8">Tags</h3><div class="tags_container__links__X38Ga"><a class="tags_container__link__1Ts3a" href="/tag/Apache/1">Apache</a><a class="tags_container__link__1Ts3a" href="/tag/Appium/1">Appium</a><a class="tags_container__link__1Ts3a" href="/tag/ASR/1">ASR</a><a class="tags_container__link__1Ts3a" href="/tag/atmaCup/1">atmaCup</a><a class="tags_container__link__1Ts3a" href="/tag/AWS/1">AWS</a><a class="tags_container__link__1Ts3a" href="/tag/brew/1">brew</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS7/1">CentOS7</a><a class="tags_container__link__1Ts3a" href="/tag/CentOS8/1">CentOS8</a><a class="tags_container__link__1Ts3a" href="/tag/Colab/1">Colab</a><a class="tags_container__link__1Ts3a" href="/tag/COMET/1">COMET</a><a class="tags_container__link__1Ts3a" href="/tag/commonsense/1">commonsense</a><a class="tags_container__link__1Ts3a" href="/tag/conda/1">conda</a><a class="tags_container__link__1Ts3a" href="/tag/Contrasive%20Learning/1">Contrasive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/Contrastive%20Learning/1">Contrastive Learning</a><a class="tags_container__link__1Ts3a" href="/tag/CSS/1">CSS</a><a class="tags_container__link__1Ts3a" href="/tag/Demo/1">Demo</a><a class="tags_container__link__1Ts3a" href="/tag/Dialogue%20Structure%20Learning/1">Dialogue Structure Learning</a><a class="tags_container__link__1Ts3a" href="/tag/dialogue%20system/1">dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/DST/1">DST</a><a class="tags_container__link__1Ts3a" href="/tag/Emotion%20Recognition/1">Emotion Recognition</a><a class="tags_container__link__1Ts3a" href="/tag/empathetic%20dialogue%20system/1">empathetic dialogue system</a><a class="tags_container__link__1Ts3a" href="/tag/encyclopedic/1">encyclopedic</a><a class="tags_container__link__1Ts3a" href="/tag/Error%20Correction/1">Error Correction</a><a class="tags_container__link__1Ts3a" href="/tag/ESPNet/1">ESPNet</a><a class="tags_container__link__1Ts3a" href="/tag/ffmpeg/1">ffmpeg</a><a class="tags_container__link__1Ts3a" href="/tag/Flask/1">Flask</a><a class="tags_container__link__1Ts3a" href="/tag/Gating%20Mechanism/1">Gating Mechanism</a><a class="tags_container__link__1Ts3a" href="/tag/Go/1">Go</a><a class="tags_container__link__1Ts3a" href="/tag/Google%20Colaboratory/1">Google Colaboratory</a><a class="tags_container__link__1Ts3a" href="/tag/Heroku/1">Heroku</a><a class="tags_container__link__1Ts3a" href="/tag/Highway%20Transformer/1">Highway Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/HTML/1">HTML</a><a class="tags_container__link__1Ts3a" href="/tag/humor%20detection/1">humor detection</a><a class="tags_container__link__1Ts3a" href="/tag/Intent%20Classification/1">Intent Classification</a><a class="tags_container__link__1Ts3a" href="/tag/Internet-Augmented/1">Internet-Augmented</a><a class="tags_container__link__1Ts3a" href="/tag/JavaScript/1">JavaScript</a><a class="tags_container__link__1Ts3a" href="/tag/JSON/1">JSON</a><a class="tags_container__link__1Ts3a" href="/tag/Kaggle/1">Kaggle</a><a class="tags_container__link__1Ts3a" href="/tag/KC-Net/1">KC-Net</a><a class="tags_container__link__1Ts3a" href="/tag/knowledge-base/1">knowledge-base</a><a class="tags_container__link__1Ts3a" href="/tag/Knowledge-Intensive%20NLP/1">Knowledge-Intensive NLP</a><a class="tags_container__link__1Ts3a" href="/tag/laughter/1">laughter</a><a class="tags_container__link__1Ts3a" href="/tag/Linux/1">Linux</a><a class="tags_container__link__1Ts3a" href="/tag/LLM/1">LLM</a><a class="tags_container__link__1Ts3a" href="/tag/Mac/1">Mac</a><a class="tags_container__link__1Ts3a" href="/tag/make/1">make</a><a class="tags_container__link__1Ts3a" href="/tag/map/1">map</a><a class="tags_container__link__1Ts3a" href="/tag/MeCab/1">MeCab</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20health/1">mental health</a><a class="tags_container__link__1Ts3a" href="/tag/mental%20state%20knowledge/1">mental state knowledge</a><a class="tags_container__link__1Ts3a" href="/tag/mentalisation/1">mentalisation</a><a class="tags_container__link__1Ts3a" href="/tag/MentalRoBERTa/1">MentalRoBERTa</a><a class="tags_container__link__1Ts3a" href="/tag/Merging%20Models/1">Merging Models</a><a class="tags_container__link__1Ts3a" href="/tag/ML/1">ML</a><a class="tags_container__link__1Ts3a" href="/tag/Model%20Editing/1">Model Editing</a><a class="tags_container__link__1Ts3a" href="/tag/Model%20Patching/1">Model Patching</a><a class="tags_container__link__1Ts3a" href="/tag/MT/1">MT</a><a class="tags_container__link__1Ts3a" href="/tag/Multi-Hop%20Transformer/1">Multi-Hop Transformer</a><a class="tags_container__link__1Ts3a" href="/tag/multi-modal/1">multi-modal</a><a class="tags_container__link__1Ts3a" href="/tag/MySQL/1">MySQL</a><a class="tags_container__link__1Ts3a" href="/tag/NLG/1">NLG</a><a class="tags_container__link__1Ts3a" href="/tag/NLI/1">NLI</a><a class="tags_container__link__1Ts3a" href="/tag/NLP/1">NLP</a><a class="tags_container__link__1Ts3a" href="/tag/Node/1">Node</a><a class="tags_container__link__1Ts3a" href="/tag/node.js/1">node.js</a><a class="tags_container__link__1Ts3a" href="/tag/npm/1">npm</a><a class="tags_container__link__1Ts3a" href="/tag/Overleaf/1">Overleaf</a><a class="tags_container__link__1Ts3a" href="/tag/Pandas/1">Pandas</a><a class="tags_container__link__1Ts3a" href="/tag/persona/1">persona</a><a class="tags_container__link__1Ts3a" href="/tag/PLMKE/1">PLMKE</a><a class="tags_container__link__1Ts3a" href="/tag/Poetry/1">Poetry</a><a class="tags_container__link__1Ts3a" href="/tag/Prompt-Tuning/1">Prompt-Tuning</a><a class="tags_container__link__1Ts3a" href="/tag/Python/1">Python</a><a class="tags_container__link__1Ts3a" href="/tag/Pytorch/1">Pytorch</a><a class="tags_container__link__1Ts3a" href="/tag/pytorch-lightning/1">pytorch-lightning</a><a class="tags_container__link__1Ts3a" href="/tag/Scikit-learn/1">Scikit-learn</a><a class="tags_container__link__1Ts3a" href="/tag/Selenium/1">Selenium</a><a class="tags_container__link__1Ts3a" href="/tag/Self-Dependency-Units%20(SDU)/1">Self-Dependency-Units (SDU)</a><a class="tags_container__link__1Ts3a" href="/tag/shared%20laughter/1">shared laughter</a><a class="tags_container__link__1Ts3a" href="/tag/SISR/1">SISR</a><a class="tags_container__link__1Ts3a" href="/tag/SLU/1">SLU</a><a class="tags_container__link__1Ts3a" href="/tag/Speech%20Disfluency/1">Speech Disfluency</a><a class="tags_container__link__1Ts3a" href="/tag/subprocess/1">subprocess</a><a class="tags_container__link__1Ts3a" href="/tag/Super-Resolution/1">Super-Resolution</a><a class="tags_container__link__1Ts3a" href="/tag/survey/1">survey</a><a class="tags_container__link__1Ts3a" href="/tag/tensorflow/1">tensorflow</a><a class="tags_container__link__1Ts3a" href="/tag/Tkinter/1">Tkinter</a><a class="tags_container__link__1Ts3a" href="/tag/Transfer%20Learning/1">Transfer Learning</a><a class="tags_container__link__1Ts3a" href="/tag/transformer/1">transformer</a><a class="tags_container__link__1Ts3a" href="/tag/Weight%20Interpolation/1">Weight Interpolation</a><a class="tags_container__link__1Ts3a" href="/tag/zsh/1">zsh</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91/1">オブジェクト指向</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%82%B3%E3%83%AC%E3%83%BC%E3%82%BF/1">デコレータ</a><a class="tags_container__link__1Ts3a" href="/tag/%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90/1">データ分析</a><a class="tags_container__link__1Ts3a" href="/tag/%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/1">特殊メソッド</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%81%9E%E3%81%8D%E6%89%8B%E5%8F%8D%E5%BF%9C/1">聞き手反応</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%AB%96%E6%96%87%E5%9F%B7%E7%AD%86/1">論文執筆</a><a class="tags_container__link__1Ts3a" href="/tag/%E8%B6%85%E8%A7%A3%E5%83%8F/1">超解像</a></div></div><ins class="adsbygoogle " style="display:block" data-ad-client="ca-pub-4998278830587376" data-ad-slot="8978700883" data-ad-layout="" data-ad-layout-key="" data-ad-format="auto" data-full-width-responsive="true"></ins><div id="TOC"></div></aside></div></div><footer class="footer_footer__WCChH"><div class="footer_footer__inner__287VQ"><div><a class="footer_footer__link__Ql5Ng" href="/privacy-policy">プライバシーポリシー</a></div><div class="footer_footer__title__PRn_u"><a href="/">ゆうぼうの書跡棚</a></div><div class="footer_footer__small__RlIHP"><small>Powered by <a target="_blank" class="footer_footer__small__link__u5kuV" href="https://twitter.com/Sloth65557166">ゆうぼう</a></small></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"contentHtml":"\u003cp\u003eColab Proでなんとかpytorch-lightningを使ってTPUで実験を回そうと奮闘した結果，得られたベストプラクティス?(怪しい)の共有です．\u003c/p\u003e\n\u003cp\u003eもっといい方法があったら教えてください．\u003c/p\u003e\n\u003cp\u003eちなみに，現行コンペで動作確認したので，コード全容の公開はないです．すみません．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRuntimeは必ずTPUにしてください\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003epytorch-lightningでTPUを使いたいメリット?\u003c/h2\u003e\n\u003cp\u003e黙って\u003cem\u003etensorflow\u003c/em\u003eを勉強して書けば，それでいいのでしょうが，\u003cbr\u003e\n普段\u003cem\u003epytorch\u003c/em\u003eを使ってコーディングをしていて，僕の場合\u003cem\u003epytorch-lightning\u003c/em\u003eを好んで使っているので，その延長戦でTPUを使いたいという欲がありました．\u003c/p\u003e\n\u003cp\u003eまた，\u003cem\u003epytorch-lightning\u003c/em\u003eは，\u003cstrong\u003eTrainerクラス\u003c/strong\u003eをちょろっと書き換えるだけで，CPU/GPU/TPUの変換ができ，マルチGPU等のDDPの処理も自動で書き加えてくれるため，\u003cbr\u003e\nかなり良いものかなと思っています．\u003c/p\u003e\n\u003cp\u003eCPU/GPU/TPUの書き換えは基本的に以下で十分です．\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pytorch_lightning \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pl\n\u003cspan class=\"hljs-comment\"\u003e# これらは共通\u003c/span\u003e\nmodel = LitModel()  \u003cspan class=\"hljs-comment\"\u003e# pl.LightningModuleを継承した何か\u003c/span\u003e\ndm = LitDataset()  \u003cspan class=\"hljs-comment\"\u003e# pl.LightningDataModuleを継承した何か\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# CPUを使う\u003c/span\u003e\ntrainer = Trainer()\ntrainer.fit(model, dm)\n\n\u003cspan class=\"hljs-comment\"\u003e# GPUを使う\u003c/span\u003e\ntrainer = Trainer(gpus=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)  \u003cspan class=\"hljs-comment\"\u003e# ありったけのGPU使う\u003c/span\u003e\ntrainer.fit(model, dm)\n\n\u003cspan class=\"hljs-comment\"\u003e# Single Core TPUを使う\u003c/span\u003e\ntrainer = Trainer(tpu_cores=[\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e])  \u003cspan class=\"hljs-comment\"\u003e# TPUのインデックス指定\u003c/span\u003e\ntrainer.fit(model, dm)\n\n\u003cspan class=\"hljs-comment\"\u003e# Multi Core TPUを使う\u003c/span\u003e\ntrainer = Trainer(tpu_cores=\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e)  \u003cspan class=\"hljs-comment\"\u003e# 1 or 8じゃないと怒られます...\u003c/span\u003e\ntrainer.fit(model, dm)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eああ，楽だ楽だ．\u003c/p\u003e\n\u003cp\u003eTPU簡単に動いてくれたらいいのに(願望)\u003c/p\u003e\n\u003ch2\u003eTPUを使うためのセットアップ\u003c/h2\u003e\n\u003cp\u003e以下を実行して，必要なライブラリ群を入れてしまいます．\u003c/p\u003e\n\u003cp\u003e一応僕が動作確認取れたのは，pytorch-xlaのバージョンが\u003cem\u003e1.8\u003c/em\u003eと\u003cem\u003e1.9\u003c/em\u003eです．\u003c/p\u003e\n\u003cp\u003eCircle CIのTPU環境に合わせたユニットテストは，確か1.8が通っていて，1.9がエラーを吐いていた気がします．\u003c/p\u003e\n\u003cp\u003e1.8の方が安全なのですかね？(わかりません)\u003c/p\u003e\n\u003cp\u003e\u003cem\u003epytorch-xla 1.8\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e!pip install cloud-tpu-client==\u003cspan class=\"hljs-number\"\u003e0.10\u003c/span\u003e https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-\u003cspan class=\"hljs-number\"\u003e1.8\u003c/span\u003e-cp37-cp37m-linux_x86_64.whl\n!pip install -q torch==\u003cspan class=\"hljs-number\"\u003e1.8\u003c/span\u003e torchvision torchtext\n!pip install -q pytorch-lightning==\u003cspan class=\"hljs-number\"\u003e1.4\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.9\u003c/span\u003e torchmetrics\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003epytorch-xla 1.9\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e!pip install cloud-tpu-client==\u003cspan class=\"hljs-number\"\u003e0.10\u003c/span\u003e https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-\u003cspan class=\"hljs-number\"\u003e1.9\u003c/span\u003e-cp37-cp37m-linux_x86_64.whl\n!pip install -q torch==\u003cspan class=\"hljs-number\"\u003e1.9\u003c/span\u003e torchvision torchtext\n!pip install -q pytorch-lightning==\u003cspan class=\"hljs-number\"\u003e1.4\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.9\u003c/span\u003e torchmetrics\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003epipでのインストール分割していたり，--quit(-q)をつけているのは特に理由はないです．\u003c/p\u003e\n\u003cp\u003eその辺はお好みでやってください．\u003c/p\u003e\n\u003cp\u003e一応僕の環境ではこんな感じで，以後のプログラムはうまいこと動作しました．\u003c/p\u003e\n\u003cp\u003eおそらく以下のようなエラーは起きますが，特に動作に影響はなさそうでした．あまり不安がらなくても良さそうです．\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003eERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nearthengine-api 0.1.284 requires google-api-python-client\u0026#x3C;2,\u003e=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eセットアップ後にimport pytorch_lightning\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pytorch_lightning \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eimport\u003c/em\u003eすると以下のような出力を吐きます．\u003c/p\u003e\n\u003cp\u003eインストールした\u003cem\u003exla\u003c/em\u003eのバージョンが1.9ならば，以下のようなメッセージが出ます．\u003c/p\u003e\n\u003cp\u003e少し時間がかかりますが，待ちましょう．\u003c/p\u003e\n\u003cp\u003e\u003cem\u003epytorch-xla 1.9\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003eWARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\nWARNING:root:TPU has started up successfully with version pytorch-1.9\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eもし，\u003cem\u003eTPU not Found\u003c/em\u003e的なことが出たら，一回\u003cem\u003eFactory reset Runtime\u003c/em\u003eして，またインストールからやり直せばうまく行くと思います．\u003c/p\u003e\n\u003ch2\u003eimportまでうまく行けたら\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eimport\u003c/em\u003eするまで行けたら，このあとは特に問題なく動くはずです．\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# ModelとかDataModuleとかは適宜設定してください．\u003c/span\u003e\n\ntrainer = Trainer(tpu_cores=\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e)\ntrainer.fit(model, dm)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eこのあとすぐエラー出なきゃ，勝手に動くと思います(雑ww)\u003c/p\u003e\n\u003ch2\u003e注意するべきこと\u003c/h2\u003e\n\u003cp\u003e幾つか僕が遭遇したエラーを挙げておきます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewandb loggerが使えない\n\u003cul\u003e\n\u003cli\u003e多分Commet loggerも使えない？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eマルチコアTPU使うなら，predict時return_predictionsができない\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003epytorch_lightning.callbacks.BasePredictionWriter\u003c/em\u003eを使おう(\u003ca href=\"#BasePredictionWriter%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB\"\u003eサンプル\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eなぜか知らんけど，\u003ca href=\"#RAM%E3%81%AE%E4%BD%BF%E7%94%A8%E7%8E%8750%25%E3%81%8F%E3%82%89%E3%81%84%E3%81%AB%E3%81%AA%E3%82%8B%E3%81%A8stack%E3%81%99%E3%82%8B%E4%BB%B6%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"\u003e\u003cstrong\u003eRAMの使用率が50%あたりになるとstackする\u003c/strong\u003e\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eこうなると一生動かなくなるので，\u003cstrong\u003e最初安定するまで監視が必要\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eColabの下のデバッガみたいなやつが，ずっと*select() \u003e spawn()*的な非同期のところで止まるので，そうなったらRAMチェックしよう!!!\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBasePredictionWriterのサンプル\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pytorch_lightning.callbacks \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e BasePredictionWriter\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCustomWriter\u003c/span\u003e(\u003cspan class=\"hljs-title class_ inherited__\"\u003eBasePredictionWriter\u003c/span\u003e):\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, output_dir: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, write_interval: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'batch'\u003c/span\u003e\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__(write_interval)\n        self.output_dir = output_dir\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ewrite_on_batch_end\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n        self, trainer, pl_module: \u003cspan class=\"hljs-string\"\u003e'LightningModule'\u003c/span\u003e, prediction: \u003cspan class=\"hljs-type\"\u003eAny\u003c/span\u003e, batch_indices: \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e], batch: \u003cspan class=\"hljs-type\"\u003eAny\u003c/span\u003e,\n        batch_idx: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e, dataloader_idx: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e\n    \u003c/span\u003e):\n        torch.save(prediction, os.path.join(self.output_dir, dataloader_idx, \u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{batch_idx}\u003c/span\u003e.pt\"\u003c/span\u003e))\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ewrite_on_epoch_end\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n        self, trainer, pl_module: \u003cspan class=\"hljs-string\"\u003e'LightningModule'\u003c/span\u003e, predictions: \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-type\"\u003eAny\u003c/span\u003e], batch_indices: \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-type\"\u003eAny\u003c/span\u003e]\n    \u003c/span\u003e):\n        torch.save(predictions, os.path.join(self.output_dir, \u003cspan class=\"hljs-string\"\u003e\"predictions.pt\"\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eRAMの使用率50%くらいになるとstackする件について\u003c/h2\u003e\n\u003cp\u003eはっきり言って，これは本当に原因がわかりません．\u003c/p\u003e\n\u003cp\u003e50%くらいで張りついちゃうので，batch_size下げようが，RAMの使用率はあまり変わる気がしません．\u003c/p\u003e\n\u003cp\u003eこうなってstackしちゃったら，もうHigh-RAM設定にしてRAMにゆとりを持たせた方がいいと思います．潔く．\u003c/p\u003e\n\u003cp\u003eHigh-RAMでも張りついちゃう場合は，僕はわかりません．\u003c/p\u003e\n\u003cp\u003eRAM周りの最適化とか知見ある方，解決策わかったら共有してくださるととてもありがたいです......\u003c/p\u003e\n\u003ch2\u003eまとめ\u003c/h2\u003e\n\u003cp\u003eまとめじゃないです．嘘つきました．\u003c/p\u003e\n\u003cp\u003eでも，まとめないとなんかあれなので，CPU/GPU/TPUで使いまわせるように，セットアップの部分整理します．\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sys\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e os.environ.get(\u003cspan class=\"hljs-string\"\u003e'COLAB_TPU_ADDR'\u003c/span\u003e, \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e):\n    !pip install cloud-tpu-client==\u003cspan class=\"hljs-number\"\u003e0.10\u003c/span\u003e https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-\u003cspan class=\"hljs-number\"\u003e1.8\u003c/span\u003e-cp37-cp37m-linux_x86_64.whl\n    !pip install -q torch==\u003cspan class=\"hljs-number\"\u003e1.8\u003c/span\u003e torchvision torchtext\n\n!pip install pytorch-lightning==\u003cspan class=\"hljs-number\"\u003e1.4\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.9\u003c/span\u003e torchmetrics\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eもし，KaggleとColabでも行けるようにしたいのであれば，\u003ccode\u003e'google.colab' in sys.modules\u003c/code\u003eでやるといいと思います．\u003c/p\u003e\n\u003cp\u003eColab TPU使って，pytorch-lightningでうまく実験回しまくっている方いたら，色々意見ください．お願いします :)\u003c/p\u003e\n\u003cp\u003e知見たまって，エラーハンドリングとかわかってきたら，この記事更新するか，新たに書き始めます．よろしくお願いします．\u003c/p\u003e","Title":"Pytorch LightningでTPUを回す on Colab Pro in 2021","Date":"2021-10-11","Category":"Python","Tags":["pytorch-lightning","Colab"],"Authors":"ゆうぼう","Slug":"pytorch-lightning_on_colab-tpu_2021","Thumbnail":"/images/thumbnails/pytorch-lightning_on_colab-pro-tpu.png","Description":"Colab Proでなんとかpytorch-lightningを使ってTPUで実験を回そうと奮闘した結果，得られたベストプラクティスの共有です．もっといい方法があったら教えてください．","Published":true},"categories":["論文","Web","JavaScript","Competition","Cloud","Python","Linux","ML","Go","SQL"],"tags":["Apache","Appium","ASR","atmaCup","AWS","brew","CentOS7","CentOS8","Colab","COMET","commonsense","conda","Contrasive Learning","Contrastive Learning","CSS","Demo","Dialogue Structure Learning","dialogue system","DST","Emotion Recognition","empathetic dialogue system","encyclopedic","Error Correction","ESPNet","ffmpeg","Flask","Gating Mechanism","Go","Google Colaboratory","Heroku","Highway Transformer","HTML","humor detection","Intent Classification","Internet-Augmented","JavaScript","JSON","Kaggle","KC-Net","knowledge-base","Knowledge-Intensive NLP","laughter","Linux","LLM","Mac","make","map","MeCab","mental health","mental state knowledge","mentalisation","MentalRoBERTa","Merging Models","ML","Model Editing","Model Patching","MT","Multi-Hop Transformer","multi-modal","MySQL","NLG","NLI","NLP","Node","node.js","npm","Overleaf","Pandas","persona","PLMKE","Poetry","Prompt-Tuning","Python","Pytorch","pytorch-lightning","Scikit-learn","Selenium","Self-Dependency-Units (SDU)","shared laughter","SISR","SLU","Speech Disfluency","subprocess","Super-Resolution","survey","tensorflow","Tkinter","Transfer Learning","transformer","Weight Interpolation","zsh","オブジェクト指向","デコレータ","データ分析","特殊メソッド","聞き手反応","論文執筆","超解像"]},"__N_SSG":true},"page":"/[slug]","query":{"slug":"pytorch-lightning_on_colab-tpu_2021"},"buildId":"lR7wjVyUe1SvaFyxOcA82","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>